begin_unit
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_package
DECL|package|org.apache.solr.hadoop
package|package
name|org
operator|.
name|apache
operator|.
name|solr
operator|.
name|hadoop
package|;
end_package
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedReader
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedWriter
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStreamReader
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStreamWriter
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Writer
import|;
end_import
begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|invoke
operator|.
name|MethodHandles
import|;
end_import
begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URISyntaxException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import
begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLClassLoader
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|StandardCharsets
import|;
end_import
begin_import
import|import
name|java
operator|.
name|text
operator|.
name|NumberFormat
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Comparator
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Locale
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|ArgumentParsers
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|impl
operator|.
name|Arguments
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|impl
operator|.
name|action
operator|.
name|HelpArgumentAction
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|impl
operator|.
name|choice
operator|.
name|RangeArgumentChoice
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|impl
operator|.
name|type
operator|.
name|FileArgumentType
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|Argument
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|ArgumentGroup
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|ArgumentParser
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|ArgumentParserException
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|FeatureControl
import|;
end_import
begin_import
import|import
name|net
operator|.
name|sourceforge
operator|.
name|argparse4j
operator|.
name|inf
operator|.
name|Namespace
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configured
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|LongWritable
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobClient
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|input
operator|.
name|NLineInputFormat
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|output
operator|.
name|FileOutputFormat
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|output
operator|.
name|TextOutputFormat
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|GenericOptionsParser
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Tool
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ToolRunner
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|solr
operator|.
name|common
operator|.
name|cloud
operator|.
name|SolrZkClient
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|solr
operator|.
name|hadoop
operator|.
name|dedup
operator|.
name|RetainMostRecentUpdateConflictResolver
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|solr
operator|.
name|hadoop
operator|.
name|morphline
operator|.
name|MorphlineMapRunner
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|solr
operator|.
name|hadoop
operator|.
name|morphline
operator|.
name|MorphlineMapper
import|;
end_import
begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import
begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|kitesdk
operator|.
name|morphline
operator|.
name|base
operator|.
name|Fields
import|;
end_import
begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import
begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|io
operator|.
name|ByteStreams
import|;
end_import
begin_comment
comment|/**  * Public API for a MapReduce batch job driver that creates a set of Solr index shards from a set of  * input files and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner.  * Also supports merging the output shards into a set of live customer facing Solr servers,  * typically a SolrCloud.  */
end_comment
begin_class
DECL|class|MapReduceIndexerTool
specifier|public
class|class
name|MapReduceIndexerTool
extends|extends
name|Configured
implements|implements
name|Tool
block|{
DECL|field|job
name|Job
name|job
decl_stmt|;
comment|// visible for testing only
DECL|field|RESULTS_DIR
specifier|public
specifier|static
specifier|final
name|String
name|RESULTS_DIR
init|=
literal|"results"
decl_stmt|;
DECL|field|MAIN_MEMORY_RANDOMIZATION_THRESHOLD
specifier|static
specifier|final
name|String
name|MAIN_MEMORY_RANDOMIZATION_THRESHOLD
init|=
name|MapReduceIndexerTool
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|+
literal|".mainMemoryRandomizationThreshold"
decl_stmt|;
DECL|field|FULL_INPUT_LIST
specifier|private
specifier|static
specifier|final
name|String
name|FULL_INPUT_LIST
init|=
literal|"full-input-list.txt"
decl_stmt|;
DECL|field|LOG
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|MethodHandles
operator|.
name|lookup
argument_list|()
operator|.
name|lookupClass
argument_list|()
argument_list|)
decl_stmt|;
comment|/**    * See http://argparse4j.sourceforge.net and for details see http://argparse4j.sourceforge.net/usage.html    */
DECL|class|MyArgumentParser
specifier|static
specifier|final
class|class
name|MyArgumentParser
block|{
DECL|field|SHOW_NON_SOLR_CLOUD
specifier|private
specifier|static
specifier|final
name|String
name|SHOW_NON_SOLR_CLOUD
init|=
literal|"--show-non-solr-cloud"
decl_stmt|;
DECL|field|showNonSolrCloud
specifier|private
name|boolean
name|showNonSolrCloud
init|=
literal|false
decl_stmt|;
comment|/**      * Parses the given command line arguments.      *       * @return exitCode null indicates the caller shall proceed with processing,      *         non-null indicates the caller shall exit the program with the      *         given exit status code.      */
DECL|method|parseArgs
specifier|public
name|Integer
name|parseArgs
parameter_list|(
name|String
index|[]
name|args
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|Options
name|opts
parameter_list|)
block|{
assert|assert
name|args
operator|!=
literal|null
assert|;
assert|assert
name|conf
operator|!=
literal|null
assert|;
assert|assert
name|opts
operator|!=
literal|null
assert|;
if|if
condition|(
name|args
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|args
operator|=
operator|new
name|String
index|[]
block|{
literal|"--help"
block|}
expr_stmt|;
block|}
name|showNonSolrCloud
operator|=
name|Arrays
operator|.
name|asList
argument_list|(
name|args
argument_list|)
operator|.
name|contains
argument_list|(
name|SHOW_NON_SOLR_CLOUD
argument_list|)
expr_stmt|;
comment|// intercept it first
name|ArgumentParser
name|parser
init|=
name|ArgumentParsers
operator|.
name|newArgumentParser
argument_list|(
literal|"hadoop [GenericOptions]... jar solr-map-reduce-*.jar "
argument_list|,
literal|false
argument_list|)
operator|.
name|defaultHelp
argument_list|(
literal|true
argument_list|)
operator|.
name|description
argument_list|(
literal|"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files "
operator|+
literal|"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. "
operator|+
literal|"It also supports merging the output shards into a set of live customer facing Solr servers, "
operator|+
literal|"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:"
operator|+
literal|"\n\n"
operator|+
literal|"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread "
operator|+
literal|"indexing load more evenly among the mappers of the subsequent phase."
operator|+
literal|"\n\n"
operator|+
literal|"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it "
operator|+
literal|"and hands SolrInputDocuments to a set of reducers. "
operator|+
literal|"The ETL functionality is flexible and "
operator|+
literal|"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. "
operator|+
literal|"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, "
operator|+
literal|"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional "
operator|+
literal|"file or data formats can be added as morphline plugins. "
operator|+
literal|"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream "
operator|+
literal|"plus some headers plus contextual metadata) and generates as output zero or more records. "
operator|+
literal|"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, "
operator|+
literal|"and any custom ETL logic can be registered and executed.\n"
operator|+
literal|"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: "
operator|+
literal|"hadoop ... -D "
operator|+
name|MorphlineMapRunner
operator|.
name|MORPHLINE_FIELD_PREFIX
operator|+
name|Fields
operator|.
name|ATTACHMENT_MIME_TYPE
operator|+
literal|"=text/csv"
operator|+
literal|"\n\n"
operator|+
literal|"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. "
operator|+
literal|"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their "
operator|+
literal|"data in HDFS."
operator|+
literal|"\n\n"
operator|+
literal|"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr "
operator|+
literal|"shards expected by the user, using a mapper-only job. This phase is omitted if the number "
operator|+
literal|"of shards is already equal to the number of shards expected by the user. "
operator|+
literal|"\n\n"
operator|+
literal|"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of "
operator|+
literal|"live customer facing Solr servers, typically a SolrCloud. "
operator|+
literal|"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories."
operator|+
literal|"\n\n"
operator|+
literal|"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. "
operator|+
literal|"On program startup all data in the --output-dir is deleted if that output directory already exists. "
operator|+
literal|"If the whole job fails you can retry simply by rerunning the program again using the same arguments."
argument_list|)
decl_stmt|;
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--help"
argument_list|,
literal|"-help"
argument_list|,
literal|"-h"
argument_list|)
operator|.
name|help
argument_list|(
literal|"Show this help message and exit"
argument_list|)
operator|.
name|action
argument_list|(
operator|new
name|HelpArgumentAction
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|(
name|ArgumentParser
name|parser
parameter_list|,
name|Argument
name|arg
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
name|attrs
parameter_list|,
name|String
name|flag
parameter_list|,
name|Object
name|value
parameter_list|)
throws|throws
name|ArgumentParserException
block|{
name|parser
operator|.
name|printHelp
argument_list|()
expr_stmt|;
name|System
operator|.
name|out
operator|.
name|println
argument_list|()
expr_stmt|;
name|System
operator|.
name|out
operator|.
name|print
argument_list|(
name|ToolRunnerHelpFormatter
operator|.
name|getGenericCommandUsage
argument_list|()
argument_list|)
expr_stmt|;
comment|//ToolRunner.printGenericCommandUsage(System.out);
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Examples: \n\n"
operator|+
literal|"# (Re)index an Avro based Twitter tweet file:\n"
operator|+
literal|"sudo -u hdfs hadoop \\\n"
operator|+
literal|"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n"
operator|+
literal|"  jar target/solr-map-reduce-*.jar \\\n"
operator|+
literal|"  -D 'mapred.child.java.opts=-Xmx500m' \\\n"
operator|+
comment|//            "  -D 'mapreduce.child.java.opts=-Xmx500m' \\\n" +
literal|"  --log4j src/test/resources/log4j.properties \\\n"
operator|+
literal|"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n"
operator|+
literal|"  --solr-home-dir src/test/resources/solr/minimr \\\n"
operator|+
literal|"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n"
operator|+
literal|"  --shards 1 \\\n"
operator|+
literal|"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\n"
operator|+
literal|"\n"
operator|+
literal|"# Go live by merging resulting index shards into a live Solr cluster\n"
operator|+
literal|"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\n"
operator|+
literal|"sudo -u hdfs hadoop \\\n"
operator|+
literal|"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n"
operator|+
literal|"  jar target/solr-map-reduce-*.jar \\\n"
operator|+
literal|"  -D 'mapred.child.java.opts=-Xmx500m' \\\n"
operator|+
comment|//            "  -D 'mapreduce.child.java.opts=-Xmx500m' \\\n" +
literal|"  --log4j src/test/resources/log4j.properties \\\n"
operator|+
literal|"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n"
operator|+
literal|"  --solr-home-dir src/test/resources/solr/minimr \\\n"
operator|+
literal|"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n"
operator|+
literal|"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\n"
operator|+
literal|"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\n"
operator|+
literal|"  --go-live \\\n"
operator|+
literal|"  hdfs:///user/foo/indir\n"
operator|+
literal|"\n"
operator|+
literal|"# Go live by merging resulting index shards into a live SolrCloud cluster\n"
operator|+
literal|"# (discover shards and Solr URLs through ZooKeeper):\n"
operator|+
literal|"sudo -u hdfs hadoop \\\n"
operator|+
literal|"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n"
operator|+
literal|"  jar target/solr-map-reduce-*.jar \\\n"
operator|+
literal|"  -D 'mapred.child.java.opts=-Xmx500m' \\\n"
operator|+
comment|//            "  -D 'mapreduce.child.java.opts=-Xmx500m' \\\n" +
literal|"  --log4j src/test/resources/log4j.properties \\\n"
operator|+
literal|"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n"
operator|+
literal|"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n"
operator|+
literal|"  --zk-host zk01.mycompany.com:2181/solr \\\n"
operator|+
literal|"  --collection collection1 \\\n"
operator|+
literal|"  --go-live \\\n"
operator|+
literal|"  hdfs:///user/foo/indir\n"
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|FoundHelpArgument
argument_list|()
throw|;
comment|// Trick to prevent processing of any remaining arguments
block|}
block|}
argument_list|)
expr_stmt|;
name|ArgumentGroup
name|requiredGroup
init|=
name|parser
operator|.
name|addArgumentGroup
argument_list|(
literal|"Required arguments"
argument_list|)
decl_stmt|;
name|Argument
name|outputDirArg
init|=
name|requiredGroup
operator|.
name|addArgument
argument_list|(
literal|"--output-dir"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"HDFS_URI"
argument_list|)
operator|.
name|type
argument_list|(
operator|new
name|PathArgumentType
argument_list|(
name|conf
argument_list|)
block|{
annotation|@
name|Override
specifier|public
name|Path
name|convert
parameter_list|(
name|ArgumentParser
name|parser
parameter_list|,
name|Argument
name|arg
parameter_list|,
name|String
name|value
parameter_list|)
throws|throws
name|ArgumentParserException
block|{
name|Path
name|path
init|=
name|super
operator|.
name|convert
argument_list|(
name|parser
argument_list|,
name|arg
argument_list|,
name|value
argument_list|)
decl_stmt|;
if|if
condition|(
literal|"hdfs"
operator|.
name|equals
argument_list|(
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
argument_list|)
operator|&&
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getAuthority
argument_list|()
operator|==
literal|null
condition|)
block|{
comment|// TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"Missing authority in path URI: "
operator|+
name|path
argument_list|,
name|parser
argument_list|)
throw|;
block|}
return|return
name|path
return|;
block|}
block|}
operator|.
name|verifyHasScheme
argument_list|()
operator|.
name|verifyIsAbsolute
argument_list|()
operator|.
name|verifyCanWriteParent
argument_list|()
argument_list|)
operator|.
name|required
argument_list|(
literal|true
argument_list|)
operator|.
name|help
argument_list|(
literal|"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. "
operator|+
literal|"Example: hdfs://c2202.mycompany.com/user/$USER/test"
argument_list|)
decl_stmt|;
name|Argument
name|inputListArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--input-list"
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|append
argument_list|()
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"URI"
argument_list|)
comment|//      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())
operator|.
name|type
argument_list|(
name|Path
operator|.
name|class
argument_list|)
operator|.
name|help
argument_list|(
literal|"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, "
operator|+
literal|"one URI per line in the file. If '-' is specified, URIs are read from the standard input. "
operator|+
literal|"Multiple --input-list arguments can be specified."
argument_list|)
decl_stmt|;
name|Argument
name|morphlineFileArg
init|=
name|requiredGroup
operator|.
name|addArgument
argument_list|(
literal|"--morphline-file"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"FILE"
argument_list|)
operator|.
name|type
argument_list|(
operator|new
name|FileArgumentType
argument_list|()
operator|.
name|verifyExists
argument_list|()
operator|.
name|verifyIsFile
argument_list|()
operator|.
name|verifyCanRead
argument_list|()
argument_list|)
operator|.
name|required
argument_list|(
literal|true
argument_list|)
operator|.
name|help
argument_list|(
literal|"Relative or absolute path to a local config file that contains one or more morphlines. "
operator|+
literal|"The file must be UTF-8 encoded. Example: /path/to/morphline.conf"
argument_list|)
decl_stmt|;
name|Argument
name|morphlineIdArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--morphline-id"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"STRING"
argument_list|)
operator|.
name|type
argument_list|(
name|String
operator|.
name|class
argument_list|)
operator|.
name|help
argument_list|(
literal|"The identifier of the morphline that shall be executed within the morphline config file "
operator|+
literal|"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. "
operator|+
literal|"top-most) morphline within the config file is used. Example: morphline1"
argument_list|)
decl_stmt|;
name|Argument
name|solrHomeDirArg
init|=
name|nonSolrCloud
argument_list|(
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--solr-home-dir"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"DIR"
argument_list|)
operator|.
name|type
argument_list|(
operator|new
name|FileArgumentType
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|File
name|convert
parameter_list|(
name|ArgumentParser
name|parser
parameter_list|,
name|Argument
name|arg
parameter_list|,
name|String
name|value
parameter_list|)
throws|throws
name|ArgumentParserException
block|{
name|File
name|solrHomeDir
init|=
name|super
operator|.
name|convert
argument_list|(
name|parser
argument_list|,
name|arg
argument_list|,
name|value
argument_list|)
decl_stmt|;
name|File
name|solrConfigFile
init|=
operator|new
name|File
argument_list|(
operator|new
name|File
argument_list|(
name|solrHomeDir
argument_list|,
literal|"conf"
argument_list|)
argument_list|,
literal|"solrconfig.xml"
argument_list|)
decl_stmt|;
operator|new
name|FileArgumentType
argument_list|()
operator|.
name|verifyExists
argument_list|()
operator|.
name|verifyIsFile
argument_list|()
operator|.
name|verifyCanRead
argument_list|()
operator|.
name|convert
argument_list|(
name|parser
argument_list|,
name|arg
argument_list|,
name|solrConfigFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|solrHomeDir
return|;
block|}
block|}
operator|.
name|verifyIsDirectory
argument_list|()
operator|.
name|verifyCanRead
argument_list|()
argument_list|)
operator|.
name|required
argument_list|(
literal|false
argument_list|)
operator|.
name|help
argument_list|(
literal|"Relative or absolute path to a local dir containing Solr conf/ dir and in particular "
operator|+
literal|"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. "
operator|+
literal|"Example: src/test/resources/solr/minimr"
argument_list|)
argument_list|)
decl_stmt|;
name|Argument
name|updateConflictResolverArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--update-conflict-resolver"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"FQCN"
argument_list|)
operator|.
name|type
argument_list|(
name|String
operator|.
name|class
argument_list|)
operator|.
name|setDefault
argument_list|(
name|RetainMostRecentUpdateConflictResolver
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. "
operator|+
literal|"This enables deduplication and ordering of a series of document updates for the same unique document "
operator|+
literal|"key. For example, a MapReduce batch job might index multiple files in the same job where some of the "
operator|+
literal|"files contain old and new versions of the very same document, using the same unique document key.\n"
operator|+
literal|"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but "
operator|+
literal|"the most recent document version, or, in the general case, order colliding updates ascending from least "
operator|+
literal|"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then "
operator|+
literal|"apply the updates to Solr in the order returned by the orderUpdates() method.\n"
operator|+
literal|"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document "
operator|+
literal|"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp"
argument_list|)
decl_stmt|;
name|Argument
name|mappersArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--mappers"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
operator|-
literal|1
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
comment|// TODO: also support X% syntax where X is an integer
operator|.
name|setDefault
argument_list|(
operator|-
literal|1
argument_list|)
operator|.
name|help
argument_list|(
literal|"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots "
operator|+
literal|"available on the cluster."
argument_list|)
decl_stmt|;
name|Argument
name|reducersArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--reducers"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
operator|-
literal|2
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
comment|// TODO: also support X% syntax where X is an integer
operator|.
name|setDefault
argument_list|(
operator|-
literal|1
argument_list|)
operator|.
name|help
argument_list|(
literal|"Tuning knob that indicates the number of reducers to index into. "
operator|+
literal|"0 is reserved for a mapper-only feature that may ship in a future release. "
operator|+
literal|"-1 indicates use all reduce slots available on the cluster. "
operator|+
literal|"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. "
operator|+
literal|"The mtree merge MR algorithm improves scalability by spreading load "
operator|+
literal|"(in particular CPU load) among a number of parallel reducers that can be much larger than the number "
operator|+
literal|"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges "
operator|+
literal|"and tiered lucene merges to the clustered case. The subsequent mapper-only phase "
operator|+
literal|"merges the output of said large number of reducers to the number of shards expected by the user, "
operator|+
literal|"again by utilizing more available parallelism on the cluster."
argument_list|)
decl_stmt|;
name|Argument
name|fanoutArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--fanout"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
literal|2
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
operator|.
name|setDefault
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
operator|.
name|help
argument_list|(
name|FeatureControl
operator|.
name|SUPPRESS
argument_list|)
decl_stmt|;
name|Argument
name|maxSegmentsArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--max-segments"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
literal|1
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
operator|.
name|setDefault
argument_list|(
literal|1
argument_list|)
operator|.
name|help
argument_list|(
literal|"Tuning knob that indicates the maximum number of segments to be contained on output in the index of "
operator|+
literal|"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments "
operator|+
literal|"until there are<= maxSegments lucene segments left in this index. "
operator|+
literal|"Merging segments involves reading and rewriting all data in all these segment files, "
operator|+
literal|"potentially multiple times, which is very I/O intensive and time consuming. "
operator|+
literal|"However, an index with fewer segments can later be merged faster, "
operator|+
literal|"and it can later be queried faster once deployed to a live Solr serving shard. "
operator|+
literal|"Set maxSegments to 1 to optimize the index for low query latency. "
operator|+
literal|"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. "
operator|+
literal|"This can be a reasonable trade-off for batch indexing systems."
argument_list|)
decl_stmt|;
name|Argument
name|fairSchedulerPoolArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--fair-scheduler-pool"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"STRING"
argument_list|)
operator|.
name|help
argument_list|(
literal|"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. "
operator|+
literal|"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. "
operator|+
literal|"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an "
operator|+
literal|"equal share of resources over time. When there is a single job running, that job uses the entire "
operator|+
literal|"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so "
operator|+
literal|"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which "
operator|+
literal|"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. "
operator|+
literal|"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with "
operator|+
literal|"job priorities - the priorities are used as weights to determine the fraction of total compute time "
operator|+
literal|"that each job gets."
argument_list|)
decl_stmt|;
name|Argument
name|dryRunArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--dry-run"
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|storeTrue
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Run in local mode and print documents to stdout instead of loading them into Solr. This executes "
operator|+
literal|"the morphline in the client process (without submitting a job to MR) for quicker turnaround during "
operator|+
literal|"early trial& debug sessions."
argument_list|)
decl_stmt|;
name|Argument
name|log4jConfigFileArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--log4j"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"FILE"
argument_list|)
operator|.
name|type
argument_list|(
operator|new
name|FileArgumentType
argument_list|()
operator|.
name|verifyExists
argument_list|()
operator|.
name|verifyIsFile
argument_list|()
operator|.
name|verifyCanRead
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Relative or absolute path to a log4j.properties config file on the local file system. This file "
operator|+
literal|"will be uploaded to each MR task. Example: /path/to/log4j.properties"
argument_list|)
decl_stmt|;
name|Argument
name|verboseArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"--verbose"
argument_list|,
literal|"-v"
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|storeTrue
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Turn on verbose output."
argument_list|)
decl_stmt|;
name|parser
operator|.
name|addArgument
argument_list|(
name|SHOW_NON_SOLR_CLOUD
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|storeTrue
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Also show options for Non-SolrCloud mode as part of --help."
argument_list|)
expr_stmt|;
name|ArgumentGroup
name|clusterInfoGroup
init|=
name|parser
operator|.
name|addArgumentGroup
argument_list|(
literal|"Cluster arguments"
argument_list|)
operator|.
name|description
argument_list|(
literal|"Arguments that provide information about your Solr cluster. "
operator|+
name|nonSolrCloud
argument_list|(
literal|"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. "
operator|+
literal|"If you are building shards for "
operator|+
literal|"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for "
operator|+
literal|"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. "
operator|+
literal|"Using --go-live requires either --zk-host or --shard-url."
argument_list|)
argument_list|)
decl_stmt|;
name|Argument
name|zkHostArg
init|=
name|clusterInfoGroup
operator|.
name|addArgument
argument_list|(
literal|"--zk-host"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"STRING"
argument_list|)
operator|.
name|type
argument_list|(
name|String
operator|.
name|class
argument_list|)
operator|.
name|help
argument_list|(
literal|"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. "
operator|+
literal|"This ZooKeeper ensemble will be examined to determine the number of output "
operator|+
literal|"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. "
operator|+
literal|"Requires that you also pass the --collection to merge the shards into.\n"
operator|+
literal|"\n"
operator|+
literal|"The --zk-host option implements the same partitioning semantics as the standard SolrCloud "
operator|+
literal|"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with "
operator|+
literal|"updates from standard Solr NRT ingestion on the same SolrCloud cluster, "
operator|+
literal|"using identical unique document keys.\n"
operator|+
literal|"\n"
operator|+
literal|"Format is: a list of comma separated host:port pairs, each corresponding to a zk "
operator|+
literal|"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If "
operator|+
literal|"the optional chroot suffix is used the example would look "
operator|+
literal|"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' "
operator|+
literal|"where the client would be rooted at '/solr' and all paths "
operator|+
literal|"would be relative to this root - i.e. getting/setting/etc... "
operator|+
literal|"'/foo/bar' would result in operations being run on "
operator|+
literal|"'/solr/foo/bar' (from the server perspective).\n"
operator|+
name|nonSolrCloud
argument_list|(
literal|"\n"
operator|+
literal|"If --solr-home-dir is not specified, the Solr home directory for the collection "
operator|+
literal|"will be downloaded from this ZooKeeper ensemble."
argument_list|)
argument_list|)
decl_stmt|;
name|Argument
name|shardUrlsArg
init|=
name|nonSolrCloud
argument_list|(
name|clusterInfoGroup
operator|.
name|addArgument
argument_list|(
literal|"--shard-url"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"URL"
argument_list|)
operator|.
name|type
argument_list|(
name|String
operator|.
name|class
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|append
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Solr URL to merge resulting shard into if using --go-live. "
operator|+
literal|"Example: http://solr001.mycompany.com:8983/solr/collection1. "
operator|+
literal|"Multiple --shard-url arguments can be specified, one for each desired shard. "
operator|+
literal|"If you are merging shards into a SolrCloud cluster, use --zk-host instead."
argument_list|)
argument_list|)
decl_stmt|;
name|Argument
name|shardsArg
init|=
name|nonSolrCloud
argument_list|(
name|clusterInfoGroup
operator|.
name|addArgument
argument_list|(
literal|"--shards"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
literal|1
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
operator|.
name|help
argument_list|(
literal|"Number of output shards to generate."
argument_list|)
argument_list|)
decl_stmt|;
name|ArgumentGroup
name|goLiveGroup
init|=
name|parser
operator|.
name|addArgumentGroup
argument_list|(
literal|"Go live arguments"
argument_list|)
operator|.
name|description
argument_list|(
literal|"Arguments for merging the shards that are built into a live Solr cluster. "
operator|+
literal|"Also see the Cluster arguments."
argument_list|)
decl_stmt|;
name|Argument
name|goLiveArg
init|=
name|goLiveGroup
operator|.
name|addArgument
argument_list|(
literal|"--go-live"
argument_list|)
operator|.
name|action
argument_list|(
name|Arguments
operator|.
name|storeTrue
argument_list|()
argument_list|)
operator|.
name|help
argument_list|(
literal|"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. "
operator|+
literal|"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. "
operator|+
name|nonSolrCloud
argument_list|(
literal|"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge "
operator|+
literal|"each shard into."
argument_list|)
argument_list|)
decl_stmt|;
name|Argument
name|collectionArg
init|=
name|goLiveGroup
operator|.
name|addArgument
argument_list|(
literal|"--collection"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"STRING"
argument_list|)
operator|.
name|help
argument_list|(
literal|"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1"
argument_list|)
decl_stmt|;
name|Argument
name|goLiveThreadsArg
init|=
name|goLiveGroup
operator|.
name|addArgument
argument_list|(
literal|"--go-live-threads"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"INTEGER"
argument_list|)
operator|.
name|type
argument_list|(
name|Integer
operator|.
name|class
argument_list|)
operator|.
name|choices
argument_list|(
operator|new
name|RangeArgumentChoice
argument_list|(
literal|1
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
operator|.
name|setDefault
argument_list|(
literal|1000
argument_list|)
operator|.
name|help
argument_list|(
literal|"Tuning knob that indicates the maximum number of live merges to run in parallel at one time."
argument_list|)
decl_stmt|;
comment|// trailing positional arguments
name|Argument
name|inputFilesArg
init|=
name|parser
operator|.
name|addArgument
argument_list|(
literal|"input-files"
argument_list|)
operator|.
name|metavar
argument_list|(
literal|"HDFS_URI"
argument_list|)
operator|.
name|type
argument_list|(
operator|new
name|PathArgumentType
argument_list|(
name|conf
argument_list|)
operator|.
name|verifyHasScheme
argument_list|()
operator|.
name|verifyExists
argument_list|()
operator|.
name|verifyCanRead
argument_list|()
argument_list|)
operator|.
name|nargs
argument_list|(
literal|"*"
argument_list|)
operator|.
name|setDefault
argument_list|()
operator|.
name|help
argument_list|(
literal|"HDFS URI of file or directory tree to index."
argument_list|)
decl_stmt|;
name|Namespace
name|ns
decl_stmt|;
try|try
block|{
name|ns
operator|=
name|parser
operator|.
name|parseArgs
argument_list|(
name|args
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|FoundHelpArgument
name|e
parameter_list|)
block|{
return|return
literal|0
return|;
block|}
catch|catch
parameter_list|(
name|ArgumentParserException
name|e
parameter_list|)
block|{
name|parser
operator|.
name|handleError
argument_list|(
name|e
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
name|opts
operator|.
name|log4jConfigFile
operator|=
operator|(
name|File
operator|)
name|ns
operator|.
name|get
argument_list|(
name|log4jConfigFileArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|opts
operator|.
name|log4jConfigFile
operator|!=
literal|null
condition|)
block|{
name|Utils
operator|.
name|configureLog4jProperties
argument_list|(
name|opts
operator|.
name|log4jConfigFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Parsed command line args: {}"
argument_list|,
name|ns
argument_list|)
expr_stmt|;
name|opts
operator|.
name|inputLists
operator|=
name|ns
operator|.
name|getList
argument_list|(
name|inputListArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|opts
operator|.
name|inputLists
operator|==
literal|null
condition|)
block|{
name|opts
operator|.
name|inputLists
operator|=
name|Collections
operator|.
name|EMPTY_LIST
expr_stmt|;
block|}
name|opts
operator|.
name|inputFiles
operator|=
name|ns
operator|.
name|getList
argument_list|(
name|inputFilesArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|outputDir
operator|=
operator|(
name|Path
operator|)
name|ns
operator|.
name|get
argument_list|(
name|outputDirArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|mappers
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|mappersArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|reducers
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|reducersArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|updateConflictResolver
operator|=
name|ns
operator|.
name|getString
argument_list|(
name|updateConflictResolverArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|fanout
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|fanoutArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|maxSegments
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|maxSegmentsArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|morphlineFile
operator|=
operator|(
name|File
operator|)
name|ns
operator|.
name|get
argument_list|(
name|morphlineFileArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|morphlineId
operator|=
name|ns
operator|.
name|getString
argument_list|(
name|morphlineIdArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|solrHomeDir
operator|=
operator|(
name|File
operator|)
name|ns
operator|.
name|get
argument_list|(
name|solrHomeDirArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|fairSchedulerPool
operator|=
name|ns
operator|.
name|getString
argument_list|(
name|fairSchedulerPoolArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|isDryRun
operator|=
name|ns
operator|.
name|getBoolean
argument_list|(
name|dryRunArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|isVerbose
operator|=
name|ns
operator|.
name|getBoolean
argument_list|(
name|verboseArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|zkHost
operator|=
name|ns
operator|.
name|getString
argument_list|(
name|zkHostArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|shards
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|shardsArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|shardUrls
operator|=
name|buildShardUrls
argument_list|(
name|ns
operator|.
name|getList
argument_list|(
name|shardUrlsArg
operator|.
name|getDest
argument_list|()
argument_list|)
argument_list|,
name|opts
operator|.
name|shards
argument_list|)
expr_stmt|;
name|opts
operator|.
name|goLive
operator|=
name|ns
operator|.
name|getBoolean
argument_list|(
name|goLiveArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|goLiveThreads
operator|=
name|ns
operator|.
name|getInt
argument_list|(
name|goLiveThreadsArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
name|opts
operator|.
name|collection
operator|=
name|ns
operator|.
name|getString
argument_list|(
name|collectionArg
operator|.
name|getDest
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|opts
operator|.
name|reducers
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--reducers must not be zero"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
name|verifyGoLiveArgs
argument_list|(
name|opts
argument_list|,
name|parser
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ArgumentParserException
name|e
parameter_list|)
block|{
name|parser
operator|.
name|handleError
argument_list|(
name|e
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
if|if
condition|(
name|opts
operator|.
name|inputLists
operator|.
name|isEmpty
argument_list|()
operator|&&
name|opts
operator|.
name|inputFiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No input files specified - nothing to process"
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
comment|// nothing to process
block|}
return|return
literal|null
return|;
block|}
comment|// make it a "hidden" option, i.e. the option is functional and enabled but not shown in --help output
DECL|method|nonSolrCloud
specifier|private
name|Argument
name|nonSolrCloud
parameter_list|(
name|Argument
name|arg
parameter_list|)
block|{
return|return
name|showNonSolrCloud
condition|?
name|arg
else|:
name|arg
operator|.
name|help
argument_list|(
name|FeatureControl
operator|.
name|SUPPRESS
argument_list|)
return|;
block|}
DECL|method|nonSolrCloud
specifier|private
name|String
name|nonSolrCloud
parameter_list|(
name|String
name|msg
parameter_list|)
block|{
return|return
name|showNonSolrCloud
condition|?
name|msg
else|:
literal|""
return|;
block|}
comment|/** Marker trick to prevent processing of any remaining arguments once --help option has been parsed */
DECL|class|FoundHelpArgument
specifier|private
specifier|static
specifier|final
class|class
name|FoundHelpArgument
extends|extends
name|RuntimeException
block|{           }
block|}
comment|// END OF INNER CLASS
DECL|method|buildShardUrls
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|buildShardUrls
parameter_list|(
name|List
argument_list|<
name|Object
argument_list|>
name|urls
parameter_list|,
name|Integer
name|numShards
parameter_list|)
block|{
if|if
condition|(
name|urls
operator|==
literal|null
condition|)
return|return
literal|null
return|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|shardUrls
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|urls
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|list
init|=
literal|null
decl_stmt|;
name|int
name|sz
decl_stmt|;
if|if
condition|(
name|numShards
operator|==
literal|null
condition|)
block|{
name|numShards
operator|=
name|urls
operator|.
name|size
argument_list|()
expr_stmt|;
block|}
name|sz
operator|=
operator|(
name|int
operator|)
name|Math
operator|.
name|ceil
argument_list|(
name|urls
operator|.
name|size
argument_list|()
operator|/
operator|(
name|float
operator|)
name|numShards
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|urls
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|i
operator|%
name|sz
operator|==
literal|0
condition|)
block|{
name|list
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
name|shardUrls
operator|.
name|add
argument_list|(
name|list
argument_list|)
expr_stmt|;
block|}
name|list
operator|.
name|add
argument_list|(
operator|(
name|String
operator|)
name|urls
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|shardUrls
return|;
block|}
DECL|class|Options
specifier|static
specifier|final
class|class
name|Options
block|{
DECL|field|goLive
name|boolean
name|goLive
decl_stmt|;
DECL|field|collection
name|String
name|collection
decl_stmt|;
DECL|field|zkHost
name|String
name|zkHost
decl_stmt|;
DECL|field|goLiveThreads
name|Integer
name|goLiveThreads
decl_stmt|;
DECL|field|shardUrls
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|shardUrls
decl_stmt|;
DECL|field|inputLists
name|List
argument_list|<
name|Path
argument_list|>
name|inputLists
decl_stmt|;
DECL|field|inputFiles
name|List
argument_list|<
name|Path
argument_list|>
name|inputFiles
decl_stmt|;
DECL|field|outputDir
name|Path
name|outputDir
decl_stmt|;
DECL|field|mappers
name|int
name|mappers
decl_stmt|;
DECL|field|reducers
name|int
name|reducers
decl_stmt|;
DECL|field|updateConflictResolver
name|String
name|updateConflictResolver
decl_stmt|;
DECL|field|fanout
name|int
name|fanout
decl_stmt|;
DECL|field|shards
name|Integer
name|shards
decl_stmt|;
DECL|field|maxSegments
name|int
name|maxSegments
decl_stmt|;
DECL|field|morphlineFile
name|File
name|morphlineFile
decl_stmt|;
DECL|field|morphlineId
name|String
name|morphlineId
decl_stmt|;
DECL|field|solrHomeDir
name|File
name|solrHomeDir
decl_stmt|;
DECL|field|fairSchedulerPool
name|String
name|fairSchedulerPool
decl_stmt|;
DECL|field|isDryRun
name|boolean
name|isDryRun
decl_stmt|;
DECL|field|log4jConfigFile
name|File
name|log4jConfigFile
decl_stmt|;
DECL|field|isVerbose
name|boolean
name|isVerbose
decl_stmt|;
block|}
comment|// END OF INNER CLASS
comment|/** API for command line clients */
DECL|method|main
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|res
init|=
name|ToolRunner
operator|.
name|run
argument_list|(
operator|new
name|Configuration
argument_list|()
argument_list|,
operator|new
name|MapReduceIndexerTool
argument_list|()
argument_list|,
name|args
argument_list|)
decl_stmt|;
name|System
operator|.
name|exit
argument_list|(
name|res
argument_list|)
expr_stmt|;
block|}
DECL|method|MapReduceIndexerTool
specifier|public
name|MapReduceIndexerTool
parameter_list|()
block|{}
annotation|@
name|Override
DECL|method|run
specifier|public
name|int
name|run
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|Options
name|opts
init|=
operator|new
name|Options
argument_list|()
decl_stmt|;
name|Integer
name|exitCode
init|=
operator|new
name|MyArgumentParser
argument_list|()
operator|.
name|parseArgs
argument_list|(
name|args
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|opts
argument_list|)
decl_stmt|;
if|if
condition|(
name|exitCode
operator|!=
literal|null
condition|)
block|{
return|return
name|exitCode
return|;
block|}
return|return
name|run
argument_list|(
name|opts
argument_list|)
return|;
block|}
comment|/** API for Java clients; visible for testing; may become a public API eventually */
DECL|method|run
name|int
name|run
parameter_list|(
name|Options
name|options
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|getConf
argument_list|()
operator|.
name|getBoolean
argument_list|(
literal|"isMR1"
argument_list|,
literal|false
argument_list|)
operator|&&
literal|"local"
operator|.
name|equals
argument_list|(
name|getConf
argument_list|()
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|)
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Running with LocalJobRunner (i.e. all of Hadoop inside a single JVM) is not supported "
operator|+
literal|"because LocalJobRunner does not (yet) implement the Hadoop Distributed Cache feature, "
operator|+
literal|"which is required for passing files via --files and --libjars"
argument_list|)
throw|;
block|}
name|long
name|programStartTime
init|=
name|System
operator|.
name|nanoTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|options
operator|.
name|fairSchedulerPool
operator|!=
literal|null
condition|)
block|{
name|getConf
argument_list|()
operator|.
name|set
argument_list|(
literal|"mapred.fairscheduler.pool"
argument_list|,
name|options
operator|.
name|fairSchedulerPool
argument_list|)
expr_stmt|;
block|}
name|getConf
argument_list|()
operator|.
name|setInt
argument_list|(
name|SolrOutputFormat
operator|.
name|SOLR_RECORD_WRITER_MAX_SEGMENTS
argument_list|,
name|options
operator|.
name|maxSegments
argument_list|)
expr_stmt|;
comment|// switch off a false warning about allegedly not implementing Tool
comment|// also see http://hadoop.6.n7.nabble.com/GenericOptionsParser-warning-td8103.html
comment|// also see https://issues.apache.org/jira/browse/HADOOP-8183
name|getConf
argument_list|()
operator|.
name|setBoolean
argument_list|(
literal|"mapred.used.genericoptionsparser"
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|log4jConfigFile
operator|!=
literal|null
condition|)
block|{
name|Utils
operator|.
name|setLogConfigFile
argument_list|(
name|options
operator|.
name|log4jConfigFile
argument_list|,
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|addDistributedCacheFile
argument_list|(
name|options
operator|.
name|log4jConfigFile
argument_list|,
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|job
operator|=
name|Job
operator|.
name|getInstance
argument_list|(
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setJarByClass
argument_list|(
name|getClass
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|morphlineFile
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"Argument --morphline-file is required"
argument_list|,
literal|null
argument_list|)
throw|;
block|}
name|verifyGoLiveArgs
argument_list|(
name|options
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|verifyZKStructure
argument_list|(
name|options
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|int
name|mappers
init|=
operator|new
name|JobClient
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|getClusterStatus
argument_list|()
operator|.
name|getMaxMapTasks
argument_list|()
decl_stmt|;
comment|// MR1
comment|//int mappers = job.getCluster().getClusterStatus().getMapSlotCapacity(); // Yarn only
name|LOG
operator|.
name|info
argument_list|(
literal|"Cluster reports {} mapper slots"
argument_list|,
name|mappers
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|mappers
operator|==
operator|-
literal|1
condition|)
block|{
name|mappers
operator|=
literal|8
operator|*
name|mappers
expr_stmt|;
comment|// better accomodate stragglers
block|}
else|else
block|{
name|mappers
operator|=
name|options
operator|.
name|mappers
expr_stmt|;
block|}
if|if
condition|(
name|mappers
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal number of mappers: "
operator|+
name|mappers
argument_list|)
throw|;
block|}
name|options
operator|.
name|mappers
operator|=
name|mappers
expr_stmt|;
name|FileSystem
name|fs
init|=
name|options
operator|.
name|outputDir
operator|.
name|getFileSystem
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|options
operator|.
name|outputDir
argument_list|)
operator|&&
operator|!
name|delete
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|true
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|Path
name|outputResultsDir
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
name|RESULTS_DIR
argument_list|)
decl_stmt|;
name|Path
name|outputReduceDir
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|"reducers"
argument_list|)
decl_stmt|;
name|Path
name|outputStep1Dir
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|"tmp1"
argument_list|)
decl_stmt|;
name|Path
name|outputStep2Dir
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|"tmp2"
argument_list|)
decl_stmt|;
name|Path
name|outputTreeMergeStep
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|"mtree-merge-output"
argument_list|)
decl_stmt|;
name|Path
name|fullInputList
init|=
operator|new
name|Path
argument_list|(
name|outputStep1Dir
argument_list|,
name|FULL_INPUT_LIST
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Creating list of input files for mappers: {}"
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
name|long
name|numFiles
init|=
name|addInputFiles
argument_list|(
name|options
operator|.
name|inputFiles
argument_list|,
name|options
operator|.
name|inputLists
argument_list|,
name|fullInputList
argument_list|,
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|numFiles
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No input files found - nothing to process"
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
name|int
name|numLinesPerSplit
init|=
operator|(
name|int
operator|)
name|ceilDivide
argument_list|(
name|numFiles
argument_list|,
name|mappers
argument_list|)
decl_stmt|;
if|if
condition|(
name|numLinesPerSplit
operator|<
literal|0
condition|)
block|{
comment|// numeric overflow from downcasting long to int?
name|numLinesPerSplit
operator|=
name|Integer
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
name|numLinesPerSplit
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|1
argument_list|,
name|numLinesPerSplit
argument_list|)
expr_stmt|;
name|int
name|realMappers
init|=
name|Math
operator|.
name|min
argument_list|(
name|mappers
argument_list|,
operator|(
name|int
operator|)
name|ceilDivide
argument_list|(
name|numFiles
argument_list|,
name|numLinesPerSplit
argument_list|)
argument_list|)
decl_stmt|;
name|calculateNumReducers
argument_list|(
name|options
argument_list|,
name|realMappers
argument_list|)
expr_stmt|;
name|int
name|reducers
init|=
name|options
operator|.
name|reducers
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Using these parameters: "
operator|+
literal|"numFiles: {}, mappers: {}, realMappers: {}, reducers: {}, shards: {}, fanout: {}, maxSegments: {}"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|numFiles
block|,
name|mappers
block|,
name|realMappers
block|,
name|reducers
block|,
name|options
operator|.
name|shards
block|,
name|options
operator|.
name|fanout
block|,
name|options
operator|.
name|maxSegments
block|}
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Randomizing list of {} input files to spread indexing load more evenly among mappers"
argument_list|,
name|numFiles
argument_list|)
expr_stmt|;
name|long
name|startTime
init|=
name|System
operator|.
name|nanoTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|numFiles
operator|<
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|getInt
argument_list|(
name|MAIN_MEMORY_RANDOMIZATION_THRESHOLD
argument_list|,
literal|100001
argument_list|)
condition|)
block|{
comment|// If there are few input files reduce latency by directly running main memory randomization
comment|// instead of launching a high latency MapReduce job
name|randomizeFewInputFiles
argument_list|(
name|fs
argument_list|,
name|outputStep2Dir
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Randomize using a MapReduce job. Use sequential algorithm below a certain threshold because there's no
comment|// benefit in using many parallel mapper tasks just to randomize the order of a few lines each
name|int
name|numLinesPerRandomizerSplit
init|=
name|Math
operator|.
name|max
argument_list|(
literal|10
operator|*
literal|1000
operator|*
literal|1000
argument_list|,
name|numLinesPerSplit
argument_list|)
decl_stmt|;
name|Job
name|randomizerJob
init|=
name|randomizeManyInputFiles
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|fullInputList
argument_list|,
name|outputStep2Dir
argument_list|,
name|numLinesPerRandomizerSplit
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|waitForCompletion
argument_list|(
name|randomizerJob
argument_list|,
name|options
operator|.
name|isVerbose
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
comment|// job failed
block|}
block|}
name|float
name|secs
init|=
operator|(
name|System
operator|.
name|nanoTime
argument_list|()
operator|-
name|startTime
operator|)
operator|/
call|(
name|float
call|)
argument_list|(
literal|10
operator|^
literal|9
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Done. Randomizing list of {} input files took {} secs"
argument_list|,
name|numFiles
argument_list|,
name|secs
argument_list|)
expr_stmt|;
name|job
operator|.
name|setInputFormatClass
argument_list|(
name|NLineInputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
name|NLineInputFormat
operator|.
name|addInputPath
argument_list|(
name|job
argument_list|,
name|outputStep2Dir
argument_list|)
expr_stmt|;
name|NLineInputFormat
operator|.
name|setNumLinesPerSplit
argument_list|(
name|job
argument_list|,
name|numLinesPerSplit
argument_list|)
expr_stmt|;
name|FileOutputFormat
operator|.
name|setOutputPath
argument_list|(
name|job
argument_list|,
name|outputReduceDir
argument_list|)
expr_stmt|;
name|String
name|mapperClass
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|JobContext
operator|.
name|MAP_CLASS_ATTR
argument_list|)
decl_stmt|;
if|if
condition|(
name|mapperClass
operator|==
literal|null
condition|)
block|{
comment|// enable customization
name|Class
name|clazz
init|=
name|MorphlineMapper
operator|.
name|class
decl_stmt|;
name|mapperClass
operator|=
name|clazz
operator|.
name|getName
argument_list|()
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|clazz
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setJobName
argument_list|(
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"/"
operator|+
name|Utils
operator|.
name|getShortClassName
argument_list|(
name|mapperClass
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|JobContext
operator|.
name|REDUCE_CLASS_ATTR
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// enable customization
name|job
operator|.
name|setReducerClass
argument_list|(
name|SolrReducer
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|options
operator|.
name|updateConflictResolver
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"updateConflictResolver must not be null"
argument_list|)
throw|;
block|}
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|SolrReducer
operator|.
name|UPDATE_CONFLICT_RESOLVER
argument_list|,
name|options
operator|.
name|updateConflictResolver
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|zkHost
operator|!=
literal|null
condition|)
block|{
assert|assert
name|options
operator|.
name|collection
operator|!=
literal|null
assert|;
comment|/*        * MapReduce partitioner that partitions the Mapper output such that each        * SolrInputDocument gets sent to the SolrCloud shard that it would have        * been sent to if the document were ingested via the standard SolrCloud        * Near Real Time (NRT) API.        *         * In other words, this class implements the same partitioning semantics        * as the standard SolrCloud NRT API. This enables to mix batch updates        * from MapReduce ingestion with updates from standard NRT ingestion on        * the same SolrCloud cluster, using identical unique document keys.        */
if|if
condition|(
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|JobContext
operator|.
name|PARTITIONER_CLASS_ATTR
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// enable customization
name|job
operator|.
name|setPartitionerClass
argument_list|(
name|SolrCloudPartitioner
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|SolrCloudPartitioner
operator|.
name|ZKHOST
argument_list|,
name|options
operator|.
name|zkHost
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|SolrCloudPartitioner
operator|.
name|COLLECTION
argument_list|,
name|options
operator|.
name|collection
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|setInt
argument_list|(
name|SolrCloudPartitioner
operator|.
name|SHARDS
argument_list|,
name|options
operator|.
name|shards
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputFormatClass
argument_list|(
name|SolrOutputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|solrHomeDir
operator|!=
literal|null
condition|)
block|{
name|SolrOutputFormat
operator|.
name|setupSolrHomeCache
argument_list|(
name|options
operator|.
name|solrHomeDir
argument_list|,
name|job
argument_list|)
expr_stmt|;
block|}
else|else
block|{
assert|assert
name|options
operator|.
name|zkHost
operator|!=
literal|null
assert|;
comment|// use the config that this collection uses for the SolrHomeCache.
name|ZooKeeperInspector
name|zki
init|=
operator|new
name|ZooKeeperInspector
argument_list|()
decl_stmt|;
name|SolrZkClient
name|zkClient
init|=
name|zki
operator|.
name|getZkClient
argument_list|(
name|options
operator|.
name|zkHost
argument_list|)
decl_stmt|;
try|try
block|{
name|String
name|configName
init|=
name|zki
operator|.
name|readConfigName
argument_list|(
name|zkClient
argument_list|,
name|options
operator|.
name|collection
argument_list|)
decl_stmt|;
name|File
name|tmpSolrHomeDir
init|=
name|zki
operator|.
name|downloadConfigDir
argument_list|(
name|zkClient
argument_list|,
name|configName
argument_list|)
decl_stmt|;
name|SolrOutputFormat
operator|.
name|setupSolrHomeCache
argument_list|(
name|tmpSolrHomeDir
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|options
operator|.
name|solrHomeDir
operator|=
name|tmpSolrHomeDir
expr_stmt|;
block|}
finally|finally
block|{
name|zkClient
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|MorphlineMapRunner
name|runner
init|=
name|setupMorphline
argument_list|(
name|options
argument_list|)
decl_stmt|;
if|if
condition|(
name|options
operator|.
name|isDryRun
operator|&&
name|runner
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Indexing {} files in dryrun mode"
argument_list|,
name|numFiles
argument_list|)
expr_stmt|;
name|startTime
operator|=
name|System
operator|.
name|nanoTime
argument_list|()
expr_stmt|;
name|dryRun
argument_list|(
name|runner
argument_list|,
name|fs
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
name|secs
operator|=
operator|(
name|System
operator|.
name|nanoTime
argument_list|()
operator|-
name|startTime
operator|)
operator|/
call|(
name|float
call|)
argument_list|(
literal|10
operator|^
literal|9
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Done. Indexing {} files in dryrun mode took {} secs"
argument_list|,
name|numFiles
argument_list|,
name|secs
argument_list|)
expr_stmt|;
name|goodbye
argument_list|(
literal|null
argument_list|,
name|programStartTime
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|MorphlineMapRunner
operator|.
name|MORPHLINE_FILE_PARAM
argument_list|,
name|options
operator|.
name|morphlineFile
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|reducers
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|SolrInputDocumentWritable
operator|.
name|class
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Indexing {} files using {} real mappers into {} reducers"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|numFiles
block|,
name|realMappers
block|,
name|reducers
block|}
argument_list|)
expr_stmt|;
name|startTime
operator|=
name|System
operator|.
name|nanoTime
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|waitForCompletion
argument_list|(
name|job
argument_list|,
name|options
operator|.
name|isVerbose
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
comment|// job failed
block|}
name|secs
operator|=
operator|(
name|System
operator|.
name|nanoTime
argument_list|()
operator|-
name|startTime
operator|)
operator|/
call|(
name|float
call|)
argument_list|(
literal|10
operator|^
literal|9
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Done. Indexing {} files using {} real mappers into {} reducers took {} secs"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|numFiles
block|,
name|realMappers
block|,
name|reducers
block|,
name|secs
block|}
argument_list|)
expr_stmt|;
name|int
name|mtreeMergeIterations
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|reducers
operator|>
name|options
operator|.
name|shards
condition|)
block|{
name|mtreeMergeIterations
operator|=
operator|(
name|int
operator|)
name|Math
operator|.
name|round
argument_list|(
name|log
argument_list|(
name|options
operator|.
name|fanout
argument_list|,
name|reducers
operator|/
name|options
operator|.
name|shards
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"MTree merge iterations to do: {}"
argument_list|,
name|mtreeMergeIterations
argument_list|)
expr_stmt|;
name|int
name|mtreeMergeIteration
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|reducers
operator|>
name|options
operator|.
name|shards
condition|)
block|{
comment|// run a mtree merge iteration
name|job
operator|=
name|Job
operator|.
name|getInstance
argument_list|(
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setJarByClass
argument_list|(
name|getClass
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setJobName
argument_list|(
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"/"
operator|+
name|Utils
operator|.
name|getShortClassName
argument_list|(
name|TreeMergeMapper
operator|.
name|class
argument_list|)
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|TreeMergeMapper
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputFormatClass
argument_list|(
name|TreeMergeOutputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|NullWritable
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setInputFormatClass
argument_list|(
name|NLineInputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
name|Path
name|inputStepDir
init|=
operator|new
name|Path
argument_list|(
name|options
operator|.
name|outputDir
argument_list|,
literal|"mtree-merge-input-iteration"
operator|+
name|mtreeMergeIteration
argument_list|)
decl_stmt|;
name|fullInputList
operator|=
operator|new
name|Path
argument_list|(
name|inputStepDir
argument_list|,
name|FULL_INPUT_LIST
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"MTree merge iteration {}/{}: Creating input list file for mappers {}"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|mtreeMergeIteration
block|,
name|mtreeMergeIterations
block|,
name|fullInputList
block|}
argument_list|)
expr_stmt|;
name|numFiles
operator|=
name|createTreeMergeInputDirList
argument_list|(
name|outputReduceDir
argument_list|,
name|fs
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
if|if
condition|(
name|numFiles
operator|!=
name|reducers
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not same reducers: "
operator|+
name|reducers
operator|+
literal|", numFiles: "
operator|+
name|numFiles
argument_list|)
throw|;
block|}
name|NLineInputFormat
operator|.
name|addInputPath
argument_list|(
name|job
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
name|NLineInputFormat
operator|.
name|setNumLinesPerSplit
argument_list|(
name|job
argument_list|,
name|options
operator|.
name|fanout
argument_list|)
expr_stmt|;
name|FileOutputFormat
operator|.
name|setOutputPath
argument_list|(
name|job
argument_list|,
name|outputTreeMergeStep
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"MTree merge iteration {}/{}: Merging {} shards into {} shards using fanout {}"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|mtreeMergeIteration
block|,
name|mtreeMergeIterations
block|,
name|reducers
block|,
operator|(
name|reducers
operator|/
name|options
operator|.
name|fanout
operator|)
block|,
name|options
operator|.
name|fanout
block|}
argument_list|)
expr_stmt|;
name|startTime
operator|=
name|System
operator|.
name|nanoTime
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|waitForCompletion
argument_list|(
name|job
argument_list|,
name|options
operator|.
name|isVerbose
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
comment|// job failed
block|}
if|if
condition|(
operator|!
name|renameTreeMergeShardDirs
argument_list|(
name|outputTreeMergeStep
argument_list|,
name|job
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|secs
operator|=
operator|(
name|System
operator|.
name|nanoTime
argument_list|()
operator|-
name|startTime
operator|)
operator|/
call|(
name|float
call|)
argument_list|(
literal|10
operator|^
literal|9
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"MTree merge iteration {}/{}: Done. Merging {} shards into {} shards using fanout {} took {} secs"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|mtreeMergeIteration
block|,
name|mtreeMergeIterations
block|,
name|reducers
block|,
operator|(
name|reducers
operator|/
name|options
operator|.
name|fanout
operator|)
block|,
name|options
operator|.
name|fanout
block|,
name|secs
block|}
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|delete
argument_list|(
name|outputReduceDir
argument_list|,
literal|true
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
if|if
condition|(
operator|!
name|rename
argument_list|(
name|outputTreeMergeStep
argument_list|,
name|outputReduceDir
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
assert|assert
name|reducers
operator|%
name|options
operator|.
name|fanout
operator|==
literal|0
assert|;
name|reducers
operator|=
name|reducers
operator|/
name|options
operator|.
name|fanout
expr_stmt|;
name|mtreeMergeIteration
operator|++
expr_stmt|;
block|}
assert|assert
name|reducers
operator|==
name|options
operator|.
name|shards
assert|;
comment|// normalize output shard dir prefix, i.e.
comment|// rename part-r-00000 to part-00000 (stems from zero tree merge iterations)
comment|// rename part-m-00000 to part-00000 (stems from> 0 tree merge iterations)
for|for
control|(
name|FileStatus
name|stats
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|outputReduceDir
argument_list|)
control|)
block|{
name|String
name|dirPrefix
init|=
name|SolrOutputFormat
operator|.
name|getOutputName
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|Path
name|srcPath
init|=
name|stats
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|stats
operator|.
name|isDirectory
argument_list|()
operator|&&
name|srcPath
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|dirPrefix
argument_list|)
condition|)
block|{
name|String
name|dstName
init|=
name|dirPrefix
operator|+
name|srcPath
operator|.
name|getName
argument_list|()
operator|.
name|substring
argument_list|(
name|dirPrefix
operator|.
name|length
argument_list|()
operator|+
literal|"-m"
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|dstPath
init|=
operator|new
name|Path
argument_list|(
name|srcPath
operator|.
name|getParent
argument_list|()
argument_list|,
name|dstName
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|rename
argument_list|(
name|srcPath
argument_list|,
name|dstPath
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
block|}
block|}
empty_stmt|;
comment|// publish results dir
if|if
condition|(
operator|!
name|rename
argument_list|(
name|outputReduceDir
argument_list|,
name|outputResultsDir
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
if|if
condition|(
name|options
operator|.
name|goLive
operator|&&
operator|!
operator|new
name|GoLive
argument_list|()
operator|.
name|goLive
argument_list|(
name|options
argument_list|,
name|listSortedOutputShardDirs
argument_list|(
name|outputResultsDir
argument_list|,
name|fs
argument_list|)
argument_list|)
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|goodbye
argument_list|(
name|job
argument_list|,
name|programStartTime
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
DECL|method|calculateNumReducers
specifier|private
name|void
name|calculateNumReducers
parameter_list|(
name|Options
name|options
parameter_list|,
name|int
name|realMappers
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|options
operator|.
name|shards
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal number of shards: "
operator|+
name|options
operator|.
name|shards
argument_list|)
throw|;
block|}
if|if
condition|(
name|options
operator|.
name|fanout
operator|<=
literal|1
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal fanout: "
operator|+
name|options
operator|.
name|fanout
argument_list|)
throw|;
block|}
if|if
condition|(
name|realMappers
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal realMappers: "
operator|+
name|realMappers
argument_list|)
throw|;
block|}
name|int
name|reducers
init|=
operator|new
name|JobClient
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|getClusterStatus
argument_list|()
operator|.
name|getMaxReduceTasks
argument_list|()
decl_stmt|;
comment|// MR1
comment|//reducers = job.getCluster().getClusterStatus().getReduceSlotCapacity(); // Yarn only
name|LOG
operator|.
name|info
argument_list|(
literal|"Cluster reports {} reduce slots"
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|reducers
operator|==
operator|-
literal|2
condition|)
block|{
name|reducers
operator|=
name|options
operator|.
name|shards
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|options
operator|.
name|reducers
operator|==
operator|-
literal|1
condition|)
block|{
name|reducers
operator|=
name|Math
operator|.
name|min
argument_list|(
name|reducers
argument_list|,
name|realMappers
argument_list|)
expr_stmt|;
comment|// no need to use many reducers when using few mappers
block|}
else|else
block|{
if|if
condition|(
name|options
operator|.
name|reducers
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal zero reducers"
argument_list|)
throw|;
block|}
name|reducers
operator|=
name|options
operator|.
name|reducers
expr_stmt|;
block|}
name|reducers
operator|=
name|Math
operator|.
name|max
argument_list|(
name|reducers
argument_list|,
name|options
operator|.
name|shards
argument_list|)
expr_stmt|;
if|if
condition|(
name|reducers
operator|!=
name|options
operator|.
name|shards
condition|)
block|{
comment|// Ensure fanout isn't misconfigured. fanout can't meaningfully be larger than what would be
comment|// required to merge all leaf shards in one single tree merge iteration into root shards
name|options
operator|.
name|fanout
operator|=
name|Math
operator|.
name|min
argument_list|(
name|options
operator|.
name|fanout
argument_list|,
operator|(
name|int
operator|)
name|ceilDivide
argument_list|(
name|reducers
argument_list|,
name|options
operator|.
name|shards
argument_list|)
argument_list|)
expr_stmt|;
comment|// Ensure invariant reducers == options.shards * (fanout ^ N) where N is an integer>= 1.
comment|// N is the number of mtree merge iterations.
comment|// This helps to evenly spread docs among root shards and simplifies the impl of the mtree merge algorithm.
name|int
name|s
init|=
name|options
operator|.
name|shards
decl_stmt|;
while|while
condition|(
name|s
operator|<
name|reducers
condition|)
block|{
name|s
operator|=
name|s
operator|*
name|options
operator|.
name|fanout
expr_stmt|;
block|}
name|reducers
operator|=
name|s
expr_stmt|;
assert|assert
name|reducers
operator|%
name|options
operator|.
name|fanout
operator|==
literal|0
assert|;
block|}
name|options
operator|.
name|reducers
operator|=
name|reducers
expr_stmt|;
block|}
DECL|method|addInputFiles
specifier|private
name|long
name|addInputFiles
parameter_list|(
name|List
argument_list|<
name|Path
argument_list|>
name|inputFiles
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|inputLists
parameter_list|,
name|Path
name|fullInputList
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|numFiles
init|=
literal|0
decl_stmt|;
name|FileSystem
name|fs
init|=
name|fullInputList
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
name|fullInputList
argument_list|)
decl_stmt|;
try|try
block|{
name|Writer
name|writer
init|=
operator|new
name|BufferedWriter
argument_list|(
operator|new
name|OutputStreamWriter
argument_list|(
name|out
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|inputFile
range|:
name|inputFiles
control|)
block|{
name|FileSystem
name|inputFileFs
init|=
name|inputFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFileFs
operator|.
name|exists
argument_list|(
name|inputFile
argument_list|)
condition|)
block|{
name|PathFilter
name|pathFilter
init|=
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|// ignore "hidden" files and dirs
return|return
operator|!
operator|(
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"."
argument_list|)
operator|||
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"_"
argument_list|)
operator|)
return|;
block|}
block|}
decl_stmt|;
name|numFiles
operator|+=
name|addInputFilesRecursively
argument_list|(
name|inputFile
argument_list|,
name|writer
argument_list|,
name|inputFileFs
argument_list|,
name|pathFilter
argument_list|)
expr_stmt|;
block|}
block|}
for|for
control|(
name|Path
name|inputList
range|:
name|inputLists
control|)
block|{
name|InputStream
name|in
decl_stmt|;
if|if
condition|(
name|inputList
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
literal|"-"
argument_list|)
condition|)
block|{
name|in
operator|=
name|System
operator|.
name|in
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|inputList
operator|.
name|isAbsoluteAndSchemeAuthorityNull
argument_list|()
condition|)
block|{
name|in
operator|=
operator|new
name|BufferedInputStream
argument_list|(
operator|new
name|FileInputStream
argument_list|(
name|inputList
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|in
operator|=
name|inputList
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
operator|.
name|open
argument_list|(
name|inputList
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|BufferedReader
name|reader
init|=
operator|new
name|BufferedReader
argument_list|(
operator|new
name|InputStreamReader
argument_list|(
name|in
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
name|String
name|line
decl_stmt|;
while|while
condition|(
operator|(
name|line
operator|=
name|reader
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|writer
operator|.
name|write
argument_list|(
name|line
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
name|numFiles
operator|++
expr_stmt|;
block|}
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
return|return
name|numFiles
return|;
block|}
comment|/**    * Add the specified file to the input set, if path is a directory then    * add the files contained therein.    */
DECL|method|addInputFilesRecursively
specifier|private
name|long
name|addInputFilesRecursively
parameter_list|(
name|Path
name|path
parameter_list|,
name|Writer
name|writer
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|PathFilter
name|pathFilter
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|numFiles
init|=
literal|0
decl_stmt|;
for|for
control|(
name|FileStatus
name|stat
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|,
name|pathFilter
argument_list|)
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding path {}"
argument_list|,
name|stat
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|stat
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
name|numFiles
operator|+=
name|addInputFilesRecursively
argument_list|(
name|stat
operator|.
name|getPath
argument_list|()
argument_list|,
name|writer
argument_list|,
name|fs
argument_list|,
name|pathFilter
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|writer
operator|.
name|write
argument_list|(
name|stat
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
name|numFiles
operator|++
expr_stmt|;
block|}
block|}
return|return
name|numFiles
return|;
block|}
DECL|method|randomizeFewInputFiles
specifier|private
name|void
name|randomizeFewInputFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|outputStep2Dir
parameter_list|,
name|Path
name|fullInputList
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|lines
init|=
operator|new
name|ArrayList
argument_list|()
decl_stmt|;
name|BufferedReader
name|reader
init|=
operator|new
name|BufferedReader
argument_list|(
operator|new
name|InputStreamReader
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|fullInputList
argument_list|)
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|String
name|line
decl_stmt|;
while|while
condition|(
operator|(
name|line
operator|=
name|reader
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|lines
operator|.
name|add
argument_list|(
name|line
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|Collections
operator|.
name|shuffle
argument_list|(
name|lines
argument_list|,
operator|new
name|Random
argument_list|(
literal|421439783L
argument_list|)
argument_list|)
expr_stmt|;
comment|// constant seed for reproducability
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
operator|new
name|Path
argument_list|(
name|outputStep2Dir
argument_list|,
name|FULL_INPUT_LIST
argument_list|)
argument_list|)
decl_stmt|;
name|Writer
name|writer
init|=
operator|new
name|BufferedWriter
argument_list|(
operator|new
name|OutputStreamWriter
argument_list|(
name|out
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|String
name|line
range|:
name|lines
control|)
block|{
name|writer
operator|.
name|write
argument_list|(
name|line
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * To uniformly spread load across all mappers we randomize fullInputList    * with a separate small Mapper& Reducer preprocessing step. This way    * each input line ends up on a random position in the output file list.    * Each mapper indexes a disjoint consecutive set of files such that each    * set has roughly the same size, at least from a probabilistic    * perspective.    *     * For example an input file with the following input list of URLs:    *     * A    * B    * C    * D    *     * might be randomized into the following output list of URLs:    *     * C    * A    * D    * B    *     * The implementation sorts the list of lines by randomly generated numbers.    */
DECL|method|randomizeManyInputFiles
specifier|private
name|Job
name|randomizeManyInputFiles
parameter_list|(
name|Configuration
name|baseConfig
parameter_list|,
name|Path
name|fullInputList
parameter_list|,
name|Path
name|outputStep2Dir
parameter_list|,
name|int
name|numLinesPerSplit
parameter_list|)
throws|throws
name|IOException
block|{
name|Job
name|job2
init|=
name|Job
operator|.
name|getInstance
argument_list|(
name|baseConfig
argument_list|)
decl_stmt|;
name|job2
operator|.
name|setJarByClass
argument_list|(
name|getClass
argument_list|()
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setJobName
argument_list|(
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"/"
operator|+
name|Utils
operator|.
name|getShortClassName
argument_list|(
name|LineRandomizerMapper
operator|.
name|class
argument_list|)
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setInputFormatClass
argument_list|(
name|NLineInputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
name|NLineInputFormat
operator|.
name|addInputPath
argument_list|(
name|job2
argument_list|,
name|fullInputList
argument_list|)
expr_stmt|;
name|NLineInputFormat
operator|.
name|setNumLinesPerSplit
argument_list|(
name|job2
argument_list|,
name|numLinesPerSplit
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setMapperClass
argument_list|(
name|LineRandomizerMapper
operator|.
name|class
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setReducerClass
argument_list|(
name|LineRandomizerReducer
operator|.
name|class
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setOutputFormatClass
argument_list|(
name|TextOutputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
name|FileOutputFormat
operator|.
name|setOutputPath
argument_list|(
name|job2
argument_list|,
name|outputStep2Dir
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setNumReduceTasks
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setOutputKeyClass
argument_list|(
name|LongWritable
operator|.
name|class
argument_list|)
expr_stmt|;
name|job2
operator|.
name|setOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
return|return
name|job2
return|;
block|}
comment|// do the same as if the user had typed 'hadoop ... --files<file>'
DECL|method|addDistributedCacheFile
specifier|private
name|void
name|addDistributedCacheFile
parameter_list|(
name|File
name|file
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|HADOOP_TMP_FILES
init|=
literal|"tmpfiles"
decl_stmt|;
comment|// see Hadoop's GenericOptionsParser
name|String
name|tmpFiles
init|=
name|conf
operator|.
name|get
argument_list|(
name|HADOOP_TMP_FILES
argument_list|,
literal|""
argument_list|)
decl_stmt|;
if|if
condition|(
name|tmpFiles
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// already present?
name|tmpFiles
operator|=
name|tmpFiles
operator|+
literal|","
expr_stmt|;
block|}
name|GenericOptionsParser
name|parser
init|=
operator|new
name|GenericOptionsParser
argument_list|(
operator|new
name|Configuration
argument_list|(
name|conf
argument_list|)
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"--files"
block|,
name|file
operator|.
name|getCanonicalPath
argument_list|()
block|}
argument_list|)
decl_stmt|;
name|String
name|additionalTmpFiles
init|=
name|parser
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HADOOP_TMP_FILES
argument_list|)
decl_stmt|;
assert|assert
name|additionalTmpFiles
operator|!=
literal|null
assert|;
assert|assert
name|additionalTmpFiles
operator|.
name|length
argument_list|()
operator|>
literal|0
assert|;
name|tmpFiles
operator|+=
name|additionalTmpFiles
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|HADOOP_TMP_FILES
argument_list|,
name|tmpFiles
argument_list|)
expr_stmt|;
block|}
DECL|method|setupMorphline
specifier|private
name|MorphlineMapRunner
name|setupMorphline
parameter_list|(
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
throws|,
name|URISyntaxException
block|{
if|if
condition|(
name|options
operator|.
name|morphlineId
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|MorphlineMapRunner
operator|.
name|MORPHLINE_ID_PARAM
argument_list|,
name|options
operator|.
name|morphlineId
argument_list|)
expr_stmt|;
block|}
name|addDistributedCacheFile
argument_list|(
name|options
operator|.
name|morphlineFile
argument_list|,
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|options
operator|.
name|isDryRun
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|/*      * Ensure scripting support for Java via morphline "java" command works even in dryRun mode,      * i.e. when executed in the client side driver JVM. To do so, collect all classpath URLs from      * the class loaders chain that org.apache.hadoop.util.RunJar (hadoop jar xyz-job.jar) and      * org.apache.hadoop.util.GenericOptionsParser (--libjars) have installed, then tell      * FastJavaScriptEngine.parse() where to find classes that JavaBuilder scripts might depend on.      * This ensures that scripts that reference external java classes compile without exceptions      * like this:      *       * ... caused by compilation failed: mfm:///MyJavaClass1.java:2: package      * org.kitesdk.morphline.api does not exist      */
name|LOG
operator|.
name|trace
argument_list|(
literal|"dryRun: java.class.path: {}"
argument_list|,
name|System
operator|.
name|getProperty
argument_list|(
literal|"java.class.path"
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|fullClassPath
init|=
literal|""
decl_stmt|;
name|ClassLoader
name|loader
init|=
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|getContextClassLoader
argument_list|()
decl_stmt|;
comment|// see org.apache.hadoop.util.RunJar
while|while
condition|(
name|loader
operator|!=
literal|null
condition|)
block|{
comment|// walk class loaders, collect all classpath URLs
if|if
condition|(
name|loader
operator|instanceof
name|URLClassLoader
condition|)
block|{
name|URL
index|[]
name|classPathPartURLs
init|=
operator|(
operator|(
name|URLClassLoader
operator|)
name|loader
operator|)
operator|.
name|getURLs
argument_list|()
decl_stmt|;
comment|// see org.apache.hadoop.util.RunJar
name|LOG
operator|.
name|trace
argument_list|(
literal|"dryRun: classPathPartURLs: {}"
argument_list|,
name|Arrays
operator|.
name|asList
argument_list|(
name|classPathPartURLs
argument_list|)
argument_list|)
expr_stmt|;
name|StringBuilder
name|classPathParts
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|URL
name|url
range|:
name|classPathPartURLs
control|)
block|{
name|File
name|file
init|=
operator|new
name|File
argument_list|(
name|url
operator|.
name|toURI
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|classPathPartURLs
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|classPathParts
operator|.
name|append
argument_list|(
name|File
operator|.
name|pathSeparator
argument_list|)
expr_stmt|;
block|}
name|classPathParts
operator|.
name|append
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|trace
argument_list|(
literal|"dryRun: classPathParts: {}"
argument_list|,
name|classPathParts
argument_list|)
expr_stmt|;
name|String
name|separator
init|=
name|File
operator|.
name|pathSeparator
decl_stmt|;
if|if
condition|(
name|fullClassPath
operator|.
name|length
argument_list|()
operator|==
literal|0
operator|||
name|classPathParts
operator|.
name|length
argument_list|()
operator|==
literal|0
condition|)
block|{
name|separator
operator|=
literal|""
expr_stmt|;
block|}
name|fullClassPath
operator|=
name|classPathParts
operator|+
name|separator
operator|+
name|fullClassPath
expr_stmt|;
block|}
name|loader
operator|=
name|loader
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
comment|// tell FastJavaScriptEngine.parse() where to find the classes that the script might depend on
if|if
condition|(
name|fullClassPath
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
assert|assert
name|System
operator|.
name|getProperty
argument_list|(
literal|"java.class.path"
argument_list|)
operator|!=
literal|null
assert|;
name|fullClassPath
operator|=
name|System
operator|.
name|getProperty
argument_list|(
literal|"java.class.path"
argument_list|)
operator|+
name|File
operator|.
name|pathSeparator
operator|+
name|fullClassPath
expr_stmt|;
name|LOG
operator|.
name|trace
argument_list|(
literal|"dryRun: fullClassPath: {}"
argument_list|,
name|fullClassPath
argument_list|)
expr_stmt|;
name|System
operator|.
name|setProperty
argument_list|(
literal|"java.class.path"
argument_list|,
name|fullClassPath
argument_list|)
expr_stmt|;
comment|// see FastJavaScriptEngine.parse()
block|}
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|MorphlineMapRunner
operator|.
name|MORPHLINE_FILE_PARAM
argument_list|,
name|options
operator|.
name|morphlineFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|new
name|MorphlineMapRunner
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|,
operator|new
name|DryRunDocumentLoader
argument_list|()
argument_list|,
name|options
operator|.
name|solrHomeDir
operator|.
name|getPath
argument_list|()
argument_list|)
return|;
block|}
comment|/*    * Executes the morphline in the current process (without submitting a job to MR) for quicker    * turnaround during trial& debug sessions    */
DECL|method|dryRun
specifier|private
name|void
name|dryRun
parameter_list|(
name|MorphlineMapRunner
name|runner
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|fullInputList
parameter_list|)
throws|throws
name|IOException
block|{
name|BufferedReader
name|reader
init|=
operator|new
name|BufferedReader
argument_list|(
operator|new
name|InputStreamReader
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|fullInputList
argument_list|)
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|String
name|line
decl_stmt|;
while|while
condition|(
operator|(
name|line
operator|=
name|reader
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|runner
operator|.
name|map
argument_list|(
name|line
argument_list|,
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
name|runner
operator|.
name|cleanup
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
DECL|method|createTreeMergeInputDirList
specifier|private
name|int
name|createTreeMergeInputDirList
parameter_list|(
name|Path
name|outputReduceDir
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|fullInputList
parameter_list|)
throws|throws
name|FileNotFoundException
throws|,
name|IOException
block|{
name|FileStatus
index|[]
name|dirs
init|=
name|listSortedOutputShardDirs
argument_list|(
name|outputReduceDir
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|int
name|numFiles
init|=
literal|0
decl_stmt|;
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
name|fullInputList
argument_list|)
decl_stmt|;
try|try
block|{
name|Writer
name|writer
init|=
operator|new
name|BufferedWriter
argument_list|(
operator|new
name|OutputStreamWriter
argument_list|(
name|out
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|stat
range|:
name|dirs
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding path {}"
argument_list|,
name|stat
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|Path
name|dir
init|=
operator|new
name|Path
argument_list|(
name|stat
operator|.
name|getPath
argument_list|()
argument_list|,
literal|"data/index"
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|isDirectory
argument_list|(
name|dir
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not a directory: "
operator|+
name|dir
argument_list|)
throw|;
block|}
name|writer
operator|.
name|write
argument_list|(
name|dir
operator|.
name|toString
argument_list|()
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
name|numFiles
operator|++
expr_stmt|;
block|}
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
return|return
name|numFiles
return|;
block|}
DECL|method|listSortedOutputShardDirs
specifier|private
name|FileStatus
index|[]
name|listSortedOutputShardDirs
parameter_list|(
name|Path
name|outputReduceDir
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|FileNotFoundException
throws|,
name|IOException
block|{
specifier|final
name|String
name|dirPrefix
init|=
name|SolrOutputFormat
operator|.
name|getOutputName
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|dirs
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|outputReduceDir
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|dirPrefix
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirs
control|)
block|{
if|if
condition|(
operator|!
name|dir
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not a directory: "
operator|+
name|dir
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
comment|// use alphanumeric sort (rather than lexicographical sort) to properly handle more than 99999 shards
name|Arrays
operator|.
name|sort
argument_list|(
name|dirs
argument_list|,
operator|new
name|Comparator
argument_list|<
name|FileStatus
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|int
name|compare
parameter_list|(
name|FileStatus
name|f1
parameter_list|,
name|FileStatus
name|f2
parameter_list|)
block|{
return|return
operator|new
name|AlphaNumericComparator
argument_list|()
operator|.
name|compare
argument_list|(
name|f1
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|,
name|f2
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
block|}
argument_list|)
expr_stmt|;
return|return
name|dirs
return|;
block|}
comment|/*    * You can run MapReduceIndexerTool in Solrcloud mode, and once the MR job completes, you can use    * the standard solrj Solrcloud API to send doc updates and deletes to SolrCloud, and those updates    * and deletes will go to the right Solr shards, and it will work just fine.    *     * The MapReduce framework doesn't guarantee that input split N goes to the map task with the    * taskId = N. The job tracker and Yarn schedule and assign tasks, considering data locality    * aspects, but without regard of the input split# withing the overall list of input splits. In    * other words, split# != taskId can be true.    *     * To deal with this issue, our mapper tasks write a little auxiliary metadata file (per task)    * that tells the job driver which taskId processed which split#. Once the mapper-only job is    * completed, the job driver renames the output dirs such that the dir name contains the true solr    * shard id, based on these auxiliary files.    *     * This way each doc gets assigned to the right Solr shard even with #reducers> #solrshards    *     * Example for a merge with two shards:    *     * part-m-00000 and part-m-00001 goes to outputShardNum = 0 and will end up in merged part-m-00000    * part-m-00002 and part-m-00003 goes to outputShardNum = 1 and will end up in merged part-m-00001    * part-m-00004 and part-m-00005 goes to outputShardNum = 2 and will end up in merged part-m-00002    * ... and so on    *     * Also see run() method above where it uses NLineInputFormat.setNumLinesPerSplit(job,    * options.fanout)    *     * Also see TreeMergeOutputFormat.TreeMergeRecordWriter.writeShardNumberFile()    */
DECL|method|renameTreeMergeShardDirs
specifier|private
name|boolean
name|renameTreeMergeShardDirs
parameter_list|(
name|Path
name|outputTreeMergeStep
parameter_list|,
name|Job
name|job
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|String
name|dirPrefix
init|=
name|SolrOutputFormat
operator|.
name|getOutputName
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|dirs
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|outputTreeMergeStep
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|dirPrefix
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirs
control|)
block|{
if|if
condition|(
operator|!
name|dir
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not a directory: "
operator|+
name|dir
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
comment|// Example: rename part-m-00004 to _part-m-00004
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirs
control|)
block|{
name|Path
name|path
init|=
name|dir
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|renamedPath
init|=
operator|new
name|Path
argument_list|(
name|path
operator|.
name|getParent
argument_list|()
argument_list|,
literal|"_"
operator|+
name|path
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|rename
argument_list|(
name|path
argument_list|,
name|renamedPath
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
comment|// Example: rename _part-m-00004 to part-m-00002
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirs
control|)
block|{
name|Path
name|path
init|=
name|dir
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|renamedPath
init|=
operator|new
name|Path
argument_list|(
name|path
operator|.
name|getParent
argument_list|()
argument_list|,
literal|"_"
operator|+
name|path
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|// read auxiliary metadata file (per task) that tells which taskId
comment|// processed which split# aka solrShard
name|Path
name|solrShardNumberFile
init|=
operator|new
name|Path
argument_list|(
name|renamedPath
argument_list|,
name|TreeMergeMapper
operator|.
name|SOLR_SHARD_NUMBER
argument_list|)
decl_stmt|;
name|InputStream
name|in
init|=
name|fs
operator|.
name|open
argument_list|(
name|solrShardNumberFile
argument_list|)
decl_stmt|;
name|byte
index|[]
name|bytes
init|=
name|ByteStreams
operator|.
name|toByteArray
argument_list|(
name|in
argument_list|)
decl_stmt|;
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|bytes
operator|.
name|length
operator|>
literal|0
argument_list|)
expr_stmt|;
name|int
name|solrShard
init|=
name|Integer
operator|.
name|parseInt
argument_list|(
operator|new
name|String
argument_list|(
name|bytes
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|delete
argument_list|(
name|solrShardNumberFile
argument_list|,
literal|false
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// same as FileOutputFormat.NUMBER_FORMAT
name|NumberFormat
name|numberFormat
init|=
name|NumberFormat
operator|.
name|getInstance
argument_list|(
name|Locale
operator|.
name|ENGLISH
argument_list|)
decl_stmt|;
name|numberFormat
operator|.
name|setMinimumIntegerDigits
argument_list|(
literal|5
argument_list|)
expr_stmt|;
name|numberFormat
operator|.
name|setGroupingUsed
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|Path
name|finalPath
init|=
operator|new
name|Path
argument_list|(
name|renamedPath
operator|.
name|getParent
argument_list|()
argument_list|,
name|dirPrefix
operator|+
literal|"-m-"
operator|+
name|numberFormat
operator|.
name|format
argument_list|(
name|solrShard
argument_list|)
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"MTree merge renaming solr shard: "
operator|+
name|solrShard
operator|+
literal|" from dir: "
operator|+
name|dir
operator|.
name|getPath
argument_list|()
operator|+
literal|" to dir: "
operator|+
name|finalPath
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|rename
argument_list|(
name|renamedPath
argument_list|,
name|finalPath
argument_list|,
name|fs
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
DECL|method|verifyGoLiveArgs
specifier|private
specifier|static
name|void
name|verifyGoLiveArgs
parameter_list|(
name|Options
name|opts
parameter_list|,
name|ArgumentParser
name|parser
parameter_list|)
throws|throws
name|ArgumentParserException
block|{
if|if
condition|(
name|opts
operator|.
name|zkHost
operator|==
literal|null
operator|&&
name|opts
operator|.
name|solrHomeDir
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"At least one of --zk-host or --solr-home-dir is required"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
if|if
condition|(
name|opts
operator|.
name|goLive
operator|&&
name|opts
operator|.
name|zkHost
operator|==
literal|null
operator|&&
name|opts
operator|.
name|shardUrls
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--go-live requires that you also pass --shard-url or --zk-host"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
if|if
condition|(
name|opts
operator|.
name|zkHost
operator|!=
literal|null
operator|&&
name|opts
operator|.
name|collection
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--zk-host requires that you also pass --collection"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
if|if
condition|(
name|opts
operator|.
name|zkHost
operator|!=
literal|null
condition|)
block|{
return|return;
comment|// verify structure of ZK directory later, to avoid checking run-time errors during parsing.
block|}
elseif|else
if|if
condition|(
name|opts
operator|.
name|shardUrls
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|opts
operator|.
name|shardUrls
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--shard-url requires at least one URL"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
block|}
elseif|else
if|if
condition|(
name|opts
operator|.
name|shards
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|opts
operator|.
name|shards
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--shards must be a positive number: "
operator|+
name|opts
operator|.
name|shards
argument_list|,
name|parser
argument_list|)
throw|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"You must specify one of the following (mutually exclusive) arguments: "
operator|+
literal|"--zk-host or --shard-url or --shards"
argument_list|,
name|parser
argument_list|)
throw|;
block|}
if|if
condition|(
name|opts
operator|.
name|shardUrls
operator|!=
literal|null
condition|)
block|{
name|opts
operator|.
name|shards
operator|=
name|opts
operator|.
name|shardUrls
operator|.
name|size
argument_list|()
expr_stmt|;
block|}
assert|assert
name|opts
operator|.
name|shards
operator|!=
literal|null
assert|;
assert|assert
name|opts
operator|.
name|shards
operator|>
literal|0
assert|;
block|}
DECL|method|verifyZKStructure
specifier|private
specifier|static
name|void
name|verifyZKStructure
parameter_list|(
name|Options
name|opts
parameter_list|,
name|ArgumentParser
name|parser
parameter_list|)
throws|throws
name|ArgumentParserException
block|{
if|if
condition|(
name|opts
operator|.
name|zkHost
operator|!=
literal|null
condition|)
block|{
assert|assert
name|opts
operator|.
name|collection
operator|!=
literal|null
assert|;
name|ZooKeeperInspector
name|zki
init|=
operator|new
name|ZooKeeperInspector
argument_list|()
decl_stmt|;
try|try
block|{
name|opts
operator|.
name|shardUrls
operator|=
name|zki
operator|.
name|extractShardUrls
argument_list|(
name|opts
operator|.
name|zkHost
argument_list|,
name|opts
operator|.
name|collection
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Cannot extract SolrCloud shard URLs from ZooKeeper"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
name|e
argument_list|,
name|parser
argument_list|)
throw|;
block|}
assert|assert
name|opts
operator|.
name|shardUrls
operator|!=
literal|null
assert|;
if|if
condition|(
name|opts
operator|.
name|shardUrls
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|ArgumentParserException
argument_list|(
literal|"--zk-host requires ZooKeeper "
operator|+
name|opts
operator|.
name|zkHost
operator|+
literal|" to contain at least one SolrCore for collection: "
operator|+
name|opts
operator|.
name|collection
argument_list|,
name|parser
argument_list|)
throw|;
block|}
name|opts
operator|.
name|shards
operator|=
name|opts
operator|.
name|shardUrls
operator|.
name|size
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Using SolrCloud shard URLs: {}"
argument_list|,
name|opts
operator|.
name|shardUrls
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|waitForCompletion
specifier|private
name|boolean
name|waitForCompletion
parameter_list|(
name|Job
name|job
parameter_list|,
name|boolean
name|isVerbose
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
throws|,
name|ClassNotFoundException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Running job: "
operator|+
name|getJobInfo
argument_list|(
name|job
argument_list|)
argument_list|)
expr_stmt|;
name|boolean
name|success
init|=
name|job
operator|.
name|waitForCompletion
argument_list|(
name|isVerbose
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Job failed! "
operator|+
name|getJobInfo
argument_list|(
name|job
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|success
return|;
block|}
DECL|method|goodbye
specifier|private
name|void
name|goodbye
parameter_list|(
name|Job
name|job
parameter_list|,
name|long
name|startTime
parameter_list|)
block|{
name|float
name|secs
init|=
operator|(
name|System
operator|.
name|nanoTime
argument_list|()
operator|-
name|startTime
operator|)
operator|/
call|(
name|float
call|)
argument_list|(
literal|10
operator|^
literal|9
argument_list|)
decl_stmt|;
if|if
condition|(
name|job
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Succeeded with job: "
operator|+
name|getJobInfo
argument_list|(
name|job
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Success. Done. Program took {} secs. Goodbye."
argument_list|,
name|secs
argument_list|)
expr_stmt|;
block|}
DECL|method|getJobInfo
specifier|private
name|String
name|getJobInfo
parameter_list|(
name|Job
name|job
parameter_list|)
block|{
return|return
literal|"jobName: "
operator|+
name|job
operator|.
name|getJobName
argument_list|()
operator|+
literal|", jobId: "
operator|+
name|job
operator|.
name|getJobID
argument_list|()
return|;
block|}
DECL|method|rename
specifier|private
name|boolean
name|rename
parameter_list|(
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|success
init|=
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot rename "
operator|+
name|src
operator|+
literal|" to "
operator|+
name|dst
argument_list|)
expr_stmt|;
block|}
return|return
name|success
return|;
block|}
DECL|method|delete
specifier|private
name|boolean
name|delete
parameter_list|(
name|Path
name|path
parameter_list|,
name|boolean
name|recursive
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|success
init|=
name|fs
operator|.
name|delete
argument_list|(
name|path
argument_list|,
name|recursive
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot delete "
operator|+
name|path
argument_list|)
expr_stmt|;
block|}
return|return
name|success
return|;
block|}
comment|// same as IntMath.divide(p, q, RoundingMode.CEILING)
DECL|method|ceilDivide
specifier|private
name|long
name|ceilDivide
parameter_list|(
name|long
name|p
parameter_list|,
name|long
name|q
parameter_list|)
block|{
name|long
name|result
init|=
name|p
operator|/
name|q
decl_stmt|;
if|if
condition|(
name|p
operator|%
name|q
operator|!=
literal|0
condition|)
block|{
name|result
operator|++
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|/**    * Returns<tt>log<sub>base</sub>value</tt>.    */
DECL|method|log
specifier|private
name|double
name|log
parameter_list|(
name|double
name|base
parameter_list|,
name|double
name|value
parameter_list|)
block|{
return|return
name|Math
operator|.
name|log
argument_list|(
name|value
argument_list|)
operator|/
name|Math
operator|.
name|log
argument_list|(
name|base
argument_list|)
return|;
block|}
block|}
end_class
end_unit
