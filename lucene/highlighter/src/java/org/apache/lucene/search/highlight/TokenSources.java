begin_unit
begin_comment
comment|/*  * Created on 28-Oct-2004  */
end_comment
begin_package
DECL|package|org.apache.lucene.search.highlight
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|highlight
package|;
end_package
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|Analyzer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|Token
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|CharTermAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|OffsetAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|PositionIncrementAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|DocsAndPositionsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Fields
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|StoredDocument
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Terms
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|TermsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|ArrayUtil
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|StringReader
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Comparator
import|;
end_import
begin_comment
comment|/**  * Hides implementation issues associated with obtaining a TokenStream for use  * with the higlighter - can obtain from TermFreqVectors with offsets and  * (optionally) positions or from Analyzer class reparsing the stored content.  */
end_comment
begin_class
DECL|class|TokenSources
specifier|public
class|class
name|TokenSources
block|{
comment|/**    * A convenience method that tries to first get a TermPositionVector for the    * specified docId, then, falls back to using the passed in    * {@link org.apache.lucene.document.Document} to retrieve the TokenStream.    * This is useful when you already have the document, but would prefer to use    * the vector first.    *     * @param reader The {@link org.apache.lucene.index.IndexReader} to use to try    *        and get the vector from    * @param docId The docId to retrieve.    * @param field The field to retrieve on the document    * @param document The document to fall back on    * @param analyzer The analyzer to use for creating the TokenStream if the    *        vector doesn't exist    * @return The {@link org.apache.lucene.analysis.TokenStream} for the    *         {@link org.apache.lucene.index.IndexableField} on the    *         {@link org.apache.lucene.document.Document}    * @throws IOException if there was an error loading    */
DECL|method|getAnyTokenStream
specifier|public
specifier|static
name|TokenStream
name|getAnyTokenStream
parameter_list|(
name|IndexReader
name|reader
parameter_list|,
name|int
name|docId
parameter_list|,
name|String
name|field
parameter_list|,
name|StoredDocument
name|document
parameter_list|,
name|Analyzer
name|analyzer
parameter_list|)
throws|throws
name|IOException
block|{
name|TokenStream
name|ts
init|=
literal|null
decl_stmt|;
name|Fields
name|vectors
init|=
name|reader
operator|.
name|getTermVectors
argument_list|(
name|docId
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectors
operator|!=
literal|null
condition|)
block|{
name|Terms
name|vector
init|=
name|vectors
operator|.
name|terms
argument_list|(
name|field
argument_list|)
decl_stmt|;
if|if
condition|(
name|vector
operator|!=
literal|null
condition|)
block|{
name|ts
operator|=
name|getTokenStream
argument_list|(
name|vector
argument_list|)
expr_stmt|;
block|}
block|}
comment|// No token info stored so fall back to analyzing raw content
if|if
condition|(
name|ts
operator|==
literal|null
condition|)
block|{
name|ts
operator|=
name|getTokenStream
argument_list|(
name|document
argument_list|,
name|field
argument_list|,
name|analyzer
argument_list|)
expr_stmt|;
block|}
return|return
name|ts
return|;
block|}
comment|/**    * A convenience method that tries a number of approaches to getting a token    * stream. The cost of finding there are no termVectors in the index is    * minimal (1000 invocations still registers 0 ms). So this "lazy" (flexible?)    * approach to coding is probably acceptable    *     * @return null if field not stored correctly    * @throws IOException If there is a low-level I/O error    */
DECL|method|getAnyTokenStream
specifier|public
specifier|static
name|TokenStream
name|getAnyTokenStream
parameter_list|(
name|IndexReader
name|reader
parameter_list|,
name|int
name|docId
parameter_list|,
name|String
name|field
parameter_list|,
name|Analyzer
name|analyzer
parameter_list|)
throws|throws
name|IOException
block|{
name|TokenStream
name|ts
init|=
literal|null
decl_stmt|;
name|Fields
name|vectors
init|=
name|reader
operator|.
name|getTermVectors
argument_list|(
name|docId
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectors
operator|!=
literal|null
condition|)
block|{
name|Terms
name|vector
init|=
name|vectors
operator|.
name|terms
argument_list|(
name|field
argument_list|)
decl_stmt|;
if|if
condition|(
name|vector
operator|!=
literal|null
condition|)
block|{
name|ts
operator|=
name|getTokenStream
argument_list|(
name|vector
argument_list|)
expr_stmt|;
block|}
block|}
comment|// No token info stored so fall back to analyzing raw content
if|if
condition|(
name|ts
operator|==
literal|null
condition|)
block|{
name|ts
operator|=
name|getTokenStream
argument_list|(
name|reader
argument_list|,
name|docId
argument_list|,
name|field
argument_list|,
name|analyzer
argument_list|)
expr_stmt|;
block|}
return|return
name|ts
return|;
block|}
DECL|method|getTokenStream
specifier|public
specifier|static
name|TokenStream
name|getTokenStream
parameter_list|(
name|Terms
name|vector
parameter_list|)
throws|throws
name|IOException
block|{
comment|// assumes the worst and makes no assumptions about token position
comment|// sequences.
return|return
name|getTokenStream
argument_list|(
name|vector
argument_list|,
literal|false
argument_list|)
return|;
block|}
comment|/**    * Low level api. Returns a token stream generated from a {@link Terms}. This    * can be used to feed the highlighter with a pre-parsed token    * stream.  The {@link Terms} must have offsets available.    *     * In my tests the speeds to recreate 1000 token streams using this method    * are: - with TermVector offset only data stored - 420 milliseconds - with    * TermVector offset AND position data stored - 271 milliseconds (nb timings    * for TermVector with position data are based on a tokenizer with contiguous    * positions - no overlaps or gaps) The cost of not using TermPositionVector    * to store pre-parsed content and using an analyzer to re-parse the original    * content: - reanalyzing the original content - 980 milliseconds    *     * The re-analyze timings will typically vary depending on - 1) The complexity    * of the analyzer code (timings above were using a    * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene    * reads ALL fields off the disk when accessing just one document field - can    * cost dear!) 3) Use of compression on field storage - could be faster due to    * compression (less disk IO) or slower (more CPU burn) depending on the    * content.    *     * @param tokenPositionsGuaranteedContiguous true if the token position    *        numbers have no overlaps or gaps. If looking to eek out the last    *        drops of performance, set to true. If in doubt, set to false.    *    * @throws IllegalArgumentException if no offsets are available    */
DECL|method|getTokenStream
specifier|public
specifier|static
name|TokenStream
name|getTokenStream
parameter_list|(
name|Terms
name|tpv
parameter_list|,
name|boolean
name|tokenPositionsGuaranteedContiguous
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|tpv
operator|.
name|hasOffsets
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Cannot create TokenStream from Terms without offsets"
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|tokenPositionsGuaranteedContiguous
operator|&&
name|tpv
operator|.
name|hasPositions
argument_list|()
condition|)
block|{
return|return
operator|new
name|TokenStreamFromTermPositionVector
argument_list|(
name|tpv
argument_list|)
return|;
block|}
comment|// an object used to iterate across an array of tokens
specifier|final
class|class
name|StoredTokenStream
extends|extends
name|TokenStream
block|{
name|Token
name|tokens
index|[]
decl_stmt|;
name|int
name|currentToken
init|=
literal|0
decl_stmt|;
name|CharTermAttribute
name|termAtt
decl_stmt|;
name|OffsetAttribute
name|offsetAtt
decl_stmt|;
name|PositionIncrementAttribute
name|posincAtt
decl_stmt|;
name|StoredTokenStream
parameter_list|(
name|Token
name|tokens
index|[]
parameter_list|)
block|{
name|this
operator|.
name|tokens
operator|=
name|tokens
expr_stmt|;
name|termAtt
operator|=
name|addAttribute
argument_list|(
name|CharTermAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|offsetAtt
operator|=
name|addAttribute
argument_list|(
name|OffsetAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|posincAtt
operator|=
name|addAttribute
argument_list|(
name|PositionIncrementAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|incrementToken
parameter_list|()
block|{
if|if
condition|(
name|currentToken
operator|>=
name|tokens
operator|.
name|length
condition|)
block|{
return|return
literal|false
return|;
block|}
name|Token
name|token
init|=
name|tokens
index|[
name|currentToken
operator|++
index|]
decl_stmt|;
name|clearAttributes
argument_list|()
expr_stmt|;
name|termAtt
operator|.
name|setEmpty
argument_list|()
operator|.
name|append
argument_list|(
name|token
argument_list|)
expr_stmt|;
name|offsetAtt
operator|.
name|setOffset
argument_list|(
name|token
operator|.
name|startOffset
argument_list|()
argument_list|,
name|token
operator|.
name|endOffset
argument_list|()
argument_list|)
expr_stmt|;
name|posincAtt
operator|.
name|setPositionIncrement
argument_list|(
name|currentToken
operator|<=
literal|1
operator|||
name|tokens
index|[
name|currentToken
operator|-
literal|1
index|]
operator|.
name|startOffset
argument_list|()
operator|>
name|tokens
index|[
name|currentToken
operator|-
literal|2
index|]
operator|.
name|startOffset
argument_list|()
condition|?
literal|1
else|:
literal|0
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
comment|// code to reconstruct the original sequence of Tokens
name|TermsEnum
name|termsEnum
init|=
name|tpv
operator|.
name|iterator
argument_list|(
literal|null
argument_list|)
decl_stmt|;
name|int
name|totalTokens
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|termsEnum
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|totalTokens
operator|+=
operator|(
name|int
operator|)
name|termsEnum
operator|.
name|totalTermFreq
argument_list|()
expr_stmt|;
block|}
name|Token
name|tokensInOriginalOrder
index|[]
init|=
operator|new
name|Token
index|[
name|totalTokens
index|]
decl_stmt|;
name|ArrayList
argument_list|<
name|Token
argument_list|>
name|unsortedTokens
init|=
literal|null
decl_stmt|;
name|termsEnum
operator|=
name|tpv
operator|.
name|iterator
argument_list|(
literal|null
argument_list|)
expr_stmt|;
name|BytesRef
name|text
decl_stmt|;
name|DocsAndPositionsEnum
name|dpEnum
init|=
literal|null
decl_stmt|;
while|while
condition|(
operator|(
name|text
operator|=
name|termsEnum
operator|.
name|next
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|dpEnum
operator|=
name|termsEnum
operator|.
name|docsAndPositions
argument_list|(
literal|null
argument_list|,
name|dpEnum
argument_list|)
expr_stmt|;
if|if
condition|(
name|dpEnum
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Required TermVector Offset information was not found"
argument_list|)
throw|;
block|}
specifier|final
name|String
name|term
init|=
name|text
operator|.
name|utf8ToString
argument_list|()
decl_stmt|;
name|dpEnum
operator|.
name|nextDoc
argument_list|()
expr_stmt|;
specifier|final
name|int
name|freq
init|=
name|dpEnum
operator|.
name|freq
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|posUpto
init|=
literal|0
init|;
name|posUpto
operator|<
name|freq
condition|;
name|posUpto
operator|++
control|)
block|{
specifier|final
name|int
name|pos
init|=
name|dpEnum
operator|.
name|nextPosition
argument_list|()
decl_stmt|;
if|if
condition|(
name|dpEnum
operator|.
name|startOffset
argument_list|()
operator|<
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Required TermVector Offset information was not found"
argument_list|)
throw|;
block|}
specifier|final
name|Token
name|token
init|=
operator|new
name|Token
argument_list|(
name|term
argument_list|,
name|dpEnum
operator|.
name|startOffset
argument_list|()
argument_list|,
name|dpEnum
operator|.
name|endOffset
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|tokenPositionsGuaranteedContiguous
operator|&&
name|pos
operator|!=
operator|-
literal|1
condition|)
block|{
comment|// We have positions stored and a guarantee that the token position
comment|// information is contiguous
comment|// This may be fast BUT wont work if Tokenizers used which create>1
comment|// token in same position or
comment|// creates jumps in position numbers - this code would fail under those
comment|// circumstances
comment|// tokens stored with positions - can use this to index straight into
comment|// sorted array
name|tokensInOriginalOrder
index|[
name|pos
index|]
operator|=
name|token
expr_stmt|;
block|}
else|else
block|{
comment|// tokens NOT stored with positions or not guaranteed contiguous - must
comment|// add to list and sort later
if|if
condition|(
name|unsortedTokens
operator|==
literal|null
condition|)
block|{
name|unsortedTokens
operator|=
operator|new
name|ArrayList
argument_list|<
name|Token
argument_list|>
argument_list|()
expr_stmt|;
block|}
name|unsortedTokens
operator|.
name|add
argument_list|(
name|token
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// If the field has been stored without position data we must perform a sort
if|if
condition|(
name|unsortedTokens
operator|!=
literal|null
condition|)
block|{
name|tokensInOriginalOrder
operator|=
name|unsortedTokens
operator|.
name|toArray
argument_list|(
operator|new
name|Token
index|[
name|unsortedTokens
operator|.
name|size
argument_list|()
index|]
argument_list|)
expr_stmt|;
name|ArrayUtil
operator|.
name|mergeSort
argument_list|(
name|tokensInOriginalOrder
argument_list|,
operator|new
name|Comparator
argument_list|<
name|Token
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|int
name|compare
parameter_list|(
name|Token
name|t1
parameter_list|,
name|Token
name|t2
parameter_list|)
block|{
if|if
condition|(
name|t1
operator|.
name|startOffset
argument_list|()
operator|==
name|t2
operator|.
name|startOffset
argument_list|()
condition|)
return|return
name|t1
operator|.
name|endOffset
argument_list|()
operator|-
name|t2
operator|.
name|endOffset
argument_list|()
return|;
else|else
return|return
name|t1
operator|.
name|startOffset
argument_list|()
operator|-
name|t2
operator|.
name|startOffset
argument_list|()
return|;
block|}
block|}
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|StoredTokenStream
argument_list|(
name|tokensInOriginalOrder
argument_list|)
return|;
block|}
comment|/**    * Returns a {@link TokenStream} with positions and offsets constructed from    * field termvectors.  If the field has no termvectors, or positions or offsets    * are not included in the termvector, return null.    * @param reader the {@link IndexReader} to retrieve term vectors from    * @param docId the document to retrieve termvectors for    * @param field the field to retrieve termvectors for    * @return a {@link TokenStream}, or null if positions and offsets are not available    * @throws IOException If there is a low-level I/O error    */
DECL|method|getTokenStreamWithOffsets
specifier|public
specifier|static
name|TokenStream
name|getTokenStreamWithOffsets
parameter_list|(
name|IndexReader
name|reader
parameter_list|,
name|int
name|docId
parameter_list|,
name|String
name|field
parameter_list|)
throws|throws
name|IOException
block|{
name|Fields
name|vectors
init|=
name|reader
operator|.
name|getTermVectors
argument_list|(
name|docId
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectors
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Terms
name|vector
init|=
name|vectors
operator|.
name|terms
argument_list|(
name|field
argument_list|)
decl_stmt|;
if|if
condition|(
name|vector
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
name|vector
operator|.
name|hasPositions
argument_list|()
operator|||
operator|!
name|vector
operator|.
name|hasOffsets
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|getTokenStream
argument_list|(
name|vector
argument_list|)
return|;
block|}
comment|// convenience method
DECL|method|getTokenStream
specifier|public
specifier|static
name|TokenStream
name|getTokenStream
parameter_list|(
name|IndexReader
name|reader
parameter_list|,
name|int
name|docId
parameter_list|,
name|String
name|field
parameter_list|,
name|Analyzer
name|analyzer
parameter_list|)
throws|throws
name|IOException
block|{
name|StoredDocument
name|doc
init|=
name|reader
operator|.
name|document
argument_list|(
name|docId
argument_list|)
decl_stmt|;
return|return
name|getTokenStream
argument_list|(
name|doc
argument_list|,
name|field
argument_list|,
name|analyzer
argument_list|)
return|;
block|}
DECL|method|getTokenStream
specifier|public
specifier|static
name|TokenStream
name|getTokenStream
parameter_list|(
name|StoredDocument
name|doc
parameter_list|,
name|String
name|field
parameter_list|,
name|Analyzer
name|analyzer
parameter_list|)
block|{
name|String
name|contents
init|=
name|doc
operator|.
name|get
argument_list|(
name|field
argument_list|)
decl_stmt|;
if|if
condition|(
name|contents
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Field "
operator|+
name|field
operator|+
literal|" in document is not stored and cannot be analyzed"
argument_list|)
throw|;
block|}
return|return
name|getTokenStream
argument_list|(
name|field
argument_list|,
name|contents
argument_list|,
name|analyzer
argument_list|)
return|;
block|}
comment|// convenience method
DECL|method|getTokenStream
specifier|public
specifier|static
name|TokenStream
name|getTokenStream
parameter_list|(
name|String
name|field
parameter_list|,
name|String
name|contents
parameter_list|,
name|Analyzer
name|analyzer
parameter_list|)
block|{
try|try
block|{
return|return
name|analyzer
operator|.
name|tokenStream
argument_list|(
name|field
argument_list|,
operator|new
name|StringReader
argument_list|(
name|contents
argument_list|)
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|ex
argument_list|)
throw|;
block|}
block|}
block|}
end_class
end_unit
