begin_unit
begin_package
DECL|package|org.apache.lucene.rangetree
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|rangetree
package|;
end_package
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|DirectoryStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Files
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Path
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Comparator
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|ByteArrayDataInput
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|ByteArrayDataOutput
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|IndexOutput
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|ArrayUtil
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRefBuilder
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|IOUtils
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|InPlaceMergeSorter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|OfflineSorter
operator|.
name|ByteSequencesWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|OfflineSorter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|RamUsageEstimator
import|;
end_import
begin_comment
comment|// TODO
end_comment
begin_comment
comment|//   - could we just "use postings" to map leaf -> docIDs?
end_comment
begin_comment
comment|//   - we could also index "auto-prefix terms" here, and use better compression
end_comment
begin_comment
comment|//   - the index could be efficiently encoded as an FST, so we don't have wasteful
end_comment
begin_comment
comment|//     (monotonic) long[] leafBlockFPs; or we could use MonotonicLongValues ... but then
end_comment
begin_comment
comment|//     the index is already plenty small: 60M OSM points --> 1.1 MB with 128 points
end_comment
begin_comment
comment|//     per leaf, and you can reduce that by putting more points per leaf
end_comment
begin_comment
comment|//   - we can quantize the split values to 2 bytes (short): http://people.csail.mit.edu/tmertens/papers/qkdtree.pdf
end_comment
begin_comment
comment|/** Recursively builds a 1d BKD tree to assign all incoming {@code long} values to smaller  *  and smaller ranges until the number of points in a given  *  range is&lt= the<code>maxPointsInLeafNode</code>.  The tree is  *  fully balanced, which means the leaf nodes will have between 50% and 100% of  *  the requested<code>maxPointsInLeafNode</code>, except for the adversarial case  *  of indexing exactly the same value many times.  *  *<p>  *  See<a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.  *  *<p>This consumes heap during writing: for any nodes with fewer than<code>maxPointsSortInHeap</code>, it holds  *  the points in memory as simple java arrays.  *  *<p>  *<b>NOTE</b>: This can write at most Integer.MAX_VALUE *<code>maxPointsInLeafNode</code> total values,  *  which should be plenty since a Lucene index can have at most Integer.MAX_VALUE-1 documents.  *  * @lucene.experimental */
end_comment
begin_class
DECL|class|RangeTreeWriter
class|class
name|RangeTreeWriter
block|{
comment|// value (long) + ord (long) + docID (int)
DECL|field|BYTES_PER_DOC
specifier|static
specifier|final
name|int
name|BYTES_PER_DOC
init|=
literal|2
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_LONG
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
decl_stmt|;
DECL|field|DEFAULT_MAX_VALUES_IN_LEAF_NODE
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_MAX_VALUES_IN_LEAF_NODE
init|=
literal|1024
decl_stmt|;
comment|/** This works out to max of ~10 MB peak heap tied up during writing: */
DECL|field|DEFAULT_MAX_VALUES_SORT_IN_HEAP
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_MAX_VALUES_SORT_IN_HEAP
init|=
literal|128
operator|*
literal|1024
decl_stmt|;
empty_stmt|;
DECL|field|scratchBytes
specifier|private
specifier|final
name|byte
index|[]
name|scratchBytes
init|=
operator|new
name|byte
index|[
name|BYTES_PER_DOC
index|]
decl_stmt|;
DECL|field|scratchBytesOutput
specifier|private
specifier|final
name|ByteArrayDataOutput
name|scratchBytesOutput
init|=
operator|new
name|ByteArrayDataOutput
argument_list|(
name|scratchBytes
argument_list|)
decl_stmt|;
DECL|field|writer
specifier|private
name|OfflineSorter
operator|.
name|ByteSequencesWriter
name|writer
decl_stmt|;
DECL|field|heapWriter
specifier|private
name|GrowingHeapSliceWriter
name|heapWriter
decl_stmt|;
DECL|field|tempInput
specifier|private
name|Path
name|tempInput
decl_stmt|;
DECL|field|maxValuesInLeafNode
specifier|private
specifier|final
name|int
name|maxValuesInLeafNode
decl_stmt|;
DECL|field|maxValuesSortInHeap
specifier|private
specifier|final
name|int
name|maxValuesSortInHeap
decl_stmt|;
DECL|field|valueCount
specifier|private
name|long
name|valueCount
decl_stmt|;
DECL|field|globalMinValue
specifier|private
name|long
name|globalMinValue
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
DECL|field|globalMaxValue
specifier|private
name|long
name|globalMaxValue
init|=
name|Long
operator|.
name|MIN_VALUE
decl_stmt|;
DECL|method|RangeTreeWriter
specifier|public
name|RangeTreeWriter
parameter_list|()
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|DEFAULT_MAX_VALUES_IN_LEAF_NODE
argument_list|,
name|DEFAULT_MAX_VALUES_SORT_IN_HEAP
argument_list|)
expr_stmt|;
block|}
comment|// TODO: instead of maxValuesSortInHeap, change to maxMBHeap ... the mapping is non-obvious:
DECL|method|RangeTreeWriter
specifier|public
name|RangeTreeWriter
parameter_list|(
name|int
name|maxValuesInLeafNode
parameter_list|,
name|int
name|maxValuesSortInHeap
parameter_list|)
throws|throws
name|IOException
block|{
name|verifyParams
argument_list|(
name|maxValuesInLeafNode
argument_list|,
name|maxValuesSortInHeap
argument_list|)
expr_stmt|;
name|this
operator|.
name|maxValuesInLeafNode
operator|=
name|maxValuesInLeafNode
expr_stmt|;
name|this
operator|.
name|maxValuesSortInHeap
operator|=
name|maxValuesSortInHeap
expr_stmt|;
comment|// We write first maxValuesSortInHeap in heap, then cutover to offline for additional points:
name|heapWriter
operator|=
operator|new
name|GrowingHeapSliceWriter
argument_list|(
name|maxValuesSortInHeap
argument_list|)
expr_stmt|;
block|}
DECL|method|verifyParams
specifier|public
specifier|static
name|void
name|verifyParams
parameter_list|(
name|int
name|maxValuesInLeafNode
parameter_list|,
name|int
name|maxValuesSortInHeap
parameter_list|)
block|{
if|if
condition|(
name|maxValuesInLeafNode
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"maxValuesInLeafNode must be> 0; got "
operator|+
name|maxValuesInLeafNode
argument_list|)
throw|;
block|}
if|if
condition|(
name|maxValuesInLeafNode
operator|>
name|ArrayUtil
operator|.
name|MAX_ARRAY_LENGTH
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"maxValuesInLeafNode must be<= ArrayUtil.MAX_ARRAY_LENGTH (= "
operator|+
name|ArrayUtil
operator|.
name|MAX_ARRAY_LENGTH
operator|+
literal|"); got "
operator|+
name|maxValuesInLeafNode
argument_list|)
throw|;
block|}
if|if
condition|(
name|maxValuesSortInHeap
operator|<
name|maxValuesInLeafNode
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"maxValuesSortInHeap must be>= maxValuesInLeafNode; got "
operator|+
name|maxValuesSortInHeap
operator|+
literal|" vs maxValuesInLeafNode="
operator|+
name|maxValuesInLeafNode
argument_list|)
throw|;
block|}
if|if
condition|(
name|maxValuesSortInHeap
operator|>
name|ArrayUtil
operator|.
name|MAX_ARRAY_LENGTH
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"maxValuesSortInHeap must be<= ArrayUtil.MAX_ARRAY_LENGTH (= "
operator|+
name|ArrayUtil
operator|.
name|MAX_ARRAY_LENGTH
operator|+
literal|"); got "
operator|+
name|maxValuesSortInHeap
argument_list|)
throw|;
block|}
block|}
comment|/** If the current segment has too many points then we switchover to temp files / offline sort. */
DECL|method|switchToOffline
specifier|private
name|void
name|switchToOffline
parameter_list|()
throws|throws
name|IOException
block|{
comment|// For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:
name|tempInput
operator|=
name|Files
operator|.
name|createTempFile
argument_list|(
name|OfflineSorter
operator|.
name|getDefaultTempDir
argument_list|()
argument_list|,
literal|"in"
argument_list|,
literal|""
argument_list|)
expr_stmt|;
name|writer
operator|=
operator|new
name|OfflineSorter
operator|.
name|ByteSequencesWriter
argument_list|(
name|tempInput
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|valueCount
condition|;
name|i
operator|++
control|)
block|{
name|scratchBytesOutput
operator|.
name|reset
argument_list|(
name|scratchBytes
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeLong
argument_list|(
name|heapWriter
operator|.
name|values
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeVInt
argument_list|(
name|heapWriter
operator|.
name|docIDs
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeVLong
argument_list|(
name|i
argument_list|)
expr_stmt|;
comment|// TODO: can/should OfflineSorter optimize the fixed-width case?
name|writer
operator|.
name|write
argument_list|(
name|scratchBytes
argument_list|,
literal|0
argument_list|,
name|scratchBytes
operator|.
name|length
argument_list|)
expr_stmt|;
block|}
name|heapWriter
operator|=
literal|null
expr_stmt|;
block|}
DECL|method|add
name|void
name|add
parameter_list|(
name|long
name|value
parameter_list|,
name|int
name|docID
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|valueCount
operator|>=
name|maxValuesSortInHeap
condition|)
block|{
if|if
condition|(
name|writer
operator|==
literal|null
condition|)
block|{
name|switchToOffline
argument_list|()
expr_stmt|;
block|}
name|scratchBytesOutput
operator|.
name|reset
argument_list|(
name|scratchBytes
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeLong
argument_list|(
name|value
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeVInt
argument_list|(
name|docID
argument_list|)
expr_stmt|;
name|scratchBytesOutput
operator|.
name|writeVLong
argument_list|(
name|valueCount
argument_list|)
expr_stmt|;
name|writer
operator|.
name|write
argument_list|(
name|scratchBytes
argument_list|,
literal|0
argument_list|,
name|scratchBytes
operator|.
name|length
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Not too many points added yet, continue using heap:
name|heapWriter
operator|.
name|append
argument_list|(
name|value
argument_list|,
name|valueCount
argument_list|,
name|docID
argument_list|)
expr_stmt|;
block|}
name|valueCount
operator|++
expr_stmt|;
name|globalMaxValue
operator|=
name|Math
operator|.
name|max
argument_list|(
name|value
argument_list|,
name|globalMaxValue
argument_list|)
expr_stmt|;
name|globalMinValue
operator|=
name|Math
operator|.
name|min
argument_list|(
name|value
argument_list|,
name|globalMinValue
argument_list|)
expr_stmt|;
block|}
comment|/** Changes incoming {@link ByteSequencesWriter} file to to fixed-width-per-entry file, because we need to be able to slice    *  as we recurse in {@link #build}. */
DECL|method|convertToFixedWidth
specifier|private
name|SliceWriter
name|convertToFixedWidth
parameter_list|(
name|Path
name|in
parameter_list|)
throws|throws
name|IOException
block|{
name|BytesRefBuilder
name|scratch
init|=
operator|new
name|BytesRefBuilder
argument_list|()
decl_stmt|;
name|scratch
operator|.
name|grow
argument_list|(
name|BYTES_PER_DOC
argument_list|)
expr_stmt|;
name|BytesRef
name|bytes
init|=
name|scratch
operator|.
name|get
argument_list|()
decl_stmt|;
name|ByteArrayDataInput
name|dataReader
init|=
operator|new
name|ByteArrayDataInput
argument_list|()
decl_stmt|;
name|OfflineSorter
operator|.
name|ByteSequencesReader
name|reader
init|=
literal|null
decl_stmt|;
name|SliceWriter
name|sortedWriter
init|=
literal|null
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|reader
operator|=
operator|new
name|OfflineSorter
operator|.
name|ByteSequencesReader
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|sortedWriter
operator|=
name|getWriter
argument_list|(
name|valueCount
argument_list|)
expr_stmt|;
for|for
control|(
name|long
name|i
init|=
literal|0
init|;
name|i
operator|<
name|valueCount
condition|;
name|i
operator|++
control|)
block|{
name|boolean
name|result
init|=
name|reader
operator|.
name|read
argument_list|(
name|scratch
argument_list|)
decl_stmt|;
assert|assert
name|result
assert|;
name|dataReader
operator|.
name|reset
argument_list|(
name|bytes
operator|.
name|bytes
argument_list|,
name|bytes
operator|.
name|offset
argument_list|,
name|bytes
operator|.
name|length
argument_list|)
expr_stmt|;
name|long
name|value
init|=
name|dataReader
operator|.
name|readLong
argument_list|()
decl_stmt|;
name|int
name|docID
init|=
name|dataReader
operator|.
name|readVInt
argument_list|()
decl_stmt|;
assert|assert
name|docID
operator|>=
literal|0
operator|:
literal|"docID="
operator|+
name|docID
assert|;
name|long
name|ord
init|=
name|dataReader
operator|.
name|readVLong
argument_list|()
decl_stmt|;
name|sortedWriter
operator|.
name|append
argument_list|(
name|value
argument_list|,
name|ord
argument_list|,
name|docID
argument_list|)
expr_stmt|;
block|}
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|sortedWriter
argument_list|,
name|reader
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|closeWhileHandlingException
argument_list|(
name|sortedWriter
argument_list|,
name|reader
argument_list|)
expr_stmt|;
try|try
block|{
name|sortedWriter
operator|.
name|destroy
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
comment|// Suppress to keep throwing original exc
block|}
block|}
block|}
return|return
name|sortedWriter
return|;
block|}
DECL|method|sort
specifier|private
name|SliceWriter
name|sort
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|heapWriter
operator|!=
literal|null
condition|)
block|{
assert|assert
name|valueCount
operator|<
name|Integer
operator|.
name|MAX_VALUE
assert|;
comment|// All buffered points are still in heap
operator|new
name|InPlaceMergeSorter
argument_list|()
block|{
annotation|@
name|Override
specifier|protected
name|void
name|swap
parameter_list|(
name|int
name|i
parameter_list|,
name|int
name|j
parameter_list|)
block|{
name|int
name|docID
init|=
name|heapWriter
operator|.
name|docIDs
index|[
name|i
index|]
decl_stmt|;
name|heapWriter
operator|.
name|docIDs
index|[
name|i
index|]
operator|=
name|heapWriter
operator|.
name|docIDs
index|[
name|j
index|]
expr_stmt|;
name|heapWriter
operator|.
name|docIDs
index|[
name|j
index|]
operator|=
name|docID
expr_stmt|;
name|long
name|ord
init|=
name|heapWriter
operator|.
name|ords
index|[
name|i
index|]
decl_stmt|;
name|heapWriter
operator|.
name|ords
index|[
name|i
index|]
operator|=
name|heapWriter
operator|.
name|ords
index|[
name|j
index|]
expr_stmt|;
name|heapWriter
operator|.
name|ords
index|[
name|j
index|]
operator|=
name|ord
expr_stmt|;
name|long
name|value
init|=
name|heapWriter
operator|.
name|values
index|[
name|i
index|]
decl_stmt|;
name|heapWriter
operator|.
name|values
index|[
name|i
index|]
operator|=
name|heapWriter
operator|.
name|values
index|[
name|j
index|]
expr_stmt|;
name|heapWriter
operator|.
name|values
index|[
name|j
index|]
operator|=
name|value
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|int
name|compare
parameter_list|(
name|int
name|i
parameter_list|,
name|int
name|j
parameter_list|)
block|{
name|int
name|cmp
init|=
name|Long
operator|.
name|compare
argument_list|(
name|heapWriter
operator|.
name|values
index|[
name|i
index|]
argument_list|,
name|heapWriter
operator|.
name|values
index|[
name|j
index|]
argument_list|)
decl_stmt|;
if|if
condition|(
name|cmp
operator|!=
literal|0
condition|)
block|{
return|return
name|cmp
return|;
block|}
comment|// Tie-break
name|cmp
operator|=
name|Integer
operator|.
name|compare
argument_list|(
name|heapWriter
operator|.
name|docIDs
index|[
name|i
index|]
argument_list|,
name|heapWriter
operator|.
name|docIDs
index|[
name|j
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|cmp
operator|!=
literal|0
condition|)
block|{
return|return
name|cmp
return|;
block|}
return|return
name|Long
operator|.
name|compare
argument_list|(
name|heapWriter
operator|.
name|ords
index|[
name|i
index|]
argument_list|,
name|heapWriter
operator|.
name|ords
index|[
name|j
index|]
argument_list|)
return|;
block|}
block|}
operator|.
name|sort
argument_list|(
literal|0
argument_list|,
operator|(
name|int
operator|)
name|valueCount
argument_list|)
expr_stmt|;
name|HeapSliceWriter
name|sorted
init|=
operator|new
name|HeapSliceWriter
argument_list|(
operator|(
name|int
operator|)
name|valueCount
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|valueCount
condition|;
name|i
operator|++
control|)
block|{
name|sorted
operator|.
name|append
argument_list|(
name|heapWriter
operator|.
name|values
index|[
name|i
index|]
argument_list|,
name|heapWriter
operator|.
name|ords
index|[
name|i
index|]
argument_list|,
name|heapWriter
operator|.
name|docIDs
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
name|sorted
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|sorted
return|;
block|}
else|else
block|{
comment|// Offline sort:
assert|assert
name|tempInput
operator|!=
literal|null
assert|;
specifier|final
name|ByteArrayDataInput
name|reader
init|=
operator|new
name|ByteArrayDataInput
argument_list|()
decl_stmt|;
name|Comparator
argument_list|<
name|BytesRef
argument_list|>
name|cmp
init|=
operator|new
name|Comparator
argument_list|<
name|BytesRef
argument_list|>
argument_list|()
block|{
specifier|private
specifier|final
name|ByteArrayDataInput
name|readerB
init|=
operator|new
name|ByteArrayDataInput
argument_list|()
decl_stmt|;
annotation|@
name|Override
specifier|public
name|int
name|compare
parameter_list|(
name|BytesRef
name|a
parameter_list|,
name|BytesRef
name|b
parameter_list|)
block|{
name|reader
operator|.
name|reset
argument_list|(
name|a
operator|.
name|bytes
argument_list|,
name|a
operator|.
name|offset
argument_list|,
name|a
operator|.
name|length
argument_list|)
expr_stmt|;
specifier|final
name|long
name|valueA
init|=
name|reader
operator|.
name|readLong
argument_list|()
decl_stmt|;
specifier|final
name|int
name|docIDA
init|=
name|reader
operator|.
name|readVInt
argument_list|()
decl_stmt|;
specifier|final
name|long
name|ordA
init|=
name|reader
operator|.
name|readVLong
argument_list|()
decl_stmt|;
name|reader
operator|.
name|reset
argument_list|(
name|b
operator|.
name|bytes
argument_list|,
name|b
operator|.
name|offset
argument_list|,
name|b
operator|.
name|length
argument_list|)
expr_stmt|;
specifier|final
name|long
name|valueB
init|=
name|reader
operator|.
name|readLong
argument_list|()
decl_stmt|;
specifier|final
name|int
name|docIDB
init|=
name|reader
operator|.
name|readVInt
argument_list|()
decl_stmt|;
specifier|final
name|long
name|ordB
init|=
name|reader
operator|.
name|readVLong
argument_list|()
decl_stmt|;
name|int
name|cmp
init|=
name|Long
operator|.
name|compare
argument_list|(
name|valueA
argument_list|,
name|valueB
argument_list|)
decl_stmt|;
if|if
condition|(
name|cmp
operator|!=
literal|0
condition|)
block|{
return|return
name|cmp
return|;
block|}
comment|// Tie-break
name|cmp
operator|=
name|Integer
operator|.
name|compare
argument_list|(
name|docIDA
argument_list|,
name|docIDB
argument_list|)
expr_stmt|;
if|if
condition|(
name|cmp
operator|!=
literal|0
condition|)
block|{
return|return
name|cmp
return|;
block|}
return|return
name|Long
operator|.
name|compare
argument_list|(
name|ordA
argument_list|,
name|ordB
argument_list|)
return|;
block|}
block|}
decl_stmt|;
name|Path
name|sorted
init|=
name|Files
operator|.
name|createTempFile
argument_list|(
name|OfflineSorter
operator|.
name|getDefaultTempDir
argument_list|()
argument_list|,
literal|"sorted"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|OfflineSorter
name|sorter
init|=
operator|new
name|OfflineSorter
argument_list|(
name|cmp
argument_list|)
decl_stmt|;
name|sorter
operator|.
name|sort
argument_list|(
name|tempInput
argument_list|,
name|sorted
argument_list|)
expr_stmt|;
name|SliceWriter
name|writer
init|=
name|convertToFixedWidth
argument_list|(
name|sorted
argument_list|)
decl_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
return|return
name|writer
return|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|rm
argument_list|(
name|sorted
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|deleteFilesIgnoringExceptions
argument_list|(
name|sorted
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/** Writes the 1d BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */
DECL|method|finish
specifier|public
name|long
name|finish
parameter_list|(
name|IndexOutput
name|out
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|valueCount
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"at least one value must be indexed"
argument_list|)
throw|;
block|}
comment|// TODO: we should use in-memory sort here, if number of points is small enough:
name|long
name|countPerLeaf
init|=
name|valueCount
decl_stmt|;
name|long
name|innerNodeCount
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|countPerLeaf
operator|>
name|maxValuesInLeafNode
condition|)
block|{
name|countPerLeaf
operator|=
operator|(
name|countPerLeaf
operator|+
literal|1
operator|)
operator|/
literal|2
expr_stmt|;
name|innerNodeCount
operator|*=
literal|2
expr_stmt|;
block|}
comment|//System.out.println("innerNodeCount=" + innerNodeCount);
if|if
condition|(
literal|1
operator|+
literal|2
operator|*
name|innerNodeCount
operator|>=
name|Integer
operator|.
name|MAX_VALUE
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"too many nodes; increase maxValuesInLeafNode (currently "
operator|+
name|maxValuesInLeafNode
operator|+
literal|") and reindex"
argument_list|)
throw|;
block|}
name|innerNodeCount
operator|--
expr_stmt|;
name|int
name|numLeaves
init|=
call|(
name|int
call|)
argument_list|(
name|innerNodeCount
operator|+
literal|1
argument_list|)
decl_stmt|;
comment|// Indexed by nodeID, but first (root) nodeID is 1
name|long
index|[]
name|blockMinValues
init|=
operator|new
name|long
index|[
name|numLeaves
index|]
decl_stmt|;
comment|// +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)
name|long
index|[]
name|leafBlockFPs
init|=
operator|new
name|long
index|[
name|numLeaves
index|]
decl_stmt|;
comment|// Make sure the math above "worked":
assert|assert
name|valueCount
operator|/
name|blockMinValues
operator|.
name|length
operator|<=
name|maxValuesInLeafNode
operator|:
literal|"valueCount="
operator|+
name|valueCount
operator|+
literal|" blockMinValues.length="
operator|+
name|blockMinValues
operator|.
name|length
operator|+
literal|" maxValuesInLeafNode="
operator|+
name|maxValuesInLeafNode
assert|;
comment|//System.out.println("  avg pointsPerLeaf=" + (valueCount/blockMinValues.length));
comment|// Sort all docs by value:
name|SliceWriter
name|sortedWriter
init|=
literal|null
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|sortedWriter
operator|=
name|sort
argument_list|()
expr_stmt|;
name|heapWriter
operator|=
literal|null
expr_stmt|;
name|build
argument_list|(
literal|1
argument_list|,
name|numLeaves
argument_list|,
operator|new
name|PathSlice
argument_list|(
name|sortedWriter
argument_list|,
literal|0
argument_list|,
name|valueCount
argument_list|)
argument_list|,
name|out
argument_list|,
name|globalMinValue
argument_list|,
name|globalMaxValue
argument_list|,
name|blockMinValues
argument_list|,
name|leafBlockFPs
argument_list|)
expr_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|sortedWriter
operator|.
name|destroy
argument_list|()
expr_stmt|;
name|IOUtils
operator|.
name|rm
argument_list|(
name|tempInput
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|sortedWriter
operator|.
name|destroy
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
comment|// Suppress to keep throwing original exc
block|}
name|IOUtils
operator|.
name|deleteFilesIgnoringExceptions
argument_list|(
name|tempInput
argument_list|)
expr_stmt|;
block|}
block|}
comment|//System.out.println("Total nodes: " + innerNodeCount);
comment|// Write index:
name|long
name|indexFP
init|=
name|out
operator|.
name|getFilePointer
argument_list|()
decl_stmt|;
name|out
operator|.
name|writeVInt
argument_list|(
name|numLeaves
argument_list|)
expr_stmt|;
name|out
operator|.
name|writeVInt
argument_list|(
call|(
name|int
call|)
argument_list|(
name|valueCount
operator|/
name|numLeaves
argument_list|)
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|blockMinValues
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|out
operator|.
name|writeLong
argument_list|(
name|blockMinValues
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|leafBlockFPs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|out
operator|.
name|writeVLong
argument_list|(
name|leafBlockFPs
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
name|out
operator|.
name|writeLong
argument_list|(
name|globalMaxValue
argument_list|)
expr_stmt|;
return|return
name|indexFP
return|;
block|}
comment|// Called only from assert
DECL|method|directoryIsEmpty
specifier|private
name|boolean
name|directoryIsEmpty
parameter_list|(
name|Path
name|in
parameter_list|)
block|{
try|try
init|(
name|DirectoryStream
argument_list|<
name|Path
argument_list|>
name|dir
init|=
name|Files
operator|.
name|newDirectoryStream
argument_list|(
name|in
argument_list|)
init|)
block|{
for|for
control|(
name|Path
name|path
range|:
name|dir
control|)
block|{
assert|assert
literal|false
operator|:
literal|"dir="
operator|+
name|in
operator|+
literal|" still has file="
operator|+
name|path
assert|;
return|return
literal|false
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
comment|// Just ignore: we are only called from assert
block|}
return|return
literal|true
return|;
block|}
comment|/** Sliced reference to points in an OfflineSorter.ByteSequencesWriter file. */
DECL|class|PathSlice
specifier|private
specifier|static
specifier|final
class|class
name|PathSlice
block|{
DECL|field|writer
specifier|final
name|SliceWriter
name|writer
decl_stmt|;
DECL|field|start
specifier|final
name|long
name|start
decl_stmt|;
DECL|field|count
specifier|final
name|long
name|count
decl_stmt|;
DECL|method|PathSlice
specifier|public
name|PathSlice
parameter_list|(
name|SliceWriter
name|writer
parameter_list|,
name|long
name|start
parameter_list|,
name|long
name|count
parameter_list|)
block|{
name|this
operator|.
name|writer
operator|=
name|writer
expr_stmt|;
name|this
operator|.
name|start
operator|=
name|start
expr_stmt|;
name|this
operator|.
name|count
operator|=
name|count
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|toString
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"PathSlice(start="
operator|+
name|start
operator|+
literal|" count="
operator|+
name|count
operator|+
literal|" writer="
operator|+
name|writer
operator|+
literal|")"
return|;
block|}
block|}
DECL|method|getSplitValue
specifier|private
name|long
name|getSplitValue
parameter_list|(
name|PathSlice
name|source
parameter_list|,
name|long
name|leftCount
parameter_list|,
name|long
name|minValue
parameter_list|,
name|long
name|maxValue
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Read the split value:
name|SliceReader
name|reader
init|=
name|source
operator|.
name|writer
operator|.
name|getReader
argument_list|(
name|source
operator|.
name|start
operator|+
name|leftCount
argument_list|)
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
name|long
name|splitValue
decl_stmt|;
try|try
block|{
name|boolean
name|result
init|=
name|reader
operator|.
name|next
argument_list|()
decl_stmt|;
assert|assert
name|result
assert|;
name|splitValue
operator|=
name|reader
operator|.
name|value
argument_list|()
expr_stmt|;
assert|assert
name|splitValue
operator|>=
name|minValue
operator|&&
name|splitValue
operator|<=
name|maxValue
operator|:
literal|"splitValue="
operator|+
name|splitValue
operator|+
literal|" minValue="
operator|+
name|minValue
operator|+
literal|" maxValue="
operator|+
name|maxValue
operator|+
literal|" reader="
operator|+
name|reader
assert|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|closeWhileHandlingException
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|splitValue
return|;
block|}
comment|/** The incoming PathSlice for the dim we will split is already partitioned/sorted. */
DECL|method|build
specifier|private
name|void
name|build
parameter_list|(
name|int
name|nodeID
parameter_list|,
name|int
name|leafNodeOffset
parameter_list|,
name|PathSlice
name|source
parameter_list|,
name|IndexOutput
name|out
parameter_list|,
name|long
name|minValue
parameter_list|,
name|long
name|maxValue
parameter_list|,
name|long
index|[]
name|blockMinValues
parameter_list|,
name|long
index|[]
name|leafBlockFPs
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|count
init|=
name|source
operator|.
name|count
decl_stmt|;
if|if
condition|(
name|source
operator|.
name|writer
operator|instanceof
name|OfflineSliceWriter
operator|&&
name|count
operator|<=
name|maxValuesSortInHeap
condition|)
block|{
comment|// Cutover to heap:
name|SliceWriter
name|writer
init|=
operator|new
name|HeapSliceWriter
argument_list|(
operator|(
name|int
operator|)
name|count
argument_list|)
decl_stmt|;
name|SliceReader
name|reader
init|=
name|source
operator|.
name|writer
operator|.
name|getReader
argument_list|(
name|source
operator|.
name|start
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|count
condition|;
name|i
operator|++
control|)
block|{
name|boolean
name|hasNext
init|=
name|reader
operator|.
name|next
argument_list|()
decl_stmt|;
assert|assert
name|hasNext
assert|;
name|writer
operator|.
name|append
argument_list|(
name|reader
operator|.
name|value
argument_list|()
argument_list|,
name|reader
operator|.
name|ord
argument_list|()
argument_list|,
name|reader
operator|.
name|docID
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|reader
argument_list|,
name|writer
argument_list|)
expr_stmt|;
block|}
name|source
operator|=
operator|new
name|PathSlice
argument_list|(
name|writer
argument_list|,
literal|0
argument_list|,
name|count
argument_list|)
expr_stmt|;
block|}
comment|// We should never hit dead-end nodes on recursion even in the adversarial cases:
assert|assert
name|count
operator|>
literal|0
assert|;
if|if
condition|(
name|nodeID
operator|>=
name|leafNodeOffset
condition|)
block|{
comment|// Leaf node: write block
assert|assert
name|maxValue
operator|>=
name|minValue
assert|;
comment|//System.out.println("\nleaf:\n  lat range: " + ((long) maxLatEnc-minLatEnc));
comment|//System.out.println("  lon range: " + ((long) maxLonEnc-minLonEnc));
comment|// Sort by docID in the leaf so we can .or(DISI) at search time:
name|SliceReader
name|reader
init|=
name|source
operator|.
name|writer
operator|.
name|getReader
argument_list|(
name|source
operator|.
name|start
argument_list|)
decl_stmt|;
name|int
index|[]
name|docIDs
init|=
operator|new
name|int
index|[
operator|(
name|int
operator|)
name|count
index|]
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|source
operator|.
name|count
condition|;
name|i
operator|++
control|)
block|{
comment|// NOTE: we discard ord at this point; we only needed it temporarily
comment|// during building to uniquely identify each point to properly handle
comment|// the multi-valued case (one docID having multiple values):
comment|// We also discard lat/lon, since at search time, we reside on the
comment|// wrapped doc values for this:
name|boolean
name|result
init|=
name|reader
operator|.
name|next
argument_list|()
decl_stmt|;
assert|assert
name|result
assert|;
name|docIDs
index|[
name|i
index|]
operator|=
name|reader
operator|.
name|docID
argument_list|()
expr_stmt|;
block|}
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|closeWhileHandlingException
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
block|}
comment|// TODO: not clear we need to do this anymore (we used to make a DISI over
comment|// the block at search time), but maybe it buys some memory
comment|// locality/sequentiality at search time?
name|Arrays
operator|.
name|sort
argument_list|(
name|docIDs
argument_list|)
expr_stmt|;
comment|// Dedup docIDs: for the multi-valued case where more than one value for the doc
comment|// wound up in this leaf cell, we only need to store the docID once:
name|int
name|lastDocID
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|uniqueCount
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|docIDs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|int
name|docID
init|=
name|docIDs
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|docID
operator|!=
name|lastDocID
condition|)
block|{
name|uniqueCount
operator|++
expr_stmt|;
name|lastDocID
operator|=
name|docID
expr_stmt|;
block|}
block|}
assert|assert
name|uniqueCount
operator|<=
name|count
assert|;
comment|// TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know
comment|// how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?
name|long
name|startFP
init|=
name|out
operator|.
name|getFilePointer
argument_list|()
decl_stmt|;
name|out
operator|.
name|writeVInt
argument_list|(
name|uniqueCount
argument_list|)
expr_stmt|;
comment|// Save the block file pointer:
name|int
name|blockID
init|=
name|nodeID
operator|-
name|leafNodeOffset
decl_stmt|;
name|leafBlockFPs
index|[
name|blockID
index|]
operator|=
name|startFP
expr_stmt|;
comment|//System.out.println("    leafFP=" + startFP);
name|blockMinValues
index|[
name|blockID
index|]
operator|=
name|minValue
expr_stmt|;
name|lastDocID
operator|=
operator|-
literal|1
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|docIDs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Absolute int encode; with "vInt of deltas" encoding, the .kdd size dropped from
comment|// 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.
comment|// I think if we also indexed prefix terms here we could do less costly compression
comment|// on those lists:
name|int
name|docID
init|=
name|docIDs
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|docID
operator|!=
name|lastDocID
condition|)
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|docID
argument_list|)
expr_stmt|;
name|lastDocID
operator|=
name|docID
expr_stmt|;
block|}
block|}
comment|//long endFP = out.getFilePointer();
comment|//System.out.println("  bytes/doc: " + ((endFP - startFP) / count));
block|}
else|else
block|{
comment|// Inner node: sort, partition/recurse
assert|assert
name|nodeID
operator|<
name|blockMinValues
operator|.
name|length
operator|:
literal|"nodeID="
operator|+
name|nodeID
operator|+
literal|" blockMinValues.length="
operator|+
name|blockMinValues
operator|.
name|length
assert|;
assert|assert
name|source
operator|.
name|count
operator|==
name|count
assert|;
name|long
name|leftCount
init|=
name|source
operator|.
name|count
operator|/
literal|2
decl_stmt|;
comment|// NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we
comment|// must search both left and right trees:
name|long
name|splitValue
init|=
name|getSplitValue
argument_list|(
name|source
argument_list|,
name|leftCount
argument_list|,
name|minValue
argument_list|,
name|maxValue
argument_list|)
decl_stmt|;
name|build
argument_list|(
literal|2
operator|*
name|nodeID
argument_list|,
name|leafNodeOffset
argument_list|,
operator|new
name|PathSlice
argument_list|(
name|source
operator|.
name|writer
argument_list|,
name|source
operator|.
name|start
argument_list|,
name|leftCount
argument_list|)
argument_list|,
name|out
argument_list|,
name|minValue
argument_list|,
name|splitValue
argument_list|,
name|blockMinValues
argument_list|,
name|leafBlockFPs
argument_list|)
expr_stmt|;
name|build
argument_list|(
literal|2
operator|*
name|nodeID
operator|+
literal|1
argument_list|,
name|leafNodeOffset
argument_list|,
operator|new
name|PathSlice
argument_list|(
name|source
operator|.
name|writer
argument_list|,
name|source
operator|.
name|start
operator|+
name|leftCount
argument_list|,
name|count
operator|-
name|leftCount
argument_list|)
argument_list|,
name|out
argument_list|,
name|splitValue
argument_list|,
name|maxValue
argument_list|,
name|blockMinValues
argument_list|,
name|leafBlockFPs
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|getWriter
name|SliceWriter
name|getWriter
parameter_list|(
name|long
name|count
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|count
operator|<
name|maxValuesSortInHeap
condition|)
block|{
return|return
operator|new
name|HeapSliceWriter
argument_list|(
operator|(
name|int
operator|)
name|count
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|new
name|OfflineSliceWriter
argument_list|(
name|count
argument_list|)
return|;
block|}
block|}
block|}
end_class
end_unit
