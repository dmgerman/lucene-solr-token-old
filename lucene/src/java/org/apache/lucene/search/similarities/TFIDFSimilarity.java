begin_unit
begin_package
DECL|package|org.apache.lucene.search.similarities
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|similarities
package|;
end_package
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|DocValues
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|AtomicReader
operator|.
name|AtomicReaderContext
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|CollectionStatistics
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|Explanation
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|IndexSearcher
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|PhraseQuery
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|TermStatistics
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|SmallFloat
import|;
end_import
begin_comment
comment|/**  * Implementation of {@link Similarity} with the Vector Space Model.  *<p>  * Expert: Scoring API.  *<p>TFIDFSimilarity defines the components of Lucene scoring.  * Overriding computation of these components is a convenient  * way to alter Lucene scoring.  *  *<p>Suggested reading:  *<a href="http://nlp.stanford.edu/IR-book/html/htmledition/queries-as-vectors-1.html">  * Introduction To Information Retrieval, Chapter 6</a>.  *  *<p>The following describes how Lucene scoring evolves from  * underlying information retrieval models to (efficient) implementation.  * We first brief on<i>VSM Score</i>,   * then derive from it<i>Lucene's Conceptual Scoring Formula</i>,  * from which, finally, evolves<i>Lucene's Practical Scoring Function</i>   * (the latter is connected directly with Lucene classes and methods).      *  *<p>Lucene combines  *<a href="http://en.wikipedia.org/wiki/Standard_Boolean_model">  * Boolean model (BM) of Information Retrieval</a>  * with  *<a href="http://en.wikipedia.org/wiki/Vector_Space_Model">  * Vector Space Model (VSM) of Information Retrieval</a> -  * documents "approved" by BM are scored by VSM.  *  *<p>In VSM, documents and queries are represented as  * weighted vectors in a multi-dimensional space,  * where each distinct index term is a dimension,  * and weights are  *<a href="http://en.wikipedia.org/wiki/Tfidf">Tf-idf</a> values.  *  *<p>VSM does not require weights to be<i>Tf-idf</i> values,  * but<i>Tf-idf</i> values are believed to produce search results of high quality,  * and so Lucene is using<i>Tf-idf</i>.  *<i>Tf</i> and<i>Idf</i> are described in more detail below,  * but for now, for completion, let's just say that  * for given term<i>t</i> and document (or query)<i>x</i>,  *<i>Tf(t,x)</i> varies with the number of occurrences of term<i>t</i> in<i>x</i>  * (when one increases so does the other) and  *<i>idf(t)</i> similarly varies with the inverse of the  * number of index documents containing term<i>t</i>.  *  *<p><i>VSM score</i> of document<i>d</i> for query<i>q</i> is the  *<a href="http://en.wikipedia.org/wiki/Cosine_similarity">  * Cosine Similarity</a>  * of the weighted query vectors<i>V(q)</i> and<i>V(d)</i>:  *  *<br>&nbsp;<br>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr><td>  *<table cellpadding="1" cellspacing="0" border="1" align="center">  *<tr><td>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            cosine-similarity(q,d)&nbsp; =&nbsp;  *</td>  *<td valign="middle" align="center">  *<table>  *<tr><td align="center"><small>V(q)&nbsp;&middot;&nbsp;V(d)</small></td></tr>  *<tr><td align="center">&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;</td></tr>  *<tr><td align="center"><small>|V(q)|&nbsp;|V(d)|</small></td></tr>  *</table>  *</td>  *</tr>  *</table>  *</td></tr>  *</table>  *</td></tr>  *<tr><td>  *<center><font=-1><u>VSM Score</u></font></center>  *</td></tr>  *</table>  *<br>&nbsp;<br>  *     *  * Where<i>V(q)</i>&middot;<i>V(d)</i> is the  *<a href="http://en.wikipedia.org/wiki/Dot_product">dot product</a>  * of the weighted vectors,  * and<i>|V(q)|</i> and<i>|V(d)|</i> are their  *<a href="http://en.wikipedia.org/wiki/Euclidean_norm#Euclidean_norm">Euclidean norms</a>.  *  *<p>Note: the above equation can be viewed as the dot product of  * the normalized weighted vectors, in the sense that dividing  *<i>V(q)</i> by its euclidean norm is normalizing it to a unit vector.  *  *<p>Lucene refines<i>VSM score</i> for both search quality and usability:  *<ul>  *<li>Normalizing<i>V(d)</i> to the unit vector is known to be problematic in that   *  it removes all document length information.   *  For some documents removing this info is probably ok,   *  e.g. a document made by duplicating a certain paragraph<i>10</i> times,  *  especially if that paragraph is made of distinct terms.   *  But for a document which contains no duplicated paragraphs,   *  this might be wrong.   *  To avoid this problem, a different document length normalization   *  factor is used, which normalizes to a vector equal to or larger   *  than the unit vector:<i>doc-len-norm(d)</i>.  *</li>  *  *<li>At indexing, users can specify that certain documents are more  *  important than others, by assigning a document boost.  *  For this, the score of each document is also multiplied by its boost value  *<i>doc-boost(d)</i>.  *</li>  *  *<li>Lucene is field based, hence each query term applies to a single  *  field, document length normalization is by the length of the certain field,  *  and in addition to document boost there are also document fields boosts.  *</li>  *  *<li>The same field can be added to a document during indexing several times,  *  and so the boost of that field is the multiplication of the boosts of  *  the separate additions (or parts) of that field within the document.  *</li>  *  *<li>At search time users can specify boosts to each query, sub-query, and  *  each query term, hence the contribution of a query term to the score of  *  a document is multiplied by the boost of that query term<i>query-boost(q)</i>.  *</li>  *  *<li>A document may match a multi term query without containing all  *  the terms of that query (this is correct for some of the queries),  *  and users can further reward documents matching more query terms  *  through a coordination factor, which is usually larger when  *  more terms are matched:<i>coord-factor(q,d)</i>.  *</li>  *</ul>  *  *<p>Under the simplifying assumption of a single field in the index,  * we get<i>Lucene's Conceptual scoring formula</i>:  *  *<br>&nbsp;<br>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr><td>  *<table cellpadding="1" cellspacing="0" border="1" align="center">  *<tr><td>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            score(q,d)&nbsp; =&nbsp;  *<font color="#FF9933">coord-factor(q,d)</font>&middot;&nbsp;  *<font color="#CCCC00">query-boost(q)</font>&middot;&nbsp;  *</td>  *<td valign="middle" align="center">  *<table>  *<tr><td align="center"><small><font color="#993399">V(q)&nbsp;&middot;&nbsp;V(d)</font></small></td></tr>  *<tr><td align="center">&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;</td></tr>  *<tr><td align="center"><small><font color="#FF33CC">|V(q)|</font></small></td></tr>  *</table>  *</td>  *<td valign="middle" align="right" rowspan="1">  *&nbsp;&middot;&nbsp;<font color="#3399FF">doc-len-norm(d)</font>  *&nbsp;&middot;&nbsp;<font color="#3399FF">doc-boost(d)</font>  *</td>  *</tr>  *</table>  *</td></tr>  *</table>  *</td></tr>  *<tr><td>  *<center><font=-1><u>Lucene Conceptual Scoring Formula</u></font></center>  *</td></tr>  *</table>  *<br>&nbsp;<br>  *  *<p>The conceptual formula is a simplification in the sense that (1) terms and documents  * are fielded and (2) boosts are usually per query term rather than per query.  *  *<p>We now describe how Lucene implements this conceptual scoring formula, and  * derive from it<i>Lucene's Practical Scoring Function</i>.  *    *<p>For efficient score computation some scoring components  * are computed and aggregated in advance:  *  *<ul>  *<li><i>Query-boost</i> for the query (actually for each query term)  *  is known when search starts.  *</li>  *  *<li>Query Euclidean norm<i>|V(q)|</i> can be computed when search starts,  *  as it is independent of the document being scored.  *  From search optimization perspective, it is a valid question  *  why bother to normalize the query at all, because all  *  scored documents will be multiplied by the same<i>|V(q)|</i>,  *  and hence documents ranks (their order by score) will not  *  be affected by this normalization.  *  There are two good reasons to keep this normalization:  *<ul>  *<li>Recall that  *<a href="http://en.wikipedia.org/wiki/Cosine_similarity">  *   Cosine Similarity</a> can be used find how similar  *   two documents are. One can use Lucene for e.g.  *   clustering, and use a document as a query to compute  *   its similarity to other documents.  *   In this use case it is important that the score of document<i>d3</i>  *   for query<i>d1</i> is comparable to the score of document<i>d3</i>  *   for query<i>d2</i>. In other words, scores of a document for two  *   distinct queries should be comparable.  *   There are other applications that may require this.  *   And this is exactly what normalizing the query vector<i>V(q)</i>  *   provides: comparability (to a certain extent) of two or more queries.  *</li>  *  *<li>Applying query normalization on the scores helps to keep the  *   scores around the unit vector, hence preventing loss of score data  *   because of floating point precision limitations.  *</li>  *</ul>  *</li>  *  *<li>Document length norm<i>doc-len-norm(d)</i> and document  *  boost<i>doc-boost(d)</i> are known at indexing time.  *  They are computed in advance and their multiplication  *  is saved as a single value in the index:<i>norm(d)</i>.  *  (In the equations below,<i>norm(t in d)</i> means<i>norm(field(t) in doc d)</i>  *  where<i>field(t)</i> is the field associated with term<i>t</i>.)  *</li>  *</ul>  *  *<p><i>Lucene's Practical Scoring Function</i> is derived from the above.  * The color codes demonstrate how it relates  * to those of the<i>conceptual</i> formula:  *  *<P>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr><td>  *<table cellpadding="" cellspacing="2" border="2" align="center">  *<tr><td>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *       score(q,d)&nbsp; =&nbsp;  *<A HREF="#formula_coord"><font color="#FF9933">coord(q,d)</font></A>&nbsp;&middot;&nbsp;  *<A HREF="#formula_queryNorm"><font color="#FF33CC">queryNorm(q)</font></A>&nbsp;&middot;&nbsp;  *</td>  *<td valign="bottom" align="center" rowspan="1">  *<big><big><big>&sum;</big></big></big>  *</td>  *<td valign="middle" align="right" rowspan="1">  *<big><big>(</big></big>  *<A HREF="#formula_tf"><font color="#993399">tf(t in d)</font></A>&nbsp;&middot;&nbsp;  *<A HREF="#formula_idf"><font color="#993399">idf(t)</font></A><sup>2</sup>&nbsp;&middot;&nbsp;  *<A HREF="#formula_termBoost"><font color="#CCCC00">t.getBoost()</font></A>&nbsp;&middot;&nbsp;  *<A HREF="#formula_norm"><font color="#3399FF">norm(t,d)</font></A>  *<big><big>)</big></big>  *</td>  *</tr>  *<tr valigh="top">  *<td></td>  *<td align="center"><small>t in q</small></td>  *<td></td>  *</tr>  *</table>  *</td></tr>  *</table>  *</td></tr>  *<tr><td>  *<center><font=-1><u>Lucene Practical Scoring Function</u></font></center>  *</td></tr>  *</table>  *  *<p> where  *<ol>  *<li>  *<A NAME="formula_tf"></A>  *<b><i>tf(t in d)</i></b>  *      correlates to the term's<i>frequency</i>,  *      defined as the number of times term<i>t</i> appears in the currently scored document<i>d</i>.  *      Documents that have more occurrences of a given term receive a higher score.  *      Note that<i>tf(t in q)</i> is assumed to be<i>1</i> and therefore it does not appear in this equation,  *      However if a query contains twice the same term, there will be  *      two term-queries with that same term and hence the computation would still be correct (although  *      not very efficient).  *      The default computation for<i>tf(t in d)</i> in  *      {@link org.apache.lucene.search.similarities.DefaultSimilarity#tf(float) DefaultSimilarity} is:  *  *<br>&nbsp;<br>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            {@link org.apache.lucene.search.similarities.DefaultSimilarity#tf(float) tf(t in d)}&nbsp; =&nbsp;  *</td>  *<td valign="top" align="center" rowspan="1">  *               frequency<sup><big>&frac12;</big></sup>  *</td>  *</tr>  *</table>  *<br>&nbsp;<br>  *</li>  *  *<li>  *<A NAME="formula_idf"></A>  *<b><i>idf(t)</i></b> stands for Inverse Document Frequency. This value  *      correlates to the inverse of<i>docFreq</i>  *      (the number of documents in which the term<i>t</i> appears).  *      This means rarer terms give higher contribution to the total score.  *<i>idf(t)</i> appears for<i>t</i> in both the query and the document,  *      hence it is squared in the equation.  *      The default computation for<i>idf(t)</i> in  *      {@link org.apache.lucene.search.similarities.DefaultSimilarity#idf(long, long) DefaultSimilarity} is:  *  *<br>&nbsp;<br>  *<table cellpadding="2" cellspacing="2" border="0" align="center">  *<tr>  *<td valign="middle" align="right">  *            {@link org.apache.lucene.search.similarities.DefaultSimilarity#idf(long, long) idf(t)}&nbsp; =&nbsp;  *</td>  *<td valign="middle" align="center">  *            1 + log<big>(</big>  *</td>  *<td valign="middle" align="center">  *<table>  *<tr><td align="center"><small>numDocs</small></td></tr>  *<tr><td align="center">&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;</td></tr>  *<tr><td align="center"><small>docFreq+1</small></td></tr>  *</table>  *</td>  *<td valign="middle" align="center">  *<big>)</big>  *</td>  *</tr>  *</table>  *<br>&nbsp;<br>  *</li>  *  *<li>  *<A NAME="formula_coord"></A>  *<b><i>coord(q,d)</i></b>  *      is a score factor based on how many of the query terms are found in the specified document.  *      Typically, a document that contains more of the query's terms will receive a higher score  *      than another document with fewer query terms.  *      This is a search time factor computed in  *      {@link SimilarityProvider#coord(int, int) coord(q,d)}  *      by the SimilarityProvider in effect at search time.  *<br>&nbsp;<br>  *</li>  *  *<li><b>  *<A NAME="formula_queryNorm"></A>  *<i>queryNorm(q)</i>  *</b>  *      is a normalizing factor used to make scores between queries comparable.  *      This factor does not affect document ranking (since all ranked documents are multiplied by the same factor),  *      but rather just attempts to make scores from different queries (or even different indexes) comparable.  *      This is a search time factor computed by the Similarity in effect at search time.  *  *      The default computation in  *      {@link org.apache.lucene.search.similarities.DefaultSimilarityProvider#queryNorm(float) DefaultSimilarityProvider}  *      produces a<a href="http://en.wikipedia.org/wiki/Euclidean_norm#Euclidean_norm">Euclidean norm</a>:  *<br>&nbsp;<br>  *<table cellpadding="1" cellspacing="0" border="0" align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            queryNorm(q)&nbsp; =&nbsp;  *            {@link org.apache.lucene.search.similarities.DefaultSimilarityProvider#queryNorm(float) queryNorm(sumOfSquaredWeights)}  *&nbsp; =&nbsp;  *</td>  *<td valign="middle" align="center" rowspan="1">  *<table>  *<tr><td align="center"><big>1</big></td></tr>  *<tr><td align="center"><big>  *&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;  *</big></td></tr>  *<tr><td align="center">sumOfSquaredWeights<sup><big>&frac12;</big></sup></td></tr>  *</table>  *</td>  *</tr>  *</table>  *<br>&nbsp;<br>  *  *      The sum of squared weights (of the query terms) is  *      computed by the query {@link org.apache.lucene.search.Weight} object.  *      For example, a {@link org.apache.lucene.search.BooleanQuery}  *      computes this value as:  *  *<br>&nbsp;<br>  *<table cellpadding="1" cellspacing="0" border="0"n align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            {@link org.apache.lucene.search.Weight#getValueForNormalization() sumOfSquaredWeights}&nbsp; =&nbsp;  *            {@link org.apache.lucene.search.Query#getBoost() q.getBoost()}<sup><big>2</big></sup>  *&nbsp;&middot;&nbsp;  *</td>  *<td valign="bottom" align="center" rowspan="1">  *<big><big><big>&sum;</big></big></big>  *</td>  *<td valign="middle" align="right" rowspan="1">  *<big><big>(</big></big>  *<A HREF="#formula_idf">idf(t)</A>&nbsp;&middot;&nbsp;  *<A HREF="#formula_termBoost">t.getBoost()</A>  *<big><big>)<sup>2</sup></big></big>  *</td>  *</tr>  *<tr valigh="top">  *<td></td>  *<td align="center"><small>t in q</small></td>  *<td></td>  *</tr>  *</table>  *<br>&nbsp;<br>  *  *</li>  *  *<li>  *<A NAME="formula_termBoost"></A>  *<b><i>t.getBoost()</i></b>  *      is a search time boost of term<i>t</i> in the query<i>q</i> as  *      specified in the query text  *      (see<A HREF="../../../../../../queryparsersyntax.html#Boosting a Term">query syntax</A>),  *      or as set by application calls to  *      {@link org.apache.lucene.search.Query#setBoost(float) setBoost()}.  *      Notice that there is really no direct API for accessing a boost of one term in a multi term query,  *      but rather multi terms are represented in a query as multi  *      {@link org.apache.lucene.search.TermQuery TermQuery} objects,  *      and so the boost of a term in the query is accessible by calling the sub-query  *      {@link org.apache.lucene.search.Query#getBoost() getBoost()}.  *<br>&nbsp;<br>  *</li>  *  *<li>  *<A NAME="formula_norm"></A>  *<b><i>norm(t,d)</i></b> encapsulates a few (indexing time) boost and length factors:  *  *<ul>  *<li><b>Field boost</b> - set by calling  *        {@link org.apache.lucene.document.Field#setBoost(float) field.setBoost()}  *        before adding the field to a document.  *</li>  *<li><b>lengthNorm</b> - computed  *        when the document is added to the index in accordance with the number of tokens  *        of this field in the document, so that shorter fields contribute more to the score.  *        LengthNorm is computed by the Similarity class in effect at indexing.  *</li>  *</ul>  *      The {@link #computeNorm} method is responsible for  *      combining all of these factors into a single float.  *  *<p>  *      When a document is added to the index, all the above factors are multiplied.  *      If the document has multiple fields with the same name, all their boosts are multiplied together:  *  *<br>&nbsp;<br>  *<table cellpadding="1" cellspacing="0" border="0"n align="center">  *<tr>  *<td valign="middle" align="right" rowspan="1">  *            norm(t,d)&nbsp; =&nbsp;  *            lengthNorm  *&nbsp;&middot;&nbsp;  *</td>  *<td valign="bottom" align="center" rowspan="1">  *<big><big><big>&prod;</big></big></big>  *</td>  *<td valign="middle" align="right" rowspan="1">  *            {@link org.apache.lucene.index.IndexableField#boost() f.boost}()  *</td>  *</tr>  *<tr valigh="top">  *<td></td>  *<td align="center"><small>field<i><b>f</b></i> in<i>d</i> named as<i><b>t</b></i></small></td>  *<td></td>  *</tr>  *</table>  *<br>&nbsp;<br>  *      However the resulted<i>norm</i> value is {@link #encodeNormValue(float) encoded} as a single byte  *      before being stored.  *      At search time, the norm byte value is read from the index  *      {@link org.apache.lucene.store.Directory directory} and  *      {@link #decodeNormValue(byte) decoded} back to a float<i>norm</i> value.  *      This encoding/decoding, while reducing index size, comes with the price of  *      precision loss - it is not guaranteed that<i>decode(encode(x)) = x</i>.  *      For instance,<i>decode(encode(0.89)) = 0.75</i>.  *<br>&nbsp;<br>  *      Compression of norm values to a single byte saves memory at search time,   *      because once a field is referenced at search time, its norms - for   *      all documents - are maintained in memory.  *<br>&nbsp;<br>  *      The rationale supporting such lossy compression of norm values is that  *      given the difficulty (and inaccuracy) of users to express their true information  *      need by a query, only big differences matter.  *<br>&nbsp;<br>  *      Last, note that search time is too late to modify this<i>norm</i> part of scoring, e.g. by  *      using a different {@link Similarity} for search.  *<br>&nbsp;<br>  *</li>  *</ol>  *  * @see org.apache.lucene.index.IndexWriterConfig#setSimilarityProvider(SimilarityProvider)  * @see IndexSearcher#setSimilarityProvider(SimilarityProvider)  */
end_comment
begin_class
DECL|class|TFIDFSimilarity
specifier|public
specifier|abstract
class|class
name|TFIDFSimilarity
extends|extends
name|Similarity
block|{
comment|/** Computes a score factor based on a term or phrase's frequency in a    * document.  This value is multiplied by the {@link #idf(long, long)}    * factor for each term in the query and these products are then summed to    * form the initial score for a document.    *    *<p>Terms and phrases repeated in a document indicate the topic of the    * document, so implementations of this method usually return larger values    * when<code>freq</code> is large, and smaller values when<code>freq</code>    * is small.    *    *<p>The default implementation calls {@link #tf(float)}.    *    * @param freq the frequency of a term within a document    * @return a score factor based on a term's within-document frequency    */
DECL|method|tf
specifier|public
name|float
name|tf
parameter_list|(
name|int
name|freq
parameter_list|)
block|{
return|return
name|tf
argument_list|(
operator|(
name|float
operator|)
name|freq
argument_list|)
return|;
block|}
comment|/** Computes a score factor based on a term or phrase's frequency in a    * document.  This value is multiplied by the {@link #idf(long, long)}    * factor for each term in the query and these products are then summed to    * form the initial score for a document.    *    *<p>Terms and phrases repeated in a document indicate the topic of the    * document, so implementations of this method usually return larger values    * when<code>freq</code> is large, and smaller values when<code>freq</code>    * is small.    *    * @param freq the frequency of a term within a document    * @return a score factor based on a term's within-document frequency    */
DECL|method|tf
specifier|public
specifier|abstract
name|float
name|tf
parameter_list|(
name|float
name|freq
parameter_list|)
function_decl|;
comment|/**    * Computes a score factor for a simple term and returns an explanation    * for that score factor.    *     *<p>    * The default implementation uses:    *     *<pre>    * idf(docFreq, searcher.maxDoc());    *</pre>    *     * Note that {@link CollectionStatistics#maxDoc()} is used instead of    * {@link org.apache.lucene.index.IndexReader#numDocs() IndexReader#numDocs()} because also     * {@link TermStatistics#docFreq()} is used, and when the latter     * is inaccurate, so is {@link CollectionStatistics#maxDoc()}, and in the same direction.    * In addition, {@link CollectionStatistics#maxDoc()} is more efficient to compute    *       * @param collectionStats collection-level statistics    * @param termStats term-level statistics for the term    * @return an Explain object that includes both an idf score factor               and an explanation for the term.    * @throws IOException    */
DECL|method|idfExplain
specifier|public
name|Explanation
name|idfExplain
parameter_list|(
name|CollectionStatistics
name|collectionStats
parameter_list|,
name|TermStatistics
name|termStats
parameter_list|)
block|{
specifier|final
name|long
name|df
init|=
name|termStats
operator|.
name|docFreq
argument_list|()
decl_stmt|;
specifier|final
name|long
name|max
init|=
name|collectionStats
operator|.
name|maxDoc
argument_list|()
decl_stmt|;
specifier|final
name|float
name|idf
init|=
name|idf
argument_list|(
name|df
argument_list|,
name|max
argument_list|)
decl_stmt|;
return|return
operator|new
name|Explanation
argument_list|(
name|idf
argument_list|,
literal|"idf(docFreq="
operator|+
name|df
operator|+
literal|", maxDocs="
operator|+
name|max
operator|+
literal|")"
argument_list|)
return|;
block|}
comment|/**    * Computes a score factor for a phrase.    *     *<p>    * The default implementation sums the idf factor for    * each term in the phrase.    *     * @param collectionStats collection-level statistics    * @param termStats term-level statistics for the terms in the phrase    * @return an Explain object that includes both an idf     *         score factor for the phrase and an explanation     *         for each term.    * @throws IOException    */
DECL|method|idfExplain
specifier|public
name|Explanation
name|idfExplain
parameter_list|(
name|CollectionStatistics
name|collectionStats
parameter_list|,
name|TermStatistics
name|termStats
index|[]
parameter_list|)
block|{
specifier|final
name|long
name|max
init|=
name|collectionStats
operator|.
name|maxDoc
argument_list|()
decl_stmt|;
name|float
name|idf
init|=
literal|0.0f
decl_stmt|;
specifier|final
name|Explanation
name|exp
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|exp
operator|.
name|setDescription
argument_list|(
literal|"idf(), sum of:"
argument_list|)
expr_stmt|;
for|for
control|(
specifier|final
name|TermStatistics
name|stat
range|:
name|termStats
control|)
block|{
specifier|final
name|long
name|df
init|=
name|stat
operator|.
name|docFreq
argument_list|()
decl_stmt|;
specifier|final
name|float
name|termIdf
init|=
name|idf
argument_list|(
name|df
argument_list|,
name|max
argument_list|)
decl_stmt|;
name|exp
operator|.
name|addDetail
argument_list|(
operator|new
name|Explanation
argument_list|(
name|termIdf
argument_list|,
literal|"idf(docFreq="
operator|+
name|df
operator|+
literal|", maxDocs="
operator|+
name|max
operator|+
literal|")"
argument_list|)
argument_list|)
expr_stmt|;
name|idf
operator|+=
name|termIdf
expr_stmt|;
block|}
name|exp
operator|.
name|setValue
argument_list|(
name|idf
argument_list|)
expr_stmt|;
return|return
name|exp
return|;
block|}
comment|/** Computes a score factor based on a term's document frequency (the number    * of documents which contain the term).  This value is multiplied by the    * {@link #tf(int)} factor for each term in the query and these products are    * then summed to form the initial score for a document.    *    *<p>Terms that occur in fewer documents are better indicators of topic, so    * implementations of this method usually return larger values for rare terms,    * and smaller values for common terms.    *    * @param docFreq the number of documents which contain the term    * @param numDocs the total number of documents in the collection    * @return a score factor based on the term's document frequency    */
DECL|method|idf
specifier|public
specifier|abstract
name|float
name|idf
parameter_list|(
name|long
name|docFreq
parameter_list|,
name|long
name|numDocs
parameter_list|)
function_decl|;
comment|/** Cache of decoded bytes. */
DECL|field|NORM_TABLE
specifier|private
specifier|static
specifier|final
name|float
index|[]
name|NORM_TABLE
init|=
operator|new
name|float
index|[
literal|256
index|]
decl_stmt|;
static|static
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
literal|256
condition|;
name|i
operator|++
control|)
name|NORM_TABLE
index|[
name|i
index|]
operator|=
name|SmallFloat
operator|.
name|byte315ToFloat
argument_list|(
operator|(
name|byte
operator|)
name|i
argument_list|)
expr_stmt|;
block|}
comment|/** Decodes a normalization factor stored in an index.    * @see #encodeNormValue(float)    */
DECL|method|decodeNormValue
specifier|public
name|float
name|decodeNormValue
parameter_list|(
name|byte
name|b
parameter_list|)
block|{
return|return
name|NORM_TABLE
index|[
name|b
operator|&
literal|0xFF
index|]
return|;
comment|//& 0xFF maps negative bytes to positive above 127
block|}
comment|/** Encodes a normalization factor for storage in an index.   *   *<p>The encoding uses a three-bit mantissa, a five-bit exponent, and   * the zero-exponent point at 15, thus   * representing values from around 7x10^9 to 2x10^-9 with about one   * significant decimal digit of accuracy.  Zero is also represented.   * Negative numbers are rounded up to zero.  Values too large to represent   * are rounded down to the largest representable value.  Positive values too   * small to represent are rounded up to the smallest positive representable   * value.   * @see org.apache.lucene.document.Field#setBoost(float)   * @see org.apache.lucene.util.SmallFloat   */
DECL|method|encodeNormValue
specifier|public
name|byte
name|encodeNormValue
parameter_list|(
name|float
name|f
parameter_list|)
block|{
return|return
name|SmallFloat
operator|.
name|floatToByte315
argument_list|(
name|f
argument_list|)
return|;
block|}
comment|/** Computes the amount of a sloppy phrase match, based on an edit distance.    * This value is summed for each sloppy phrase match in a document to form    * the frequency to be used in scoring instead of the exact term count.    *    *<p>A phrase match with a small edit distance to a document passage more    * closely matches the document, so implementations of this method usually    * return larger values when the edit distance is small and smaller values    * when it is large.    *    * @see PhraseQuery#setSlop(int)    * @param distance the edit distance of this sloppy phrase match    * @return the frequency increment for this match    */
DECL|method|sloppyFreq
specifier|public
specifier|abstract
name|float
name|sloppyFreq
parameter_list|(
name|int
name|distance
parameter_list|)
function_decl|;
comment|/**    * Calculate a scoring factor based on the data in the payload.  Implementations    * are responsible for interpreting what is in the payload.  Lucene makes no assumptions about    * what is in the byte array.    *    * @param doc The docId currently being scored.    * @param start The start position of the payload    * @param end The end position of the payload    * @param payload The payload byte array to be scored    * @return An implementation dependent float to be used as a scoring factor    */
DECL|method|scorePayload
specifier|public
specifier|abstract
name|float
name|scorePayload
parameter_list|(
name|int
name|doc
parameter_list|,
name|int
name|start
parameter_list|,
name|int
name|end
parameter_list|,
name|BytesRef
name|payload
parameter_list|)
function_decl|;
annotation|@
name|Override
DECL|method|computeStats
specifier|public
specifier|final
name|Stats
name|computeStats
parameter_list|(
name|CollectionStatistics
name|collectionStats
parameter_list|,
name|float
name|queryBoost
parameter_list|,
name|TermStatistics
modifier|...
name|termStats
parameter_list|)
block|{
specifier|final
name|Explanation
name|idf
init|=
name|termStats
operator|.
name|length
operator|==
literal|1
condition|?
name|idfExplain
argument_list|(
name|collectionStats
argument_list|,
name|termStats
index|[
literal|0
index|]
argument_list|)
else|:
name|idfExplain
argument_list|(
name|collectionStats
argument_list|,
name|termStats
argument_list|)
decl_stmt|;
return|return
operator|new
name|IDFStats
argument_list|(
name|idf
argument_list|,
name|queryBoost
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|exactDocScorer
specifier|public
specifier|final
name|ExactDocScorer
name|exactDocScorer
parameter_list|(
name|Stats
name|stats
parameter_list|,
name|String
name|fieldName
parameter_list|,
name|AtomicReaderContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|ExactTFIDFDocScorer
argument_list|(
operator|(
name|IDFStats
operator|)
name|stats
argument_list|,
name|context
operator|.
name|reader
argument_list|()
operator|.
name|normValues
argument_list|(
name|fieldName
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|sloppyDocScorer
specifier|public
specifier|final
name|SloppyDocScorer
name|sloppyDocScorer
parameter_list|(
name|Stats
name|stats
parameter_list|,
name|String
name|fieldName
parameter_list|,
name|AtomicReaderContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|SloppyTFIDFDocScorer
argument_list|(
operator|(
name|IDFStats
operator|)
name|stats
argument_list|,
name|context
operator|.
name|reader
argument_list|()
operator|.
name|normValues
argument_list|(
name|fieldName
argument_list|)
argument_list|)
return|;
block|}
comment|// TODO: we can specialize these for omitNorms up front, but we should test that it doesn't confuse stupid hotspot.
DECL|class|ExactTFIDFDocScorer
specifier|private
specifier|final
class|class
name|ExactTFIDFDocScorer
extends|extends
name|ExactDocScorer
block|{
DECL|field|stats
specifier|private
specifier|final
name|IDFStats
name|stats
decl_stmt|;
DECL|field|weightValue
specifier|private
specifier|final
name|float
name|weightValue
decl_stmt|;
DECL|field|norms
specifier|private
specifier|final
name|byte
index|[]
name|norms
decl_stmt|;
DECL|field|SCORE_CACHE_SIZE
specifier|private
specifier|static
specifier|final
name|int
name|SCORE_CACHE_SIZE
init|=
literal|32
decl_stmt|;
DECL|field|scoreCache
specifier|private
name|float
index|[]
name|scoreCache
init|=
operator|new
name|float
index|[
name|SCORE_CACHE_SIZE
index|]
decl_stmt|;
DECL|method|ExactTFIDFDocScorer
name|ExactTFIDFDocScorer
parameter_list|(
name|IDFStats
name|stats
parameter_list|,
name|DocValues
name|norms
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|stats
operator|=
name|stats
expr_stmt|;
name|this
operator|.
name|weightValue
operator|=
name|stats
operator|.
name|value
expr_stmt|;
name|this
operator|.
name|norms
operator|=
name|norms
operator|==
literal|null
condition|?
literal|null
else|:
operator|(
name|byte
index|[]
operator|)
name|norms
operator|.
name|getSource
argument_list|()
operator|.
name|getArray
argument_list|()
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|SCORE_CACHE_SIZE
condition|;
name|i
operator|++
control|)
name|scoreCache
index|[
name|i
index|]
operator|=
name|tf
argument_list|(
name|i
argument_list|)
operator|*
name|weightValue
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|score
specifier|public
name|float
name|score
parameter_list|(
name|int
name|doc
parameter_list|,
name|int
name|freq
parameter_list|)
block|{
specifier|final
name|float
name|raw
init|=
comment|// compute tf(f)*weight
name|freq
operator|<
name|SCORE_CACHE_SIZE
comment|// check cache
condition|?
name|scoreCache
index|[
name|freq
index|]
comment|// cache hit
else|:
name|tf
argument_list|(
name|freq
argument_list|)
operator|*
name|weightValue
decl_stmt|;
comment|// cache miss
return|return
name|norms
operator|==
literal|null
condition|?
name|raw
else|:
name|raw
operator|*
name|decodeNormValue
argument_list|(
name|norms
index|[
name|doc
index|]
argument_list|)
return|;
comment|// normalize for field
block|}
annotation|@
name|Override
DECL|method|explain
specifier|public
name|Explanation
name|explain
parameter_list|(
name|int
name|doc
parameter_list|,
name|Explanation
name|freq
parameter_list|)
block|{
return|return
name|explainScore
argument_list|(
name|doc
argument_list|,
name|freq
argument_list|,
name|stats
argument_list|,
name|norms
argument_list|)
return|;
block|}
block|}
DECL|class|SloppyTFIDFDocScorer
specifier|private
specifier|final
class|class
name|SloppyTFIDFDocScorer
extends|extends
name|SloppyDocScorer
block|{
DECL|field|stats
specifier|private
specifier|final
name|IDFStats
name|stats
decl_stmt|;
DECL|field|weightValue
specifier|private
specifier|final
name|float
name|weightValue
decl_stmt|;
DECL|field|norms
specifier|private
specifier|final
name|byte
index|[]
name|norms
decl_stmt|;
DECL|method|SloppyTFIDFDocScorer
name|SloppyTFIDFDocScorer
parameter_list|(
name|IDFStats
name|stats
parameter_list|,
name|DocValues
name|norms
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|stats
operator|=
name|stats
expr_stmt|;
name|this
operator|.
name|weightValue
operator|=
name|stats
operator|.
name|value
expr_stmt|;
name|this
operator|.
name|norms
operator|=
name|norms
operator|==
literal|null
condition|?
literal|null
else|:
operator|(
name|byte
index|[]
operator|)
name|norms
operator|.
name|getSource
argument_list|()
operator|.
name|getArray
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|score
specifier|public
name|float
name|score
parameter_list|(
name|int
name|doc
parameter_list|,
name|float
name|freq
parameter_list|)
block|{
specifier|final
name|float
name|raw
init|=
name|tf
argument_list|(
name|freq
argument_list|)
operator|*
name|weightValue
decl_stmt|;
comment|// compute tf(f)*weight
return|return
name|norms
operator|==
literal|null
condition|?
name|raw
else|:
name|raw
operator|*
name|decodeNormValue
argument_list|(
name|norms
index|[
name|doc
index|]
argument_list|)
return|;
comment|// normalize for field
block|}
annotation|@
name|Override
DECL|method|computeSlopFactor
specifier|public
name|float
name|computeSlopFactor
parameter_list|(
name|int
name|distance
parameter_list|)
block|{
return|return
name|sloppyFreq
argument_list|(
name|distance
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|computePayloadFactor
specifier|public
name|float
name|computePayloadFactor
parameter_list|(
name|int
name|doc
parameter_list|,
name|int
name|start
parameter_list|,
name|int
name|end
parameter_list|,
name|BytesRef
name|payload
parameter_list|)
block|{
return|return
name|scorePayload
argument_list|(
name|doc
argument_list|,
name|start
argument_list|,
name|end
argument_list|,
name|payload
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|explain
specifier|public
name|Explanation
name|explain
parameter_list|(
name|int
name|doc
parameter_list|,
name|Explanation
name|freq
parameter_list|)
block|{
return|return
name|explainScore
argument_list|(
name|doc
argument_list|,
name|freq
argument_list|,
name|stats
argument_list|,
name|norms
argument_list|)
return|;
block|}
block|}
comment|/** Collection statistics for the TF-IDF model. The only statistic of interest    * to this model is idf. */
DECL|class|IDFStats
specifier|private
specifier|static
class|class
name|IDFStats
extends|extends
name|Stats
block|{
comment|/** The idf and its explanation */
DECL|field|idf
specifier|private
specifier|final
name|Explanation
name|idf
decl_stmt|;
DECL|field|queryNorm
specifier|private
name|float
name|queryNorm
decl_stmt|;
DECL|field|queryWeight
specifier|private
name|float
name|queryWeight
decl_stmt|;
DECL|field|queryBoost
specifier|private
specifier|final
name|float
name|queryBoost
decl_stmt|;
DECL|field|value
specifier|private
name|float
name|value
decl_stmt|;
DECL|method|IDFStats
specifier|public
name|IDFStats
parameter_list|(
name|Explanation
name|idf
parameter_list|,
name|float
name|queryBoost
parameter_list|)
block|{
comment|// TODO: Validate?
name|this
operator|.
name|idf
operator|=
name|idf
expr_stmt|;
name|this
operator|.
name|queryBoost
operator|=
name|queryBoost
expr_stmt|;
name|this
operator|.
name|queryWeight
operator|=
name|idf
operator|.
name|getValue
argument_list|()
operator|*
name|queryBoost
expr_stmt|;
comment|// compute query weight
block|}
annotation|@
name|Override
DECL|method|getValueForNormalization
specifier|public
name|float
name|getValueForNormalization
parameter_list|()
block|{
comment|// TODO: (sorta LUCENE-1907) make non-static class and expose this squaring via a nice method to subclasses?
return|return
name|queryWeight
operator|*
name|queryWeight
return|;
comment|// sum of squared weights
block|}
annotation|@
name|Override
DECL|method|normalize
specifier|public
name|void
name|normalize
parameter_list|(
name|float
name|queryNorm
parameter_list|,
name|float
name|topLevelBoost
parameter_list|)
block|{
name|this
operator|.
name|queryNorm
operator|=
name|queryNorm
operator|*
name|topLevelBoost
expr_stmt|;
name|queryWeight
operator|*=
name|this
operator|.
name|queryNorm
expr_stmt|;
comment|// normalize query weight
name|value
operator|=
name|queryWeight
operator|*
name|idf
operator|.
name|getValue
argument_list|()
expr_stmt|;
comment|// idf for document
block|}
block|}
DECL|method|explainScore
specifier|private
name|Explanation
name|explainScore
parameter_list|(
name|int
name|doc
parameter_list|,
name|Explanation
name|freq
parameter_list|,
name|IDFStats
name|stats
parameter_list|,
name|byte
index|[]
name|norms
parameter_list|)
block|{
name|Explanation
name|result
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|result
operator|.
name|setDescription
argument_list|(
literal|"score(doc="
operator|+
name|doc
operator|+
literal|",freq="
operator|+
name|freq
operator|+
literal|"), product of:"
argument_list|)
expr_stmt|;
comment|// explain query weight
name|Explanation
name|queryExpl
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|queryExpl
operator|.
name|setDescription
argument_list|(
literal|"queryWeight, product of:"
argument_list|)
expr_stmt|;
name|Explanation
name|boostExpl
init|=
operator|new
name|Explanation
argument_list|(
name|stats
operator|.
name|queryBoost
argument_list|,
literal|"boost"
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|.
name|queryBoost
operator|!=
literal|1.0f
condition|)
name|queryExpl
operator|.
name|addDetail
argument_list|(
name|boostExpl
argument_list|)
expr_stmt|;
name|queryExpl
operator|.
name|addDetail
argument_list|(
name|stats
operator|.
name|idf
argument_list|)
expr_stmt|;
name|Explanation
name|queryNormExpl
init|=
operator|new
name|Explanation
argument_list|(
name|stats
operator|.
name|queryNorm
argument_list|,
literal|"queryNorm"
argument_list|)
decl_stmt|;
name|queryExpl
operator|.
name|addDetail
argument_list|(
name|queryNormExpl
argument_list|)
expr_stmt|;
name|queryExpl
operator|.
name|setValue
argument_list|(
name|boostExpl
operator|.
name|getValue
argument_list|()
operator|*
name|stats
operator|.
name|idf
operator|.
name|getValue
argument_list|()
operator|*
name|queryNormExpl
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
name|result
operator|.
name|addDetail
argument_list|(
name|queryExpl
argument_list|)
expr_stmt|;
comment|// explain field weight
name|Explanation
name|fieldExpl
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|fieldExpl
operator|.
name|setDescription
argument_list|(
literal|"fieldWeight in "
operator|+
name|doc
operator|+
literal|", product of:"
argument_list|)
expr_stmt|;
name|Explanation
name|tfExplanation
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|tfExplanation
operator|.
name|setValue
argument_list|(
name|tf
argument_list|(
name|freq
operator|.
name|getValue
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|tfExplanation
operator|.
name|setDescription
argument_list|(
literal|"tf(freq="
operator|+
name|freq
operator|.
name|getValue
argument_list|()
operator|+
literal|"), with freq of:"
argument_list|)
expr_stmt|;
name|tfExplanation
operator|.
name|addDetail
argument_list|(
name|freq
argument_list|)
expr_stmt|;
name|fieldExpl
operator|.
name|addDetail
argument_list|(
name|tfExplanation
argument_list|)
expr_stmt|;
name|fieldExpl
operator|.
name|addDetail
argument_list|(
name|stats
operator|.
name|idf
argument_list|)
expr_stmt|;
name|Explanation
name|fieldNormExpl
init|=
operator|new
name|Explanation
argument_list|()
decl_stmt|;
name|float
name|fieldNorm
init|=
name|norms
operator|!=
literal|null
condition|?
name|decodeNormValue
argument_list|(
name|norms
index|[
name|doc
index|]
argument_list|)
else|:
literal|1.0f
decl_stmt|;
name|fieldNormExpl
operator|.
name|setValue
argument_list|(
name|fieldNorm
argument_list|)
expr_stmt|;
name|fieldNormExpl
operator|.
name|setDescription
argument_list|(
literal|"fieldNorm(doc="
operator|+
name|doc
operator|+
literal|")"
argument_list|)
expr_stmt|;
name|fieldExpl
operator|.
name|addDetail
argument_list|(
name|fieldNormExpl
argument_list|)
expr_stmt|;
name|fieldExpl
operator|.
name|setValue
argument_list|(
name|tfExplanation
operator|.
name|getValue
argument_list|()
operator|*
name|stats
operator|.
name|idf
operator|.
name|getValue
argument_list|()
operator|*
name|fieldNormExpl
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
name|result
operator|.
name|addDetail
argument_list|(
name|fieldExpl
argument_list|)
expr_stmt|;
comment|// combine them
name|result
operator|.
name|setValue
argument_list|(
name|queryExpl
operator|.
name|getValue
argument_list|()
operator|*
name|fieldExpl
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|queryExpl
operator|.
name|getValue
argument_list|()
operator|==
literal|1.0f
condition|)
return|return
name|fieldExpl
return|;
return|return
name|result
return|;
block|}
block|}
end_class
end_unit
