begin_unit
begin_package
DECL|package|org.apache.lucene.index
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
package|;
end_package
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|codecs
operator|.
name|DocValuesConsumer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|codecs
operator|.
name|DocValuesFormat
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|codecs
operator|.
name|NormsConsumer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|codecs
operator|.
name|NormsFormat
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|codecs
operator|.
name|StoredFieldsWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|FieldType
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|FieldInfo
operator|.
name|DocValuesType
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|FieldInfo
operator|.
name|IndexOptions
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|similarities
operator|.
name|Similarity
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|IOContext
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|ArrayUtil
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRefHash
operator|.
name|MaxBytesLengthExceededException
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|Counter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|IOUtils
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|RamUsageEstimator
import|;
end_import
begin_comment
comment|/** Default general purpose indexing chain, which handles  *  indexing all types of fields. */
end_comment
begin_class
DECL|class|DefaultIndexingChain
specifier|final
class|class
name|DefaultIndexingChain
extends|extends
name|DocConsumer
block|{
DECL|field|bytesUsed
specifier|final
name|Counter
name|bytesUsed
decl_stmt|;
DECL|field|docState
specifier|final
name|DocumentsWriterPerThread
operator|.
name|DocState
name|docState
decl_stmt|;
DECL|field|docWriter
specifier|final
name|DocumentsWriterPerThread
name|docWriter
decl_stmt|;
DECL|field|fieldInfos
specifier|final
name|FieldInfos
operator|.
name|Builder
name|fieldInfos
decl_stmt|;
comment|// Writes postings and term vectors:
DECL|field|termsHash
specifier|final
name|TermsHash
name|termsHash
decl_stmt|;
comment|// lazy init:
DECL|field|storedFieldsWriter
specifier|private
name|StoredFieldsWriter
name|storedFieldsWriter
decl_stmt|;
DECL|field|lastStoredDocID
specifier|private
name|int
name|lastStoredDocID
decl_stmt|;
comment|// NOTE: I tried using Hash Map<String,PerField>
comment|// but it was ~2% slower on Wiki and Geonames with Java
comment|// 1.7.0_25:
DECL|field|fieldHash
specifier|private
name|PerField
index|[]
name|fieldHash
init|=
operator|new
name|PerField
index|[
literal|2
index|]
decl_stmt|;
DECL|field|hashMask
specifier|private
name|int
name|hashMask
init|=
literal|1
decl_stmt|;
DECL|field|totalFieldCount
specifier|private
name|int
name|totalFieldCount
decl_stmt|;
DECL|field|nextFieldGen
specifier|private
name|long
name|nextFieldGen
decl_stmt|;
comment|// Holds fields seen in each document
DECL|field|fields
specifier|private
name|PerField
index|[]
name|fields
init|=
operator|new
name|PerField
index|[
literal|1
index|]
decl_stmt|;
DECL|method|DefaultIndexingChain
specifier|public
name|DefaultIndexingChain
parameter_list|(
name|DocumentsWriterPerThread
name|docWriter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|docWriter
operator|=
name|docWriter
expr_stmt|;
name|this
operator|.
name|fieldInfos
operator|=
name|docWriter
operator|.
name|getFieldInfosBuilder
argument_list|()
expr_stmt|;
name|this
operator|.
name|docState
operator|=
name|docWriter
operator|.
name|docState
expr_stmt|;
name|this
operator|.
name|bytesUsed
operator|=
name|docWriter
operator|.
name|bytesUsed
expr_stmt|;
name|TermsHash
name|termVectorsWriter
init|=
operator|new
name|TermVectorsConsumer
argument_list|(
name|docWriter
argument_list|)
decl_stmt|;
name|termsHash
operator|=
operator|new
name|FreqProxTermsWriter
argument_list|(
name|docWriter
argument_list|,
name|termVectorsWriter
argument_list|)
expr_stmt|;
block|}
comment|// TODO: can we remove this lazy-init / make cleaner / do it another way...?
DECL|method|initStoredFieldsWriter
specifier|private
name|void
name|initStoredFieldsWriter
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|storedFieldsWriter
operator|==
literal|null
condition|)
block|{
name|storedFieldsWriter
operator|=
name|docWriter
operator|.
name|codec
operator|.
name|storedFieldsFormat
argument_list|()
operator|.
name|fieldsWriter
argument_list|(
name|docWriter
operator|.
name|directory
argument_list|,
name|docWriter
operator|.
name|getSegmentInfo
argument_list|()
argument_list|,
name|IOContext
operator|.
name|DEFAULT
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
DECL|method|flush
specifier|public
name|void
name|flush
parameter_list|(
name|SegmentWriteState
name|state
parameter_list|)
throws|throws
name|IOException
block|{
comment|// NOTE: caller (DocumentsWriterPerThread) handles
comment|// aborting on any exception from this method
name|int
name|numDocs
init|=
name|state
operator|.
name|segmentInfo
operator|.
name|getDocCount
argument_list|()
decl_stmt|;
name|writeNorms
argument_list|(
name|state
argument_list|)
expr_stmt|;
name|writeDocValues
argument_list|(
name|state
argument_list|)
expr_stmt|;
comment|// its possible all docs hit non-aborting exceptions...
name|initStoredFieldsWriter
argument_list|()
expr_stmt|;
name|fillStoredFields
argument_list|(
name|numDocs
argument_list|)
expr_stmt|;
name|storedFieldsWriter
operator|.
name|finish
argument_list|(
name|state
operator|.
name|fieldInfos
argument_list|,
name|numDocs
argument_list|)
expr_stmt|;
name|storedFieldsWriter
operator|.
name|close
argument_list|()
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|TermsHashPerField
argument_list|>
name|fieldsToFlush
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fieldHash
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|PerField
name|perField
init|=
name|fieldHash
index|[
name|i
index|]
decl_stmt|;
while|while
condition|(
name|perField
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|perField
operator|.
name|invertState
operator|!=
literal|null
condition|)
block|{
name|fieldsToFlush
operator|.
name|put
argument_list|(
name|perField
operator|.
name|fieldInfo
operator|.
name|name
argument_list|,
name|perField
operator|.
name|termsHashPerField
argument_list|)
expr_stmt|;
block|}
name|perField
operator|=
name|perField
operator|.
name|next
expr_stmt|;
block|}
block|}
name|termsHash
operator|.
name|flush
argument_list|(
name|fieldsToFlush
argument_list|,
name|state
argument_list|)
expr_stmt|;
comment|// Important to save after asking consumer to flush so
comment|// consumer can alter the FieldInfo* if necessary.  EG,
comment|// FreqProxTermsWriter does this with
comment|// FieldInfo.storePayload.
name|docWriter
operator|.
name|codec
operator|.
name|fieldInfosFormat
argument_list|()
operator|.
name|write
argument_list|(
name|state
operator|.
name|directory
argument_list|,
name|state
operator|.
name|segmentInfo
argument_list|,
literal|""
argument_list|,
name|state
operator|.
name|fieldInfos
argument_list|,
name|IOContext
operator|.
name|DEFAULT
argument_list|)
expr_stmt|;
block|}
comment|/** Writes all buffered doc values (called from {@link #flush}). */
DECL|method|writeDocValues
specifier|private
name|void
name|writeDocValues
parameter_list|(
name|SegmentWriteState
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|docCount
init|=
name|state
operator|.
name|segmentInfo
operator|.
name|getDocCount
argument_list|()
decl_stmt|;
name|DocValuesConsumer
name|dvConsumer
init|=
literal|null
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fieldHash
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|PerField
name|perField
init|=
name|fieldHash
index|[
name|i
index|]
decl_stmt|;
while|while
condition|(
name|perField
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|perField
operator|.
name|docValuesWriter
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|dvConsumer
operator|==
literal|null
condition|)
block|{
comment|// lazy init
name|DocValuesFormat
name|fmt
init|=
name|state
operator|.
name|segmentInfo
operator|.
name|getCodec
argument_list|()
operator|.
name|docValuesFormat
argument_list|()
decl_stmt|;
name|dvConsumer
operator|=
name|fmt
operator|.
name|fieldsConsumer
argument_list|(
name|state
argument_list|)
expr_stmt|;
block|}
name|perField
operator|.
name|docValuesWriter
operator|.
name|finish
argument_list|(
name|docCount
argument_list|)
expr_stmt|;
name|perField
operator|.
name|docValuesWriter
operator|.
name|flush
argument_list|(
name|state
argument_list|,
name|dvConsumer
argument_list|)
expr_stmt|;
name|perField
operator|.
name|docValuesWriter
operator|=
literal|null
expr_stmt|;
block|}
name|perField
operator|=
name|perField
operator|.
name|next
expr_stmt|;
block|}
block|}
comment|// TODO: catch missing DV fields here?  else we have
comment|// null/"" depending on how docs landed in segments?
comment|// but we can't detect all cases, and we should leave
comment|// this behavior undefined. dv is not "schemaless": its column-stride.
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|dvConsumer
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|closeWhileHandlingException
argument_list|(
name|dvConsumer
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/** Catch up for all docs before us that had no stored    *  fields, or hit non-aborting exceptions before writing    *  stored fields. */
DECL|method|fillStoredFields
specifier|private
name|void
name|fillStoredFields
parameter_list|(
name|int
name|docID
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
name|lastStoredDocID
operator|<
name|docID
condition|)
block|{
name|startStoredFields
argument_list|()
expr_stmt|;
name|finishStoredFields
argument_list|()
expr_stmt|;
block|}
block|}
DECL|method|writeNorms
specifier|private
name|void
name|writeNorms
parameter_list|(
name|SegmentWriteState
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|success
init|=
literal|false
decl_stmt|;
name|NormsConsumer
name|normsConsumer
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|state
operator|.
name|fieldInfos
operator|.
name|hasNorms
argument_list|()
condition|)
block|{
name|NormsFormat
name|normsFormat
init|=
name|state
operator|.
name|segmentInfo
operator|.
name|getCodec
argument_list|()
operator|.
name|normsFormat
argument_list|()
decl_stmt|;
assert|assert
name|normsFormat
operator|!=
literal|null
assert|;
name|normsConsumer
operator|=
name|normsFormat
operator|.
name|normsConsumer
argument_list|(
name|state
argument_list|)
expr_stmt|;
for|for
control|(
name|FieldInfo
name|fi
range|:
name|state
operator|.
name|fieldInfos
control|)
block|{
name|PerField
name|perField
init|=
name|getPerField
argument_list|(
name|fi
operator|.
name|name
argument_list|)
decl_stmt|;
assert|assert
name|perField
operator|!=
literal|null
assert|;
comment|// we must check the final value of omitNorms for the fieldinfo: it could have
comment|// changed for this field since the first time we added it.
if|if
condition|(
name|fi
operator|.
name|omitsNorms
argument_list|()
operator|==
literal|false
operator|&&
name|fi
operator|.
name|isIndexed
argument_list|()
condition|)
block|{
assert|assert
name|perField
operator|.
name|norms
operator|!=
literal|null
operator|:
literal|"field="
operator|+
name|fi
operator|.
name|name
assert|;
name|perField
operator|.
name|norms
operator|.
name|finish
argument_list|(
name|state
operator|.
name|segmentInfo
operator|.
name|getDocCount
argument_list|()
argument_list|)
expr_stmt|;
name|perField
operator|.
name|norms
operator|.
name|flush
argument_list|(
name|state
argument_list|,
name|normsConsumer
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
condition|)
block|{
name|IOUtils
operator|.
name|close
argument_list|(
name|normsConsumer
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|IOUtils
operator|.
name|closeWhileHandlingException
argument_list|(
name|normsConsumer
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
DECL|method|abort
specifier|public
name|void
name|abort
parameter_list|()
block|{
try|try
block|{
comment|// E.g. close any open files in the stored fields writer:
name|storedFieldsWriter
operator|.
name|abort
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{     }
try|try
block|{
comment|// E.g. close any open files in the term vectors writer:
name|termsHash
operator|.
name|abort
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{     }
name|Arrays
operator|.
name|fill
argument_list|(
name|fieldHash
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
DECL|method|rehash
specifier|private
name|void
name|rehash
parameter_list|()
block|{
name|int
name|newHashSize
init|=
operator|(
name|fieldHash
operator|.
name|length
operator|*
literal|2
operator|)
decl_stmt|;
assert|assert
name|newHashSize
operator|>
name|fieldHash
operator|.
name|length
assert|;
name|PerField
name|newHashArray
index|[]
init|=
operator|new
name|PerField
index|[
name|newHashSize
index|]
decl_stmt|;
comment|// Rehash
name|int
name|newHashMask
init|=
name|newHashSize
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|fieldHash
operator|.
name|length
condition|;
name|j
operator|++
control|)
block|{
name|PerField
name|fp0
init|=
name|fieldHash
index|[
name|j
index|]
decl_stmt|;
while|while
condition|(
name|fp0
operator|!=
literal|null
condition|)
block|{
specifier|final
name|int
name|hashPos2
init|=
name|fp0
operator|.
name|fieldInfo
operator|.
name|name
operator|.
name|hashCode
argument_list|()
operator|&
name|newHashMask
decl_stmt|;
name|PerField
name|nextFP0
init|=
name|fp0
operator|.
name|next
decl_stmt|;
name|fp0
operator|.
name|next
operator|=
name|newHashArray
index|[
name|hashPos2
index|]
expr_stmt|;
name|newHashArray
index|[
name|hashPos2
index|]
operator|=
name|fp0
expr_stmt|;
name|fp0
operator|=
name|nextFP0
expr_stmt|;
block|}
block|}
name|fieldHash
operator|=
name|newHashArray
expr_stmt|;
name|hashMask
operator|=
name|newHashMask
expr_stmt|;
block|}
comment|/** Calls StoredFieldsWriter.startDocument, aborting the    *  segment if it hits any exception. */
DECL|method|startStoredFields
specifier|private
name|void
name|startStoredFields
parameter_list|()
throws|throws
name|IOException
block|{
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|initStoredFieldsWriter
argument_list|()
expr_stmt|;
name|storedFieldsWriter
operator|.
name|startDocument
argument_list|()
expr_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
operator|==
literal|false
condition|)
block|{
name|docWriter
operator|.
name|setAborting
argument_list|()
expr_stmt|;
block|}
block|}
name|lastStoredDocID
operator|++
expr_stmt|;
block|}
comment|/** Calls StoredFieldsWriter.finishDocument, aborting the    *  segment if it hits any exception. */
DECL|method|finishStoredFields
specifier|private
name|void
name|finishStoredFields
parameter_list|()
throws|throws
name|IOException
block|{
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|storedFieldsWriter
operator|.
name|finishDocument
argument_list|()
expr_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
operator|==
literal|false
condition|)
block|{
name|docWriter
operator|.
name|setAborting
argument_list|()
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
DECL|method|processDocument
specifier|public
name|void
name|processDocument
parameter_list|()
throws|throws
name|IOException
block|{
comment|// How many indexed field names we've seen (collapses
comment|// multiple field instances by the same name):
name|int
name|fieldCount
init|=
literal|0
decl_stmt|;
name|long
name|fieldGen
init|=
name|nextFieldGen
operator|++
decl_stmt|;
comment|// NOTE: we need two passes here, in case there are
comment|// multi-valued fields, because we must process all
comment|// instances of a given field at once, since the
comment|// analyzer is free to reuse TokenStream across fields
comment|// (i.e., we cannot have more than one TokenStream
comment|// running "at once"):
name|termsHash
operator|.
name|startDocument
argument_list|()
expr_stmt|;
comment|// Invert indexed fields:
try|try
block|{
for|for
control|(
name|IndexableField
name|field
range|:
name|docState
operator|.
name|doc
operator|.
name|indexableFields
argument_list|()
control|)
block|{
name|IndexableFieldType
name|fieldType
init|=
name|field
operator|.
name|fieldType
argument_list|()
decl_stmt|;
comment|// if the field omits norms, the boost cannot be indexed.
if|if
condition|(
name|fieldType
operator|.
name|omitNorms
argument_list|()
operator|&&
name|field
operator|.
name|boost
argument_list|()
operator|!=
literal|1.0f
condition|)
block|{
throw|throw
operator|new
name|UnsupportedOperationException
argument_list|(
literal|"You cannot set an index-time boost: norms are omitted for field '"
operator|+
name|field
operator|.
name|name
argument_list|()
operator|+
literal|"'"
argument_list|)
throw|;
block|}
name|PerField
name|fp
init|=
name|getOrAddField
argument_list|(
name|field
operator|.
name|name
argument_list|()
argument_list|,
name|fieldType
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|boolean
name|first
init|=
name|fp
operator|.
name|fieldGen
operator|!=
name|fieldGen
decl_stmt|;
name|fp
operator|.
name|invert
argument_list|(
name|field
argument_list|,
name|first
argument_list|)
expr_stmt|;
if|if
condition|(
name|first
condition|)
block|{
name|fields
index|[
name|fieldCount
operator|++
index|]
operator|=
name|fp
expr_stmt|;
name|fp
operator|.
name|fieldGen
operator|=
name|fieldGen
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
comment|// Finish each field name seen in the document:
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fieldCount
condition|;
name|i
operator|++
control|)
block|{
name|fields
index|[
name|i
index|]
operator|.
name|finish
argument_list|()
expr_stmt|;
block|}
block|}
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|termsHash
operator|.
name|finishDocument
argument_list|()
expr_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|success
operator|==
literal|false
condition|)
block|{
comment|// Must abort, on the possibility that on-disk term
comment|// vectors are now corrupt:
name|docWriter
operator|.
name|setAborting
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Add stored fields:
name|fillStoredFields
argument_list|(
name|docState
operator|.
name|docID
argument_list|)
expr_stmt|;
name|startStoredFields
argument_list|()
expr_stmt|;
comment|// TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...
name|boolean
name|abort
init|=
literal|false
decl_stmt|;
try|try
block|{
for|for
control|(
name|StorableField
name|field
range|:
name|docState
operator|.
name|doc
operator|.
name|storableFields
argument_list|()
control|)
block|{
name|String
name|fieldName
init|=
name|field
operator|.
name|name
argument_list|()
decl_stmt|;
name|IndexableFieldType
name|fieldType
init|=
name|field
operator|.
name|fieldType
argument_list|()
decl_stmt|;
name|verifyFieldType
argument_list|(
name|fieldName
argument_list|,
name|fieldType
argument_list|)
expr_stmt|;
name|PerField
name|fp
init|=
name|getOrAddField
argument_list|(
name|fieldName
argument_list|,
name|fieldType
argument_list|,
literal|false
argument_list|)
decl_stmt|;
if|if
condition|(
name|fieldType
operator|.
name|stored
argument_list|()
condition|)
block|{
name|abort
operator|=
literal|true
expr_stmt|;
name|storedFieldsWriter
operator|.
name|writeField
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|field
argument_list|)
expr_stmt|;
name|abort
operator|=
literal|false
expr_stmt|;
block|}
name|DocValuesType
name|dvType
init|=
name|fieldType
operator|.
name|docValueType
argument_list|()
decl_stmt|;
if|if
condition|(
name|dvType
operator|!=
literal|null
condition|)
block|{
name|indexDocValue
argument_list|(
name|fp
argument_list|,
name|dvType
argument_list|,
name|field
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|abort
condition|)
block|{
name|docWriter
operator|.
name|setAborting
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|finishStoredFields
argument_list|()
expr_stmt|;
block|}
block|}
block|}
DECL|method|verifyFieldType
specifier|private
specifier|static
name|void
name|verifyFieldType
parameter_list|(
name|String
name|name
parameter_list|,
name|IndexableFieldType
name|ft
parameter_list|)
block|{
if|if
condition|(
name|ft
operator|.
name|indexed
argument_list|()
operator|==
literal|false
condition|)
block|{
if|if
condition|(
name|ft
operator|.
name|storeTermVectors
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"cannot store term vectors "
operator|+
literal|"for a field that is not indexed (field=\""
operator|+
name|name
operator|+
literal|"\")"
argument_list|)
throw|;
block|}
if|if
condition|(
name|ft
operator|.
name|storeTermVectorPositions
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"cannot store term vector positions "
operator|+
literal|"for a field that is not indexed (field=\""
operator|+
name|name
operator|+
literal|"\")"
argument_list|)
throw|;
block|}
if|if
condition|(
name|ft
operator|.
name|storeTermVectorOffsets
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"cannot store term vector offsets "
operator|+
literal|"for a field that is not indexed (field=\""
operator|+
name|name
operator|+
literal|"\")"
argument_list|)
throw|;
block|}
if|if
condition|(
name|ft
operator|.
name|storeTermVectorPayloads
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"cannot store term vector payloads "
operator|+
literal|"for a field that is not indexed (field=\""
operator|+
name|name
operator|+
literal|"\")"
argument_list|)
throw|;
block|}
block|}
block|}
comment|/** Called from processDocument to index one field's doc    *  value */
DECL|method|indexDocValue
specifier|private
name|void
name|indexDocValue
parameter_list|(
name|PerField
name|fp
parameter_list|,
name|DocValuesType
name|dvType
parameter_list|,
name|StorableField
name|field
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|hasDocValues
init|=
name|fp
operator|.
name|fieldInfo
operator|.
name|hasDocValues
argument_list|()
decl_stmt|;
comment|// This will throw an exc if the caller tried to
comment|// change the DV type for the field:
name|fp
operator|.
name|fieldInfo
operator|.
name|setDocValuesType
argument_list|(
name|dvType
argument_list|)
expr_stmt|;
if|if
condition|(
name|hasDocValues
operator|==
literal|false
condition|)
block|{
name|fieldInfos
operator|.
name|globalFieldNumbers
operator|.
name|setDocValuesType
argument_list|(
name|fp
operator|.
name|fieldInfo
operator|.
name|number
argument_list|,
name|fp
operator|.
name|fieldInfo
operator|.
name|name
argument_list|,
name|dvType
argument_list|)
expr_stmt|;
block|}
name|int
name|docID
init|=
name|docState
operator|.
name|docID
decl_stmt|;
switch|switch
condition|(
name|dvType
condition|)
block|{
case|case
name|NUMERIC
case|:
if|if
condition|(
name|fp
operator|.
name|docValuesWriter
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|docValuesWriter
operator|=
operator|new
name|NumericDocValuesWriter
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|NumericDocValuesWriter
operator|)
name|fp
operator|.
name|docValuesWriter
operator|)
operator|.
name|addValue
argument_list|(
name|docID
argument_list|,
name|field
operator|.
name|numericValue
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|BINARY
case|:
if|if
condition|(
name|fp
operator|.
name|docValuesWriter
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|docValuesWriter
operator|=
operator|new
name|BinaryDocValuesWriter
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|BinaryDocValuesWriter
operator|)
name|fp
operator|.
name|docValuesWriter
operator|)
operator|.
name|addValue
argument_list|(
name|docID
argument_list|,
name|field
operator|.
name|binaryValue
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|SORTED
case|:
if|if
condition|(
name|fp
operator|.
name|docValuesWriter
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|docValuesWriter
operator|=
operator|new
name|SortedDocValuesWriter
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|SortedDocValuesWriter
operator|)
name|fp
operator|.
name|docValuesWriter
operator|)
operator|.
name|addValue
argument_list|(
name|docID
argument_list|,
name|field
operator|.
name|binaryValue
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|SORTED_NUMERIC
case|:
if|if
condition|(
name|fp
operator|.
name|docValuesWriter
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|docValuesWriter
operator|=
operator|new
name|SortedNumericDocValuesWriter
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|SortedNumericDocValuesWriter
operator|)
name|fp
operator|.
name|docValuesWriter
operator|)
operator|.
name|addValue
argument_list|(
name|docID
argument_list|,
name|field
operator|.
name|numericValue
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|SORTED_SET
case|:
if|if
condition|(
name|fp
operator|.
name|docValuesWriter
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|docValuesWriter
operator|=
operator|new
name|SortedSetDocValuesWriter
argument_list|(
name|fp
operator|.
name|fieldInfo
argument_list|,
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|SortedSetDocValuesWriter
operator|)
name|fp
operator|.
name|docValuesWriter
operator|)
operator|.
name|addValue
argument_list|(
name|docID
argument_list|,
name|field
operator|.
name|binaryValue
argument_list|()
argument_list|)
expr_stmt|;
break|break;
default|default:
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"unrecognized DocValues.Type: "
operator|+
name|dvType
argument_list|)
throw|;
block|}
block|}
comment|/** Returns a previously created {@link PerField}, or null    *  if this field name wasn't seen yet. */
DECL|method|getPerField
specifier|private
name|PerField
name|getPerField
parameter_list|(
name|String
name|name
parameter_list|)
block|{
specifier|final
name|int
name|hashPos
init|=
name|name
operator|.
name|hashCode
argument_list|()
operator|&
name|hashMask
decl_stmt|;
name|PerField
name|fp
init|=
name|fieldHash
index|[
name|hashPos
index|]
decl_stmt|;
while|while
condition|(
name|fp
operator|!=
literal|null
operator|&&
operator|!
name|fp
operator|.
name|fieldInfo
operator|.
name|name
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
name|fp
operator|=
name|fp
operator|.
name|next
expr_stmt|;
block|}
return|return
name|fp
return|;
block|}
comment|/** Returns a previously created {@link PerField},    *  absorbing the type information from {@link FieldType},    *  and creates a new {@link PerField} if this field name    *  wasn't seen yet. */
DECL|method|getOrAddField
specifier|private
name|PerField
name|getOrAddField
parameter_list|(
name|String
name|name
parameter_list|,
name|IndexableFieldType
name|fieldType
parameter_list|,
name|boolean
name|invert
parameter_list|)
block|{
comment|// Make sure we have a PerField allocated
specifier|final
name|int
name|hashPos
init|=
name|name
operator|.
name|hashCode
argument_list|()
operator|&
name|hashMask
decl_stmt|;
name|PerField
name|fp
init|=
name|fieldHash
index|[
name|hashPos
index|]
decl_stmt|;
while|while
condition|(
name|fp
operator|!=
literal|null
operator|&&
operator|!
name|fp
operator|.
name|fieldInfo
operator|.
name|name
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
name|fp
operator|=
name|fp
operator|.
name|next
expr_stmt|;
block|}
if|if
condition|(
name|fp
operator|==
literal|null
condition|)
block|{
comment|// First time we are seeing this field in this segment
name|FieldInfo
name|fi
init|=
name|fieldInfos
operator|.
name|addOrUpdate
argument_list|(
name|name
argument_list|,
name|fieldType
argument_list|)
decl_stmt|;
name|fp
operator|=
operator|new
name|PerField
argument_list|(
name|fi
argument_list|,
name|invert
argument_list|)
expr_stmt|;
name|fp
operator|.
name|next
operator|=
name|fieldHash
index|[
name|hashPos
index|]
expr_stmt|;
name|fieldHash
index|[
name|hashPos
index|]
operator|=
name|fp
expr_stmt|;
name|totalFieldCount
operator|++
expr_stmt|;
comment|// At most 50% load factor:
if|if
condition|(
name|totalFieldCount
operator|>=
name|fieldHash
operator|.
name|length
operator|/
literal|2
condition|)
block|{
name|rehash
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|totalFieldCount
operator|>
name|fields
operator|.
name|length
condition|)
block|{
name|PerField
index|[]
name|newFields
init|=
operator|new
name|PerField
index|[
name|ArrayUtil
operator|.
name|oversize
argument_list|(
name|totalFieldCount
argument_list|,
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
argument_list|)
index|]
decl_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|fields
argument_list|,
literal|0
argument_list|,
name|newFields
argument_list|,
literal|0
argument_list|,
name|fields
operator|.
name|length
argument_list|)
expr_stmt|;
name|fields
operator|=
name|newFields
expr_stmt|;
block|}
block|}
else|else
block|{
name|fp
operator|.
name|fieldInfo
operator|.
name|update
argument_list|(
name|fieldType
argument_list|)
expr_stmt|;
if|if
condition|(
name|invert
operator|&&
name|fp
operator|.
name|invertState
operator|==
literal|null
condition|)
block|{
name|fp
operator|.
name|setInvertState
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|fp
return|;
block|}
comment|/** NOTE: not static: accesses at least docState, termsHash. */
DECL|class|PerField
specifier|private
specifier|final
class|class
name|PerField
implements|implements
name|Comparable
argument_list|<
name|PerField
argument_list|>
block|{
DECL|field|fieldInfo
specifier|final
name|FieldInfo
name|fieldInfo
decl_stmt|;
DECL|field|similarity
specifier|final
name|Similarity
name|similarity
decl_stmt|;
DECL|field|invertState
name|FieldInvertState
name|invertState
decl_stmt|;
DECL|field|termsHashPerField
name|TermsHashPerField
name|termsHashPerField
decl_stmt|;
comment|// Non-null if this field ever had doc values in this
comment|// segment:
DECL|field|docValuesWriter
name|DocValuesWriter
name|docValuesWriter
decl_stmt|;
comment|/** We use this to know when a PerField is seen for the      *  first time in the current document. */
DECL|field|fieldGen
name|long
name|fieldGen
init|=
operator|-
literal|1
decl_stmt|;
comment|// Used by the hash table
DECL|field|next
name|PerField
name|next
decl_stmt|;
comment|// Lazy init'd:
DECL|field|norms
name|NormValuesWriter
name|norms
decl_stmt|;
comment|// reused
DECL|field|tokenStream
name|TokenStream
name|tokenStream
decl_stmt|;
DECL|method|PerField
specifier|public
name|PerField
parameter_list|(
name|FieldInfo
name|fieldInfo
parameter_list|,
name|boolean
name|invert
parameter_list|)
block|{
name|this
operator|.
name|fieldInfo
operator|=
name|fieldInfo
expr_stmt|;
name|similarity
operator|=
name|docState
operator|.
name|similarity
expr_stmt|;
if|if
condition|(
name|invert
condition|)
block|{
name|setInvertState
argument_list|()
expr_stmt|;
block|}
block|}
DECL|method|setInvertState
name|void
name|setInvertState
parameter_list|()
block|{
name|invertState
operator|=
operator|new
name|FieldInvertState
argument_list|(
name|fieldInfo
operator|.
name|name
argument_list|)
expr_stmt|;
name|termsHashPerField
operator|=
name|termsHash
operator|.
name|addField
argument_list|(
name|invertState
argument_list|,
name|fieldInfo
argument_list|)
expr_stmt|;
if|if
condition|(
name|fieldInfo
operator|.
name|omitsNorms
argument_list|()
operator|==
literal|false
condition|)
block|{
assert|assert
name|norms
operator|==
literal|null
assert|;
comment|// Even if no documents actually succeed in setting a norm, we still write norms for this segment:
name|norms
operator|=
operator|new
name|NormValuesWriter
argument_list|(
name|fieldInfo
argument_list|,
name|docState
operator|.
name|docWriter
operator|.
name|bytesUsed
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
DECL|method|compareTo
specifier|public
name|int
name|compareTo
parameter_list|(
name|PerField
name|other
parameter_list|)
block|{
return|return
name|this
operator|.
name|fieldInfo
operator|.
name|name
operator|.
name|compareTo
argument_list|(
name|other
operator|.
name|fieldInfo
operator|.
name|name
argument_list|)
return|;
block|}
DECL|method|finish
specifier|public
name|void
name|finish
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|fieldInfo
operator|.
name|omitsNorms
argument_list|()
operator|==
literal|false
operator|&&
name|invertState
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
name|norms
operator|.
name|addValue
argument_list|(
name|docState
operator|.
name|docID
argument_list|,
name|similarity
operator|.
name|computeNorm
argument_list|(
name|invertState
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|termsHashPerField
operator|.
name|finish
argument_list|()
expr_stmt|;
block|}
comment|/** Inverts one field for one document; first is true      *  if this is the first time we are seeing this field      *  name in this document. */
DECL|method|invert
specifier|public
name|void
name|invert
parameter_list|(
name|IndexableField
name|field
parameter_list|,
name|boolean
name|first
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|first
condition|)
block|{
comment|// First time we're seeing this field (indexed) in
comment|// this document:
name|invertState
operator|.
name|reset
argument_list|()
expr_stmt|;
block|}
name|IndexableFieldType
name|fieldType
init|=
name|field
operator|.
name|fieldType
argument_list|()
decl_stmt|;
specifier|final
name|boolean
name|analyzed
init|=
name|fieldType
operator|.
name|tokenized
argument_list|()
operator|&&
name|docState
operator|.
name|analyzer
operator|!=
literal|null
decl_stmt|;
comment|// only bother checking offsets if something will consume them.
comment|// TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.
specifier|final
name|boolean
name|checkOffsets
init|=
name|fieldType
operator|.
name|indexOptions
argument_list|()
operator|==
name|IndexOptions
operator|.
name|DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS
decl_stmt|;
comment|/*        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,        * but rather a finally that takes note of the problem.        */
name|boolean
name|aborting
init|=
literal|false
decl_stmt|;
name|boolean
name|succeededInProcessingField
init|=
literal|false
decl_stmt|;
try|try
init|(
name|TokenStream
name|stream
init|=
name|tokenStream
operator|=
name|field
operator|.
name|tokenStream
argument_list|(
name|docState
operator|.
name|analyzer
argument_list|,
name|tokenStream
argument_list|)
init|)
block|{
comment|// reset the TokenStream to the first token
name|stream
operator|.
name|reset
argument_list|()
expr_stmt|;
name|invertState
operator|.
name|setAttributeSource
argument_list|(
name|stream
argument_list|)
expr_stmt|;
name|termsHashPerField
operator|.
name|start
argument_list|(
name|field
argument_list|,
name|first
argument_list|)
expr_stmt|;
while|while
condition|(
name|stream
operator|.
name|incrementToken
argument_list|()
condition|)
block|{
comment|// If we hit an exception in stream.next below
comment|// (which is fairly common, e.g. if analyzer
comment|// chokes on a given document), then it's
comment|// non-aborting and (above) this one document
comment|// will be marked as deleted, but still
comment|// consume a docID
name|int
name|posIncr
init|=
name|invertState
operator|.
name|posIncrAttribute
operator|.
name|getPositionIncrement
argument_list|()
decl_stmt|;
name|invertState
operator|.
name|position
operator|+=
name|posIncr
expr_stmt|;
if|if
condition|(
name|invertState
operator|.
name|position
operator|<
name|invertState
operator|.
name|lastPosition
condition|)
block|{
if|if
condition|(
name|posIncr
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"first position increment must be> 0 (got 0) for field '"
operator|+
name|field
operator|.
name|name
argument_list|()
operator|+
literal|"'"
argument_list|)
throw|;
block|}
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"position increments (and gaps) must be>= 0 (got "
operator|+
name|posIncr
operator|+
literal|") for field '"
operator|+
name|field
operator|.
name|name
argument_list|()
operator|+
literal|"'"
argument_list|)
throw|;
block|}
name|invertState
operator|.
name|lastPosition
operator|=
name|invertState
operator|.
name|position
expr_stmt|;
if|if
condition|(
name|posIncr
operator|==
literal|0
condition|)
block|{
name|invertState
operator|.
name|numOverlap
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|checkOffsets
condition|)
block|{
name|int
name|startOffset
init|=
name|invertState
operator|.
name|offset
operator|+
name|invertState
operator|.
name|offsetAttribute
operator|.
name|startOffset
argument_list|()
decl_stmt|;
name|int
name|endOffset
init|=
name|invertState
operator|.
name|offset
operator|+
name|invertState
operator|.
name|offsetAttribute
operator|.
name|endOffset
argument_list|()
decl_stmt|;
if|if
condition|(
name|startOffset
operator|<
name|invertState
operator|.
name|lastStartOffset
operator|||
name|endOffset
operator|<
name|startOffset
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"startOffset must be non-negative, and endOffset must be>= startOffset, and offsets must not go backwards "
operator|+
literal|"startOffset="
operator|+
name|startOffset
operator|+
literal|",endOffset="
operator|+
name|endOffset
operator|+
literal|",lastStartOffset="
operator|+
name|invertState
operator|.
name|lastStartOffset
operator|+
literal|" for field '"
operator|+
name|field
operator|.
name|name
argument_list|()
operator|+
literal|"'"
argument_list|)
throw|;
block|}
name|invertState
operator|.
name|lastStartOffset
operator|=
name|startOffset
expr_stmt|;
block|}
name|invertState
operator|.
name|length
operator|++
expr_stmt|;
if|if
condition|(
name|invertState
operator|.
name|length
operator|<
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"too many tokens in field '"
operator|+
name|field
operator|.
name|name
argument_list|()
operator|+
literal|"'"
argument_list|)
throw|;
block|}
comment|//System.out.println("  term=" + invertState.termAttribute);
comment|// If we hit an exception in here, we abort
comment|// all buffered documents since the last
comment|// flush, on the likelihood that the
comment|// internal state of the terms hash is now
comment|// corrupt and should not be flushed to a
comment|// new segment:
name|aborting
operator|=
literal|true
expr_stmt|;
name|termsHashPerField
operator|.
name|add
argument_list|()
expr_stmt|;
name|aborting
operator|=
literal|false
expr_stmt|;
block|}
comment|// trigger streams to perform end-of-stream operations
name|stream
operator|.
name|end
argument_list|()
expr_stmt|;
comment|// TODO: maybe add some safety? then again, its already checked
comment|// when we come back around to the field...
name|invertState
operator|.
name|position
operator|+=
name|invertState
operator|.
name|posIncrAttribute
operator|.
name|getPositionIncrement
argument_list|()
expr_stmt|;
name|invertState
operator|.
name|offset
operator|+=
name|invertState
operator|.
name|offsetAttribute
operator|.
name|endOffset
argument_list|()
expr_stmt|;
comment|/* if there is an exception coming through, we won't set this to true here:*/
name|succeededInProcessingField
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MaxBytesLengthExceededException
name|e
parameter_list|)
block|{
name|aborting
operator|=
literal|false
expr_stmt|;
name|byte
index|[]
name|prefix
init|=
operator|new
name|byte
index|[
literal|30
index|]
decl_stmt|;
name|BytesRef
name|bigTerm
init|=
name|invertState
operator|.
name|termAttribute
operator|.
name|getBytesRef
argument_list|()
decl_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|bigTerm
operator|.
name|bytes
argument_list|,
name|bigTerm
operator|.
name|offset
argument_list|,
name|prefix
argument_list|,
literal|0
argument_list|,
literal|30
argument_list|)
expr_stmt|;
name|String
name|msg
init|=
literal|"Document contains at least one immense term in field=\""
operator|+
name|fieldInfo
operator|.
name|name
operator|+
literal|"\" (whose UTF8 encoding is longer than the max length "
operator|+
name|DocumentsWriterPerThread
operator|.
name|MAX_TERM_LENGTH_UTF8
operator|+
literal|"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '"
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|prefix
argument_list|)
operator|+
literal|"...', original message: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
if|if
condition|(
name|docState
operator|.
name|infoStream
operator|.
name|isEnabled
argument_list|(
literal|"IW"
argument_list|)
condition|)
block|{
name|docState
operator|.
name|infoStream
operator|.
name|message
argument_list|(
literal|"IW"
argument_list|,
literal|"ERROR: "
operator|+
name|msg
argument_list|)
expr_stmt|;
block|}
comment|// Document will be deleted above:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
if|if
condition|(
name|succeededInProcessingField
operator|==
literal|false
operator|&&
name|aborting
condition|)
block|{
name|docState
operator|.
name|docWriter
operator|.
name|setAborting
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|succeededInProcessingField
operator|&&
name|docState
operator|.
name|infoStream
operator|.
name|isEnabled
argument_list|(
literal|"DW"
argument_list|)
condition|)
block|{
name|docState
operator|.
name|infoStream
operator|.
name|message
argument_list|(
literal|"DW"
argument_list|,
literal|"An exception was thrown while processing field "
operator|+
name|fieldInfo
operator|.
name|name
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|analyzed
condition|)
block|{
name|invertState
operator|.
name|position
operator|+=
name|docState
operator|.
name|analyzer
operator|.
name|getPositionIncrementGap
argument_list|(
name|fieldInfo
operator|.
name|name
argument_list|)
expr_stmt|;
name|invertState
operator|.
name|offset
operator|+=
name|docState
operator|.
name|analyzer
operator|.
name|getOffsetGap
argument_list|(
name|fieldInfo
operator|.
name|name
argument_list|)
expr_stmt|;
block|}
name|invertState
operator|.
name|boost
operator|*=
name|field
operator|.
name|boost
argument_list|()
expr_stmt|;
block|}
block|}
block|}
end_class
end_unit
