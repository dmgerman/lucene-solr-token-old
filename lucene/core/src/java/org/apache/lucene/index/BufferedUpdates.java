begin_unit
begin_package
DECL|package|org.apache.lucene.index
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
package|;
end_package
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicLong
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|Query
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|RamUsageEstimator
import|;
end_import
begin_comment
comment|/* Holds buffered deletes and updates, by docID, term or query for a  * single segment. This is used to hold buffered pending  * deletes and updates against the to-be-flushed segment.  Once the  * deletes and updates are pushed (on flush in DocumentsWriter), they  * are converted to a FrozenDeletes instance. */
end_comment
begin_comment
comment|// NOTE: instances of this class are accessed either via a private
end_comment
begin_comment
comment|// instance on DocumentWriterPerThread, or via sync'd code by
end_comment
begin_comment
comment|// DocumentsWriterDeleteQueue
end_comment
begin_class
DECL|class|BufferedUpdates
class|class
name|BufferedUpdates
block|{
comment|/* Rough logic: HashMap has an array[Entry] w/ varying      load factor (say 2 * POINTER).  Entry is object w/ Term      key, Integer val, int hash, Entry next      (OBJ_HEADER + 3*POINTER + INT).  Term is object w/      String field and String text (OBJ_HEADER + 2*POINTER).      Term's field is String (OBJ_HEADER + 4*INT + POINTER +      OBJ_HEADER + string.length*CHAR).      Term's text is String (OBJ_HEADER + 4*INT + POINTER +      OBJ_HEADER + string.length*CHAR).  Integer is      OBJ_HEADER + INT. */
DECL|field|BYTES_PER_DEL_TERM
specifier|final
specifier|static
name|int
name|BYTES_PER_DEL_TERM
init|=
literal|9
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
operator|+
literal|7
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_HEADER
operator|+
literal|10
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
decl_stmt|;
comment|/* Rough logic: del docIDs are List<Integer>.  Say list      allocates ~2X size (2*POINTER).  Integer is OBJ_HEADER      + int */
DECL|field|BYTES_PER_DEL_DOCID
specifier|final
specifier|static
name|int
name|BYTES_PER_DEL_DOCID
init|=
literal|2
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_HEADER
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
decl_stmt|;
comment|/* Rough logic: HashMap has an array[Entry] w/ varying      load factor (say 2 * POINTER).  Entry is object w/      Query key, Integer val, int hash, Entry next      (OBJ_HEADER + 3*POINTER + INT).  Query we often      undercount (say 24 bytes).  Integer is OBJ_HEADER + INT. */
DECL|field|BYTES_PER_DEL_QUERY
specifier|final
specifier|static
name|int
name|BYTES_PER_DEL_QUERY
init|=
literal|5
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
operator|+
literal|2
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_HEADER
operator|+
literal|2
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
operator|+
literal|24
decl_stmt|;
comment|/* Rough logic: NumericUpdate calculates its actual size,    * including the update Term and DV field (String). The     * per-field map holds a reference to the updated field, and    * therefore we only account for the object reference and     * map space itself. This is incremented when we first see    * an updated field.    *     * HashMap has an array[Entry] w/ varying load    * factor (say 2*POINTER). Entry is an object w/ String key,     * LinkedHashMap val, int hash, Entry next (OBJ_HEADER + 3*POINTER + INT).    *     * LinkedHashMap (val) is counted as OBJ_HEADER, array[Entry] ref + header, 4*INT, 1*FLOAT,    * Set (entrySet) (2*OBJ_HEADER + ARRAY_HEADER + 2*POINTER + 4*INT + FLOAT)    */
DECL|field|BYTES_PER_NUMERIC_FIELD_ENTRY
specifier|final
specifier|static
name|int
name|BYTES_PER_NUMERIC_FIELD_ENTRY
init|=
literal|7
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
operator|+
literal|3
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_HEADER
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_ARRAY_HEADER
operator|+
literal|5
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_FLOAT
decl_stmt|;
comment|/* Rough logic: Incremented when we see another Term for an already updated    * field.    * LinkedHashMap has an array[Entry] w/ varying load factor     * (say 2*POINTER). Entry is an object w/ Term key, NumericUpdate val,     * int hash, Entry next, Entry before, Entry after (OBJ_HEADER + 5*POINTER + INT).    *     * Term (key) is counted only as POINTER.    * NumericUpdate (val) counts its own size and isn't accounted for here.    */
DECL|field|BYTES_PER_NUMERIC_UPDATE_ENTRY
specifier|final
specifier|static
name|int
name|BYTES_PER_NUMERIC_UPDATE_ENTRY
init|=
literal|7
operator|*
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_REF
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_OBJECT_HEADER
operator|+
name|RamUsageEstimator
operator|.
name|NUM_BYTES_INT
decl_stmt|;
DECL|field|numTermDeletes
specifier|final
name|AtomicInteger
name|numTermDeletes
init|=
operator|new
name|AtomicInteger
argument_list|()
decl_stmt|;
DECL|field|numNumericUpdates
specifier|final
name|AtomicInteger
name|numNumericUpdates
init|=
operator|new
name|AtomicInteger
argument_list|()
decl_stmt|;
DECL|field|terms
specifier|final
name|Map
argument_list|<
name|Term
argument_list|,
name|Integer
argument_list|>
name|terms
init|=
operator|new
name|HashMap
argument_list|<
name|Term
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
DECL|field|queries
specifier|final
name|Map
argument_list|<
name|Query
argument_list|,
name|Integer
argument_list|>
name|queries
init|=
operator|new
name|HashMap
argument_list|<
name|Query
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
DECL|field|docIDs
specifier|final
name|List
argument_list|<
name|Integer
argument_list|>
name|docIDs
init|=
operator|new
name|ArrayList
argument_list|<
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
comment|// Map<dvField,Map<updateTerm,NumericUpdate>>
comment|// For each field we keep an ordered list of NumericUpdates, key'd by the
comment|// update Term. LinkedHashMap guarantees we will later traverse the map in
comment|// insertion order (so that if two terms affect the same document, the last
comment|// one that came in wins), and helps us detect faster if the same Term is
comment|// used to update the same field multiple times (so we later traverse it
comment|// only once).
DECL|field|numericUpdates
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|LinkedHashMap
argument_list|<
name|Term
argument_list|,
name|NumericUpdate
argument_list|>
argument_list|>
name|numericUpdates
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|LinkedHashMap
argument_list|<
name|Term
argument_list|,
name|NumericUpdate
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
DECL|field|MAX_INT
specifier|public
specifier|static
specifier|final
name|Integer
name|MAX_INT
init|=
name|Integer
operator|.
name|valueOf
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
decl_stmt|;
DECL|field|bytesUsed
specifier|final
name|AtomicLong
name|bytesUsed
decl_stmt|;
DECL|field|VERBOSE_DELETES
specifier|private
specifier|final
specifier|static
name|boolean
name|VERBOSE_DELETES
init|=
literal|false
decl_stmt|;
DECL|field|gen
name|long
name|gen
decl_stmt|;
DECL|method|BufferedUpdates
specifier|public
name|BufferedUpdates
parameter_list|()
block|{
name|this
operator|.
name|bytesUsed
operator|=
operator|new
name|AtomicLong
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|toString
specifier|public
name|String
name|toString
parameter_list|()
block|{
if|if
condition|(
name|VERBOSE_DELETES
condition|)
block|{
return|return
literal|"gen="
operator|+
name|gen
operator|+
literal|" numTerms="
operator|+
name|numTermDeletes
operator|+
literal|", terms="
operator|+
name|terms
operator|+
literal|", queries="
operator|+
name|queries
operator|+
literal|", docIDs="
operator|+
name|docIDs
operator|+
literal|", numericUpdates="
operator|+
name|numericUpdates
operator|+
literal|", bytesUsed="
operator|+
name|bytesUsed
return|;
block|}
else|else
block|{
name|String
name|s
init|=
literal|"gen="
operator|+
name|gen
decl_stmt|;
if|if
condition|(
name|numTermDeletes
operator|.
name|get
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|s
operator|+=
literal|" "
operator|+
name|numTermDeletes
operator|.
name|get
argument_list|()
operator|+
literal|" deleted terms (unique count="
operator|+
name|terms
operator|.
name|size
argument_list|()
operator|+
literal|")"
expr_stmt|;
block|}
if|if
condition|(
name|queries
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|s
operator|+=
literal|" "
operator|+
name|queries
operator|.
name|size
argument_list|()
operator|+
literal|" deleted queries"
expr_stmt|;
block|}
if|if
condition|(
name|docIDs
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|s
operator|+=
literal|" "
operator|+
name|docIDs
operator|.
name|size
argument_list|()
operator|+
literal|" deleted docIDs"
expr_stmt|;
block|}
if|if
condition|(
name|numNumericUpdates
operator|.
name|get
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|s
operator|+=
literal|" "
operator|+
name|numNumericUpdates
operator|.
name|get
argument_list|()
operator|+
literal|" numeric updates (unique count="
operator|+
name|numericUpdates
operator|.
name|size
argument_list|()
operator|+
literal|")"
expr_stmt|;
block|}
if|if
condition|(
name|bytesUsed
operator|.
name|get
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|s
operator|+=
literal|" bytesUsed="
operator|+
name|bytesUsed
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
return|return
name|s
return|;
block|}
block|}
DECL|method|addQuery
specifier|public
name|void
name|addQuery
parameter_list|(
name|Query
name|query
parameter_list|,
name|int
name|docIDUpto
parameter_list|)
block|{
name|Integer
name|current
init|=
name|queries
operator|.
name|put
argument_list|(
name|query
argument_list|,
name|docIDUpto
argument_list|)
decl_stmt|;
comment|// increment bytes used only if the query wasn't added so far.
if|if
condition|(
name|current
operator|==
literal|null
condition|)
block|{
name|bytesUsed
operator|.
name|addAndGet
argument_list|(
name|BYTES_PER_DEL_QUERY
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|addDocID
specifier|public
name|void
name|addDocID
parameter_list|(
name|int
name|docID
parameter_list|)
block|{
name|docIDs
operator|.
name|add
argument_list|(
name|Integer
operator|.
name|valueOf
argument_list|(
name|docID
argument_list|)
argument_list|)
expr_stmt|;
name|bytesUsed
operator|.
name|addAndGet
argument_list|(
name|BYTES_PER_DEL_DOCID
argument_list|)
expr_stmt|;
block|}
DECL|method|addTerm
specifier|public
name|void
name|addTerm
parameter_list|(
name|Term
name|term
parameter_list|,
name|int
name|docIDUpto
parameter_list|)
block|{
name|Integer
name|current
init|=
name|terms
operator|.
name|get
argument_list|(
name|term
argument_list|)
decl_stmt|;
if|if
condition|(
name|current
operator|!=
literal|null
operator|&&
name|docIDUpto
operator|<
name|current
condition|)
block|{
comment|// Only record the new number if it's greater than the
comment|// current one.  This is important because if multiple
comment|// threads are replacing the same doc at nearly the
comment|// same time, it's possible that one thread that got a
comment|// higher docID is scheduled before the other
comment|// threads.  If we blindly replace than we can
comment|// incorrectly get both docs indexed.
return|return;
block|}
name|terms
operator|.
name|put
argument_list|(
name|term
argument_list|,
name|Integer
operator|.
name|valueOf
argument_list|(
name|docIDUpto
argument_list|)
argument_list|)
expr_stmt|;
comment|// note that if current != null then it means there's already a buffered
comment|// delete on that term, therefore we seem to over-count. this over-counting
comment|// is done to respect IndexWriterConfig.setMaxBufferedDeleteTerms.
name|numTermDeletes
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
if|if
condition|(
name|current
operator|==
literal|null
condition|)
block|{
name|bytesUsed
operator|.
name|addAndGet
argument_list|(
name|BYTES_PER_DEL_TERM
operator|+
name|term
operator|.
name|bytes
operator|.
name|length
operator|+
operator|(
name|RamUsageEstimator
operator|.
name|NUM_BYTES_CHAR
operator|*
name|term
operator|.
name|field
argument_list|()
operator|.
name|length
argument_list|()
operator|)
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|addNumericUpdate
specifier|public
name|void
name|addNumericUpdate
parameter_list|(
name|NumericUpdate
name|update
parameter_list|,
name|int
name|docIDUpto
parameter_list|)
block|{
name|LinkedHashMap
argument_list|<
name|Term
argument_list|,
name|NumericUpdate
argument_list|>
name|fieldUpdates
init|=
name|numericUpdates
operator|.
name|get
argument_list|(
name|update
operator|.
name|field
argument_list|)
decl_stmt|;
if|if
condition|(
name|fieldUpdates
operator|==
literal|null
condition|)
block|{
name|fieldUpdates
operator|=
operator|new
name|LinkedHashMap
argument_list|<
name|Term
argument_list|,
name|NumericUpdate
argument_list|>
argument_list|()
expr_stmt|;
name|numericUpdates
operator|.
name|put
argument_list|(
name|update
operator|.
name|field
argument_list|,
name|fieldUpdates
argument_list|)
expr_stmt|;
name|bytesUsed
operator|.
name|addAndGet
argument_list|(
name|BYTES_PER_NUMERIC_FIELD_ENTRY
argument_list|)
expr_stmt|;
block|}
specifier|final
name|NumericUpdate
name|current
init|=
name|fieldUpdates
operator|.
name|get
argument_list|(
name|update
operator|.
name|term
argument_list|)
decl_stmt|;
if|if
condition|(
name|current
operator|!=
literal|null
operator|&&
name|docIDUpto
operator|<
name|current
operator|.
name|docIDUpto
condition|)
block|{
comment|// Only record the new number if it's greater than or equal to the current
comment|// one. This is important because if multiple threads are replacing the
comment|// same doc at nearly the same time, it's possible that one thread that
comment|// got a higher docID is scheduled before the other threads.
return|return;
block|}
name|update
operator|.
name|docIDUpto
operator|=
name|docIDUpto
expr_stmt|;
comment|// since it's a LinkedHashMap, we must first remove the Term entry so that
comment|// it's added last (we're interested in insertion-order).
if|if
condition|(
name|current
operator|!=
literal|null
condition|)
block|{
name|fieldUpdates
operator|.
name|remove
argument_list|(
name|update
operator|.
name|term
argument_list|)
expr_stmt|;
block|}
name|fieldUpdates
operator|.
name|put
argument_list|(
name|update
operator|.
name|term
argument_list|,
name|update
argument_list|)
expr_stmt|;
name|numNumericUpdates
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
if|if
condition|(
name|current
operator|==
literal|null
condition|)
block|{
name|bytesUsed
operator|.
name|addAndGet
argument_list|(
name|BYTES_PER_NUMERIC_UPDATE_ENTRY
operator|+
name|update
operator|.
name|sizeInBytes
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|clear
name|void
name|clear
parameter_list|()
block|{
name|terms
operator|.
name|clear
argument_list|()
expr_stmt|;
name|queries
operator|.
name|clear
argument_list|()
expr_stmt|;
name|docIDs
operator|.
name|clear
argument_list|()
expr_stmt|;
name|numericUpdates
operator|.
name|clear
argument_list|()
expr_stmt|;
name|numTermDeletes
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|numNumericUpdates
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|bytesUsed
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
DECL|method|any
name|boolean
name|any
parameter_list|()
block|{
return|return
name|terms
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|||
name|docIDs
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|||
name|queries
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|||
name|numericUpdates
operator|.
name|size
argument_list|()
operator|>
literal|0
return|;
block|}
block|}
end_class
end_unit
