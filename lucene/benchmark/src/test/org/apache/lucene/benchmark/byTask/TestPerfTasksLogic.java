begin_unit
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_package
DECL|package|org.apache.lucene.benchmark.byTask
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
package|;
end_package
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedReader
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|StandardCharsets
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Files
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Path
import|;
end_import
begin_import
import|import
name|java
operator|.
name|text
operator|.
name|Collator
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Locale
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|Analyzer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|BaseTokenStreamTestCase
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|MockAnalyzer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|TermToBytesRefAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|BenchmarkTestCase
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|feeds
operator|.
name|DocMaker
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|feeds
operator|.
name|ReutersQueryMaker
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|stats
operator|.
name|TaskStats
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|tasks
operator|.
name|CountingHighlighterTestTask
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|tasks
operator|.
name|CountingSearchTestTask
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|benchmark
operator|.
name|byTask
operator|.
name|tasks
operator|.
name|WriteLineDocTask
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|collation
operator|.
name|CollationKeyAnalyzer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|TaxonomyReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|DirectoryReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|PostingsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Fields
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
operator|.
name|OpenMode
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LogDocMergePolicy
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LogMergePolicy
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|MultiFields
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|SegmentInfos
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|SerialMergeScheduler
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Terms
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|TermsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|DocIdSetIterator
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|Directory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|TestUtil
import|;
end_import
begin_comment
comment|/**  * Test very simply that perf tasks - simple algorithms - are doing what they should.  */
end_comment
begin_class
DECL|class|TestPerfTasksLogic
specifier|public
class|class
name|TestPerfTasksLogic
extends|extends
name|BenchmarkTestCase
block|{
annotation|@
name|Override
DECL|method|setUp
specifier|public
name|void
name|setUp
parameter_list|()
throws|throws
name|Exception
block|{
name|super
operator|.
name|setUp
argument_list|()
expr_stmt|;
name|copyToWorkDir
argument_list|(
literal|"reuters.first20.lines.txt"
argument_list|)
expr_stmt|;
name|copyToWorkDir
argument_list|(
literal|"test-mapping-ISOLatin1Accent-partial.txt"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test index creation logic    */
DECL|method|testIndexAndSearchTasks
specifier|public
name|void
name|testIndexAndSearchTasks
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 1000"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingSearchTest } : 200"
block|,
literal|"CloseReader"
block|,
literal|"[ CountingSearchTest> : 70"
block|,
literal|"[ CountingSearchTest> : 9"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingSearchTestTask
operator|.
name|numSearches
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 4. test specific checks after the benchmark run completed.
name|assertEquals
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
literal|279
argument_list|,
name|CountingSearchTestTask
operator|.
name|numSearches
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
literal|"Index does not exist?...!"
argument_list|,
name|DirectoryReader
operator|.
name|indexExists
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// now we should be able to open the index for write.
name|IndexWriter
name|iw
init|=
operator|new
name|IndexWriter
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|,
operator|new
name|IndexWriterConfig
argument_list|(
operator|new
name|MockAnalyzer
argument_list|(
name|random
argument_list|()
argument_list|)
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|OpenMode
operator|.
name|APPEND
argument_list|)
argument_list|)
decl_stmt|;
name|iw
operator|.
name|close
argument_list|()
expr_stmt|;
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
literal|"1000 docs were added to the index, this is what we expect to find!"
argument_list|,
literal|1000
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test timed sequence task.    */
DECL|method|testTimedSearchTask
specifier|public
name|void
name|testTimedSearchTask
parameter_list|()
throws|throws
name|Exception
block|{
name|String
name|algLines
index|[]
init|=
block|{
literal|"log.step=100000"
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 100"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingSearchTest } : .5s"
block|,
literal|"CloseReader"
block|,     }
decl_stmt|;
name|CountingSearchTestTask
operator|.
name|numSearches
operator|=
literal|0
expr_stmt|;
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
name|CountingSearchTestTask
operator|.
name|numSearches
operator|>
literal|0
argument_list|)
expr_stmt|;
name|long
name|elapsed
init|=
name|CountingSearchTestTask
operator|.
name|prevLastMillis
operator|-
name|CountingSearchTestTask
operator|.
name|startMillis
decl_stmt|;
name|assertTrue
argument_list|(
literal|"elapsed time was "
operator|+
name|elapsed
operator|+
literal|" msec"
argument_list|,
name|elapsed
operator|<=
literal|1500
argument_list|)
expr_stmt|;
block|}
comment|// disabled until we fix BG thread prio -- this test
comment|// causes build to hang
DECL|method|testBGSearchTaskThreads
specifier|public
name|void
name|testBGSearchTaskThreads
parameter_list|()
throws|throws
name|Exception
block|{
name|String
name|algLines
index|[]
init|=
block|{
literal|"log.time.step.msec = 100"
block|,
literal|"log.step=100000"
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 1000"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{"
block|,
literal|"  [ \"XSearch\" { CountingSearchTest> : * ] : 2&-1"
block|,
literal|"  Wait(0.5)"
block|,
literal|"}"
block|,
literal|"CloseReader"
block|,
literal|"RepSumByPref X"
block|}
decl_stmt|;
name|CountingSearchTestTask
operator|.
name|numSearches
operator|=
literal|0
expr_stmt|;
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
expr_stmt|;
comment|// NOTE: cannot assert this, because on a super-slow
comment|// system, it could be after waiting 0.5 seconds that
comment|// the search threads hadn't yet succeeded in starting
comment|// up and then they start up and do no searching:
comment|//assertTrue(CountingSearchTestTask.numSearches> 0);
block|}
DECL|method|testHighlighting
specifier|public
name|void
name|testHighlighting
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"doc.stored=true"
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"query.maker="
operator|+
name|ReutersQueryMaker
operator|.
name|class
operator|.
name|getName
argument_list|()
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 100"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"
block|,
literal|"CloseReader"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|=
literal|0
expr_stmt|;
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 4. test specific checks after the benchmark run completed.
name|assertEquals
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
literal|92
argument_list|,
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
argument_list|)
expr_stmt|;
comment|//pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
comment|//we probably should use a different doc/query maker, but...
name|assertTrue
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
operator|>=
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|&&
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|>
literal|0
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
literal|"Index does not exist?...!"
argument_list|,
name|DirectoryReader
operator|.
name|indexExists
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// now we should be able to open the index for write.
name|IndexWriter
name|iw
init|=
operator|new
name|IndexWriter
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|,
operator|new
name|IndexWriterConfig
argument_list|(
operator|new
name|MockAnalyzer
argument_list|(
name|random
argument_list|()
argument_list|)
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|OpenMode
operator|.
name|APPEND
argument_list|)
argument_list|)
decl_stmt|;
name|iw
operator|.
name|close
argument_list|()
expr_stmt|;
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
literal|"100 docs were added to the index, this is what we expect to find!"
argument_list|,
literal|100
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
DECL|method|testHighlightingTV
specifier|public
name|void
name|testHighlightingTV
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"doc.stored=true"
block|,
comment|//doc storage is required in order to have text to highlight
literal|"doc.term.vector=true"
block|,
literal|"doc.term.vector.offsets=true"
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"query.maker="
operator|+
name|ReutersQueryMaker
operator|.
name|class
operator|.
name|getName
argument_list|()
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 1000"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"
block|,
literal|"CloseReader"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|=
literal|0
expr_stmt|;
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 4. test specific checks after the benchmark run completed.
name|assertEquals
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
literal|92
argument_list|,
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
argument_list|)
expr_stmt|;
comment|//pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
comment|//we probably should use a different doc/query maker, but...
name|assertTrue
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
operator|>=
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|&&
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|>
literal|0
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
literal|"Index does not exist?...!"
argument_list|,
name|DirectoryReader
operator|.
name|indexExists
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// now we should be able to open the index for write.
name|IndexWriter
name|iw
init|=
operator|new
name|IndexWriter
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|,
operator|new
name|IndexWriterConfig
argument_list|(
operator|new
name|MockAnalyzer
argument_list|(
name|random
argument_list|()
argument_list|)
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|OpenMode
operator|.
name|APPEND
argument_list|)
argument_list|)
decl_stmt|;
name|iw
operator|.
name|close
argument_list|()
expr_stmt|;
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
literal|"1000 docs were added to the index, this is what we expect to find!"
argument_list|,
literal|1000
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
DECL|method|testHighlightingNoTvNoStore
specifier|public
name|void
name|testHighlightingNoTvNoStore
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"doc.stored=false"
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"query.maker="
operator|+
name|ReutersQueryMaker
operator|.
name|class
operator|.
name|getName
argument_list|()
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : 1000"
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"
block|,
literal|"CloseReader"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingHighlighterTestTask
operator|.
name|numHighlightedResults
operator|=
literal|0
expr_stmt|;
name|CountingHighlighterTestTask
operator|.
name|numDocsRetrieved
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
try|try
block|{
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
name|assertTrue
argument_list|(
literal|"CountingHighlighterTest should have thrown an exception"
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|assertNotNull
argument_list|(
name|benchmark
argument_list|)
expr_stmt|;
comment|// (avoid compile warning on unused variable)
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|assertTrue
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Test Exhasting Doc Maker logic    */
DECL|method|testExhaustContentSource
specifier|public
name|void
name|testExhaustContentSource
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"
block|,
literal|"content.source.log.step=1"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"# ----- alg "
block|,
literal|"CreateIndex"
block|,
literal|"{ AddDoc } : * "
block|,
literal|"ForceMerge(1)"
block|,
literal|"CloseIndex"
block|,
literal|"OpenReader"
block|,
literal|"{ CountingSearchTest } : 100"
block|,
literal|"CloseReader"
block|,
literal|"[ CountingSearchTest> : 30"
block|,
literal|"[ CountingSearchTest> : 9"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingSearchTestTask
operator|.
name|numSearches
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 4. test specific checks after the benchmark run completed.
name|assertEquals
argument_list|(
literal|"TestSearchTask was supposed to be called!"
argument_list|,
literal|139
argument_list|,
name|CountingSearchTestTask
operator|.
name|numSearches
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
literal|"Index does not exist?...!"
argument_list|,
name|DirectoryReader
operator|.
name|indexExists
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// now we should be able to open the index for write.
name|IndexWriter
name|iw
init|=
operator|new
name|IndexWriter
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|,
operator|new
name|IndexWriterConfig
argument_list|(
operator|new
name|MockAnalyzer
argument_list|(
name|random
argument_list|()
argument_list|)
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|OpenMode
operator|.
name|APPEND
argument_list|)
argument_list|)
decl_stmt|;
name|iw
operator|.
name|close
argument_list|()
expr_stmt|;
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
literal|"1 docs were added to the index, this is what we expect to find!"
argument_list|,
literal|1
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// LUCENE-1994: test thread safety of SortableSingleDocMaker
DECL|method|testDocMakerThreadSafety
specifier|public
name|void
name|testDocMakerThreadSafety
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource"
block|,
literal|"doc.term.vector=false"
block|,
literal|"log.step.AddDoc=10000"
block|,
literal|"content.source.forever=true"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.reuse.fields=false"
block|,
literal|"doc.stored=true"
block|,
literal|"doc.tokenized=false"
block|,
literal|"doc.index.props=true"
block|,
literal|"# ----- alg "
block|,
literal|"CreateIndex"
block|,
literal|"[ { AddDoc> : 250 ] : 4"
block|,
literal|"CloseIndex"
block|,     }
decl_stmt|;
comment|// 2. we test this value later
name|CountingSearchTestTask
operator|.
name|numSearches
operator|=
literal|0
expr_stmt|;
comment|// 3. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
name|DirectoryReader
name|r
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
specifier|final
name|int
name|maxDoc
init|=
name|r
operator|.
name|maxDoc
argument_list|()
decl_stmt|;
name|assertEquals
argument_list|(
literal|1000
argument_list|,
name|maxDoc
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
literal|1000
condition|;
name|i
operator|++
control|)
block|{
name|assertNotNull
argument_list|(
literal|"doc "
operator|+
name|i
operator|+
literal|" has null country"
argument_list|,
name|r
operator|.
name|document
argument_list|(
name|i
argument_list|)
operator|.
name|getField
argument_list|(
literal|"country"
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|r
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test Parallel Doc Maker logic (for LUCENE-940)    */
DECL|method|testParallelDocMaker
specifier|public
name|void
name|testParallelDocMaker
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=FSDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"# ----- alg "
block|,
literal|"CreateIndex"
block|,
literal|"[ { AddDoc } : * ] : 4 "
block|,
literal|"CloseIndex"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test WriteLineDoc and LineDocSource.    */
DECL|method|testLineDocFile
specifier|public
name|void
name|testLineDocFile
parameter_list|()
throws|throws
name|Exception
block|{
name|Path
name|lineFile
init|=
name|createTempFile
argument_list|(
literal|"test.reuters.lines"
argument_list|,
literal|".txt"
argument_list|)
decl_stmt|;
comment|// We will call WriteLineDocs this many times
specifier|final
name|int
name|NUM_TRY_DOCS
init|=
literal|50
decl_stmt|;
comment|// Creates a line file with first 50 docs from SingleDocSource
name|String
name|algLines1
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"
block|,
literal|"content.source.forever=true"
block|,
literal|"line.file.out="
operator|+
name|lineFile
operator|.
name|toAbsolutePath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|replace
argument_list|(
literal|'\\'
argument_list|,
literal|'/'
argument_list|)
block|,
literal|"# ----- alg "
block|,
literal|"{WriteLineDoc()}:"
operator|+
name|NUM_TRY_DOCS
block|,     }
decl_stmt|;
comment|// Run algo
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines1
argument_list|)
decl_stmt|;
name|BufferedReader
name|r
init|=
name|Files
operator|.
name|newBufferedReader
argument_list|(
name|lineFile
argument_list|,
name|StandardCharsets
operator|.
name|UTF_8
argument_list|)
decl_stmt|;
name|int
name|numLines
init|=
literal|0
decl_stmt|;
name|String
name|line
decl_stmt|;
while|while
condition|(
operator|(
name|line
operator|=
name|r
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|numLines
operator|==
literal|0
operator|&&
name|line
operator|.
name|startsWith
argument_list|(
name|WriteLineDocTask
operator|.
name|FIELDS_HEADER_INDICATOR
argument_list|)
condition|)
block|{
continue|continue;
comment|// do not count the header line as a doc
block|}
name|numLines
operator|++
expr_stmt|;
block|}
name|r
operator|.
name|close
argument_list|()
expr_stmt|;
name|assertEquals
argument_list|(
literal|"did not see the right number of docs; should be "
operator|+
name|NUM_TRY_DOCS
operator|+
literal|" but was "
operator|+
name|numLines
argument_list|,
name|NUM_TRY_DOCS
argument_list|,
name|numLines
argument_list|)
expr_stmt|;
comment|// Index the line docs
name|String
name|algLines2
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer"
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|lineFile
operator|.
name|toAbsolutePath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|replace
argument_list|(
literal|'\\'
argument_list|,
literal|'/'
argument_list|)
block|,
literal|"content.source.forever=false"
block|,
literal|"doc.reuse.fields=false"
block|,
literal|"ram.flush.mb=4"
block|,
literal|"# ----- alg "
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{AddDoc}: *"
block|,
literal|"CloseIndex"
block|,     }
decl_stmt|;
comment|// Run algo
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|algLines2
argument_list|)
expr_stmt|;
comment|// now we should be able to open the index for write.
name|IndexWriter
name|iw
init|=
operator|new
name|IndexWriter
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|,
operator|new
name|IndexWriterConfig
argument_list|(
operator|new
name|MockAnalyzer
argument_list|(
name|random
argument_list|()
argument_list|)
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|OpenMode
operator|.
name|APPEND
argument_list|)
argument_list|)
decl_stmt|;
name|iw
operator|.
name|close
argument_list|()
expr_stmt|;
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
name|numLines
operator|+
literal|" lines were created but "
operator|+
name|ir
operator|.
name|numDocs
argument_list|()
operator|+
literal|" docs are in the index"
argument_list|,
name|numLines
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
name|Files
operator|.
name|delete
argument_list|(
name|lineFile
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test ReadTokensTask    */
DECL|method|testReadTokens
specifier|public
name|void
name|testReadTokens
parameter_list|()
throws|throws
name|Exception
block|{
comment|// We will call ReadTokens on this many docs
specifier|final
name|int
name|NUM_DOCS
init|=
literal|20
decl_stmt|;
comment|// Read tokens from first NUM_DOCS docs from Reuters and
comment|// then build index from the same docs
name|String
name|algLines1
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer"
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"# ----- alg "
block|,
literal|"{ReadTokens}: "
operator|+
name|NUM_DOCS
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"{AddDoc}: "
operator|+
name|NUM_DOCS
block|,
literal|"CloseIndex"
block|,     }
decl_stmt|;
comment|// Run algo
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines1
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|TaskStats
argument_list|>
name|stats
init|=
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getPoints
argument_list|()
operator|.
name|taskStats
argument_list|()
decl_stmt|;
comment|// Count how many tokens all ReadTokens saw
name|int
name|totalTokenCount1
init|=
literal|0
decl_stmt|;
for|for
control|(
specifier|final
name|TaskStats
name|stat
range|:
name|stats
control|)
block|{
if|if
condition|(
name|stat
operator|.
name|getTask
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
literal|"ReadTokens"
argument_list|)
condition|)
block|{
name|totalTokenCount1
operator|+=
name|stat
operator|.
name|getCount
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Separately count how many tokens are actually in the index:
name|IndexReader
name|reader
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
name|NUM_DOCS
argument_list|,
name|reader
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|int
name|totalTokenCount2
init|=
literal|0
decl_stmt|;
name|Fields
name|fields
init|=
name|MultiFields
operator|.
name|getFields
argument_list|(
name|reader
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|fieldName
range|:
name|fields
control|)
block|{
if|if
condition|(
name|fieldName
operator|.
name|equals
argument_list|(
name|DocMaker
operator|.
name|ID_FIELD
argument_list|)
operator|||
name|fieldName
operator|.
name|equals
argument_list|(
name|DocMaker
operator|.
name|DATE_MSEC_FIELD
argument_list|)
operator|||
name|fieldName
operator|.
name|equals
argument_list|(
name|DocMaker
operator|.
name|TIME_SEC_FIELD
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Terms
name|terms
init|=
name|fields
operator|.
name|terms
argument_list|(
name|fieldName
argument_list|)
decl_stmt|;
if|if
condition|(
name|terms
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|TermsEnum
name|termsEnum
init|=
name|terms
operator|.
name|iterator
argument_list|(
literal|null
argument_list|)
decl_stmt|;
name|PostingsEnum
name|docs
init|=
literal|null
decl_stmt|;
while|while
condition|(
name|termsEnum
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|docs
operator|=
name|TestUtil
operator|.
name|docs
argument_list|(
name|random
argument_list|()
argument_list|,
name|termsEnum
argument_list|,
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|reader
argument_list|)
argument_list|,
name|docs
argument_list|,
name|PostingsEnum
operator|.
name|FREQS
argument_list|)
expr_stmt|;
while|while
condition|(
name|docs
operator|.
name|nextDoc
argument_list|()
operator|!=
name|DocIdSetIterator
operator|.
name|NO_MORE_DOCS
condition|)
block|{
name|totalTokenCount2
operator|+=
name|docs
operator|.
name|freq
argument_list|()
expr_stmt|;
block|}
block|}
block|}
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Make sure they are the same
name|assertEquals
argument_list|(
name|totalTokenCount1
argument_list|,
name|totalTokenCount2
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test that " {[AddDoc(4000)]: 4} : * " works corrcetly (for LUCENE-941)    */
DECL|method|testParallelExhausted
specifier|public
name|void
name|testParallelExhausted
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"task.max.depth.log=1"
block|,
literal|"# ----- alg "
block|,
literal|"CreateIndex"
block|,
literal|"{ [ AddDoc]: 4} : * "
block|,
literal|"ResetInputs "
block|,
literal|"{ [ AddDoc]: 4} : * "
block|,
literal|"CloseIndex"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|2
operator|*
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test that exhaust in loop works as expected (LUCENE-1115).    */
DECL|method|testExhaustedLooped
specifier|public
name|void
name|testExhaustedLooped
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"task.max.depth.log=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  CloseIndex"
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test that we can close IndexWriter with argument "false".    */
DECL|method|testCloseIndexFalse
specifier|public
name|void
name|testCloseIndexFalse
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"ram.flush.mb=-1"
block|,
literal|"max.buffered=2"
block|,
literal|"content.source.log.step=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  CloseIndex(false)"
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
DECL|class|MyMergeScheduler
specifier|public
specifier|static
class|class
name|MyMergeScheduler
extends|extends
name|SerialMergeScheduler
block|{
DECL|field|called
name|boolean
name|called
decl_stmt|;
DECL|method|MyMergeScheduler
specifier|public
name|MyMergeScheduler
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
name|called
operator|=
literal|true
expr_stmt|;
block|}
block|}
comment|/**    * Test that we can set merge scheduler".    */
DECL|method|testMergeScheduler
specifier|public
name|void
name|testMergeScheduler
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"merge.scheduler="
operator|+
name|MyMergeScheduler
operator|.
name|class
operator|.
name|getName
argument_list|()
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
name|assertTrue
argument_list|(
literal|"did not use the specified MergeScheduler"
argument_list|,
operator|(
operator|(
name|MyMergeScheduler
operator|)
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getIndexWriter
argument_list|()
operator|.
name|getConfig
argument_list|()
operator|.
name|getMergeScheduler
argument_list|()
operator|)
operator|.
name|called
argument_list|)
expr_stmt|;
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getIndexWriter
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
DECL|class|MyMergePolicy
specifier|public
specifier|static
class|class
name|MyMergePolicy
extends|extends
name|LogDocMergePolicy
block|{
DECL|field|called
name|boolean
name|called
decl_stmt|;
DECL|method|MyMergePolicy
specifier|public
name|MyMergePolicy
parameter_list|()
block|{
name|called
operator|=
literal|true
expr_stmt|;
block|}
block|}
comment|/**    * Test that we can set merge policy".    */
DECL|method|testMergePolicy
specifier|public
name|void
name|testMergePolicy
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"ram.flush.mb=-1"
block|,
literal|"max.buffered=2"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"merge.policy="
operator|+
name|MyMergePolicy
operator|.
name|class
operator|.
name|getName
argument_list|()
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
name|assertTrue
argument_list|(
literal|"did not use the specified MergePolicy"
argument_list|,
operator|(
operator|(
name|MyMergePolicy
operator|)
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getIndexWriter
argument_list|()
operator|.
name|getConfig
argument_list|()
operator|.
name|getMergePolicy
argument_list|()
operator|)
operator|.
name|called
argument_list|)
expr_stmt|;
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getIndexWriter
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test that IndexWriter settings stick.    */
DECL|method|testIndexWriterSettings
specifier|public
name|void
name|testIndexWriterSettings
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"ram.flush.mb=-1"
block|,
literal|"max.buffered=2"
block|,
literal|"compound=cmpnd:true:false"
block|,
literal|"doc.term.vector=vector:false:true"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"merge.factor=3"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  NewRound"
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
specifier|final
name|IndexWriter
name|writer
init|=
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getIndexWriter
argument_list|()
decl_stmt|;
name|assertEquals
argument_list|(
literal|2
argument_list|,
name|writer
operator|.
name|getConfig
argument_list|()
operator|.
name|getMaxBufferedDocs
argument_list|()
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
name|IndexWriterConfig
operator|.
name|DISABLE_AUTO_FLUSH
argument_list|,
operator|(
name|int
operator|)
name|writer
operator|.
name|getConfig
argument_list|()
operator|.
name|getRAMBufferSizeMB
argument_list|()
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
literal|3
argument_list|,
operator|(
operator|(
name|LogMergePolicy
operator|)
name|writer
operator|.
name|getConfig
argument_list|()
operator|.
name|getMergePolicy
argument_list|()
operator|)
operator|.
name|getMergeFactor
argument_list|()
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
literal|0.0d
argument_list|,
name|writer
operator|.
name|getConfig
argument_list|()
operator|.
name|getMergePolicy
argument_list|()
operator|.
name|getNoCFSRatio
argument_list|()
argument_list|,
literal|0.0
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
name|Directory
name|dir
init|=
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
decl_stmt|;
name|IndexReader
name|reader
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|dir
argument_list|)
decl_stmt|;
name|Fields
name|tfv
init|=
name|reader
operator|.
name|getTermVectors
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|assertNotNull
argument_list|(
name|tfv
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
name|tfv
operator|.
name|size
argument_list|()
operator|>
literal|0
argument_list|)
expr_stmt|;
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test indexing with facets tasks.    */
DECL|method|testIndexingWithFacets
specifier|public
name|void
name|testIndexingWithFacets
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=100"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"merge.factor=3"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"ResetSystemErase"
block|,
literal|"CreateIndex"
block|,
literal|"CreateTaxonomyIndex"
block|,
literal|"{ \"AddDocs\"  AddFacetedDoc> : * "
block|,
literal|"CloseIndex"
block|,
literal|"CloseTaxonomyIndex"
block|,
literal|"OpenTaxonomyReader"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
name|PerfRunData
name|runData
init|=
name|benchmark
operator|.
name|getRunData
argument_list|()
decl_stmt|;
name|assertNull
argument_list|(
literal|"taxo writer was not properly closed"
argument_list|,
name|runData
operator|.
name|getTaxonomyWriter
argument_list|()
argument_list|)
expr_stmt|;
name|TaxonomyReader
name|taxoReader
init|=
name|runData
operator|.
name|getTaxonomyReader
argument_list|()
decl_stmt|;
name|assertNotNull
argument_list|(
literal|"taxo reader was not opened"
argument_list|,
name|taxoReader
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
literal|"nothing was added to the taxnomy (expecting root and at least one addtional category)"
argument_list|,
name|taxoReader
operator|.
name|getSize
argument_list|()
operator|>
literal|1
argument_list|)
expr_stmt|;
name|taxoReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test that we can call forceMerge(maxNumSegments).    */
DECL|method|testForceMerge
specifier|public
name|void
name|testForceMerge
parameter_list|()
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"ram.flush.mb=-1"
block|,
literal|"max.buffered=3"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"merge.policy=org.apache.lucene.index.LogDocMergePolicy"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"debug.level=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  ForceMerge(3)"
block|,
literal|"  CloseIndex()"
block|,
literal|"} : 2"
block|,     }
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test number of docs in the index
name|IndexReader
name|ir
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|ndocsExpected
init|=
literal|20
decl_stmt|;
comment|// first 20 reuters docs.
name|assertEquals
argument_list|(
literal|"wrong number of docs in the index!"
argument_list|,
name|ndocsExpected
argument_list|,
name|ir
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
name|ir
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Make sure we have 3 segments:
name|SegmentInfos
name|infos
init|=
name|SegmentInfos
operator|.
name|readLatestCommit
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getDirectory
argument_list|()
argument_list|)
decl_stmt|;
name|assertEquals
argument_list|(
literal|3
argument_list|,
name|infos
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test disabling task count (LUCENE-1136).    */
DECL|method|testDisableCounting
specifier|public
name|void
name|testDisableCounting
parameter_list|()
throws|throws
name|Exception
block|{
name|doTestDisableCounting
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|doTestDisableCounting
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
DECL|method|doTestDisableCounting
specifier|private
name|void
name|doTestDisableCounting
parameter_list|(
name|boolean
name|disable
parameter_list|)
throws|throws
name|Exception
block|{
comment|// 1. alg definition (required in every "logic" test)
name|String
name|algLines
index|[]
init|=
name|disableCountingLines
argument_list|(
name|disable
argument_list|)
decl_stmt|;
comment|// 2. execute the algorithm  (required in every "logic" test)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|algLines
argument_list|)
decl_stmt|;
comment|// 3. test counters
name|int
name|n
init|=
name|disable
condition|?
literal|0
else|:
literal|1
decl_stmt|;
name|int
name|nChecked
init|=
literal|0
decl_stmt|;
for|for
control|(
specifier|final
name|TaskStats
name|stats
range|:
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getPoints
argument_list|()
operator|.
name|taskStats
argument_list|()
control|)
block|{
name|String
name|taskName
init|=
name|stats
operator|.
name|getTask
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
if|if
condition|(
name|taskName
operator|.
name|equals
argument_list|(
literal|"Rounds"
argument_list|)
condition|)
block|{
name|assertEquals
argument_list|(
literal|"Wrong total count!"
argument_list|,
literal|20
operator|+
literal|2
operator|*
name|n
argument_list|,
name|stats
operator|.
name|getCount
argument_list|()
argument_list|)
expr_stmt|;
name|nChecked
operator|++
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|taskName
operator|.
name|equals
argument_list|(
literal|"CreateIndex"
argument_list|)
condition|)
block|{
name|assertEquals
argument_list|(
literal|"Wrong count for CreateIndex!"
argument_list|,
name|n
argument_list|,
name|stats
operator|.
name|getCount
argument_list|()
argument_list|)
expr_stmt|;
name|nChecked
operator|++
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|taskName
operator|.
name|equals
argument_list|(
literal|"CloseIndex"
argument_list|)
condition|)
block|{
name|assertEquals
argument_list|(
literal|"Wrong count for CloseIndex!"
argument_list|,
name|n
argument_list|,
name|stats
operator|.
name|getCount
argument_list|()
argument_list|)
expr_stmt|;
name|nChecked
operator|++
expr_stmt|;
block|}
block|}
name|assertEquals
argument_list|(
literal|"Missing some tasks to check!"
argument_list|,
literal|3
argument_list|,
name|nChecked
argument_list|)
expr_stmt|;
block|}
DECL|method|disableCountingLines
specifier|private
name|String
index|[]
name|disableCountingLines
parameter_list|(
name|boolean
name|disable
parameter_list|)
block|{
name|String
name|dis
init|=
name|disable
condition|?
literal|"-"
else|:
literal|""
decl_stmt|;
return|return
operator|new
name|String
index|[]
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=30"
block|,
literal|"doc.term.vector=false"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"doc.stored=false"
block|,
literal|"doc.tokenized=false"
block|,
literal|"task.max.depth.log=1"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  "
operator|+
name|dis
operator|+
literal|"CreateIndex"
block|,
comment|// optionally disable counting here
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  "
operator|+
name|dis
operator|+
literal|"  CloseIndex"
block|,
comment|// optionally disable counting here (with extra blanks)
literal|"}"
block|,
literal|"RepSumByName"
block|,     }
return|;
block|}
comment|/**    * Test that we can change the Locale in the runData,    * that it is parsed as we expect.    */
DECL|method|testLocale
specifier|public
name|void
name|testLocale
parameter_list|()
throws|throws
name|Exception
block|{
comment|// empty Locale: clear it (null)
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|getLocaleConfig
argument_list|(
literal|""
argument_list|)
argument_list|)
decl_stmt|;
name|assertNull
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getLocale
argument_list|()
argument_list|)
expr_stmt|;
comment|// ROOT locale
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getLocaleConfig
argument_list|(
literal|"ROOT"
argument_list|)
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
operator|new
name|Locale
argument_list|(
literal|""
argument_list|)
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getLocale
argument_list|()
argument_list|)
expr_stmt|;
comment|// specify just a language
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getLocaleConfig
argument_list|(
literal|"de"
argument_list|)
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"de"
argument_list|)
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getLocale
argument_list|()
argument_list|)
expr_stmt|;
comment|// specify language + country
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getLocaleConfig
argument_list|(
literal|"en,US"
argument_list|)
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"en"
argument_list|,
literal|"US"
argument_list|)
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getLocale
argument_list|()
argument_list|)
expr_stmt|;
comment|// specify language + country + variant
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getLocaleConfig
argument_list|(
literal|"no,NO,NY"
argument_list|)
argument_list|)
expr_stmt|;
name|assertEquals
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"no"
argument_list|,
literal|"NO"
argument_list|,
literal|"NY"
argument_list|)
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getLocale
argument_list|()
argument_list|)
expr_stmt|;
block|}
DECL|method|getLocaleConfig
specifier|private
name|String
index|[]
name|getLocaleConfig
parameter_list|(
name|String
name|localeParam
parameter_list|)
block|{
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  NewLocale("
operator|+
name|localeParam
operator|+
literal|")"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  NewRound"
block|,
literal|"} : 1"
block|,     }
decl_stmt|;
return|return
name|algLines
return|;
block|}
comment|/**    * Test that we can create CollationAnalyzers.    */
DECL|method|testCollator
specifier|public
name|void
name|testCollator
parameter_list|()
throws|throws
name|Exception
block|{
comment|// ROOT locale
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|getCollatorConfig
argument_list|(
literal|"ROOT"
argument_list|,
literal|"impl:jdk"
argument_list|)
argument_list|)
decl_stmt|;
name|CollationKeyAnalyzer
name|expected
init|=
operator|new
name|CollationKeyAnalyzer
argument_list|(
name|Collator
operator|.
name|getInstance
argument_list|(
operator|new
name|Locale
argument_list|(
literal|""
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
name|assertEqualCollation
argument_list|(
name|expected
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
literal|"foobar"
argument_list|)
expr_stmt|;
comment|// specify just a language
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getCollatorConfig
argument_list|(
literal|"de"
argument_list|,
literal|"impl:jdk"
argument_list|)
argument_list|)
expr_stmt|;
name|expected
operator|=
operator|new
name|CollationKeyAnalyzer
argument_list|(
name|Collator
operator|.
name|getInstance
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"de"
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|assertEqualCollation
argument_list|(
name|expected
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
literal|"foobar"
argument_list|)
expr_stmt|;
comment|// specify language + country
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getCollatorConfig
argument_list|(
literal|"en,US"
argument_list|,
literal|"impl:jdk"
argument_list|)
argument_list|)
expr_stmt|;
name|expected
operator|=
operator|new
name|CollationKeyAnalyzer
argument_list|(
name|Collator
operator|.
name|getInstance
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"en"
argument_list|,
literal|"US"
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|assertEqualCollation
argument_list|(
name|expected
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
literal|"foobar"
argument_list|)
expr_stmt|;
comment|// specify language + country + variant
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getCollatorConfig
argument_list|(
literal|"no,NO,NY"
argument_list|,
literal|"impl:jdk"
argument_list|)
argument_list|)
expr_stmt|;
name|expected
operator|=
operator|new
name|CollationKeyAnalyzer
argument_list|(
name|Collator
operator|.
name|getInstance
argument_list|(
operator|new
name|Locale
argument_list|(
literal|"no"
argument_list|,
literal|"NO"
argument_list|,
literal|"NY"
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|assertEqualCollation
argument_list|(
name|expected
argument_list|,
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
literal|"foobar"
argument_list|)
expr_stmt|;
block|}
DECL|method|assertEqualCollation
specifier|private
name|void
name|assertEqualCollation
parameter_list|(
name|Analyzer
name|a1
parameter_list|,
name|Analyzer
name|a2
parameter_list|,
name|String
name|text
parameter_list|)
throws|throws
name|Exception
block|{
name|TokenStream
name|ts1
init|=
name|a1
operator|.
name|tokenStream
argument_list|(
literal|"bogus"
argument_list|,
name|text
argument_list|)
decl_stmt|;
name|TokenStream
name|ts2
init|=
name|a2
operator|.
name|tokenStream
argument_list|(
literal|"bogus"
argument_list|,
name|text
argument_list|)
decl_stmt|;
name|ts1
operator|.
name|reset
argument_list|()
expr_stmt|;
name|ts2
operator|.
name|reset
argument_list|()
expr_stmt|;
name|TermToBytesRefAttribute
name|termAtt1
init|=
name|ts1
operator|.
name|addAttribute
argument_list|(
name|TermToBytesRefAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
name|TermToBytesRefAttribute
name|termAtt2
init|=
name|ts2
operator|.
name|addAttribute
argument_list|(
name|TermToBytesRefAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
name|assertTrue
argument_list|(
name|ts1
operator|.
name|incrementToken
argument_list|()
argument_list|)
expr_stmt|;
name|assertTrue
argument_list|(
name|ts2
operator|.
name|incrementToken
argument_list|()
argument_list|)
expr_stmt|;
name|BytesRef
name|bytes1
init|=
name|termAtt1
operator|.
name|getBytesRef
argument_list|()
decl_stmt|;
name|BytesRef
name|bytes2
init|=
name|termAtt2
operator|.
name|getBytesRef
argument_list|()
decl_stmt|;
name|termAtt1
operator|.
name|fillBytesRef
argument_list|()
expr_stmt|;
name|termAtt2
operator|.
name|fillBytesRef
argument_list|()
expr_stmt|;
name|assertEquals
argument_list|(
name|bytes1
argument_list|,
name|bytes2
argument_list|)
expr_stmt|;
name|assertFalse
argument_list|(
name|ts1
operator|.
name|incrementToken
argument_list|()
argument_list|)
expr_stmt|;
name|assertFalse
argument_list|(
name|ts2
operator|.
name|incrementToken
argument_list|()
argument_list|)
expr_stmt|;
name|ts1
operator|.
name|close
argument_list|()
expr_stmt|;
name|ts2
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
DECL|method|getCollatorConfig
specifier|private
name|String
index|[]
name|getCollatorConfig
parameter_list|(
name|String
name|localeParam
parameter_list|,
name|String
name|collationParam
parameter_list|)
block|{
name|String
name|algLines
index|[]
init|=
block|{
literal|"# ----- properties "
block|,
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"content.source.log.step=3"
block|,
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"# ----- alg "
block|,
literal|"{ \"Rounds\""
block|,
literal|"  ResetSystemErase"
block|,
literal|"  NewLocale("
operator|+
name|localeParam
operator|+
literal|")"
block|,
literal|"  NewCollationAnalyzer("
operator|+
name|collationParam
operator|+
literal|")"
block|,
literal|"  CreateIndex"
block|,
literal|"  { \"AddDocs\"  AddDoc> : * "
block|,
literal|"  NewRound"
block|,
literal|"} : 1"
block|,     }
decl_stmt|;
return|return
name|algLines
return|;
block|}
comment|/**    * Test that we can create shingle analyzers using AnalyzerFactory.    */
DECL|method|testShingleAnalyzer
specifier|public
name|void
name|testShingleAnalyzer
parameter_list|()
throws|throws
name|Exception
block|{
name|String
name|text
init|=
literal|"one,two,three, four five six"
decl_stmt|;
comment|// StandardTokenizer, maxShingleSize, and outputUnigrams
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|getAnalyzerFactoryConfig
argument_list|(
literal|"shingle-analyzer"
argument_list|,
literal|"StandardTokenizer,ShingleFilter"
argument_list|)
argument_list|)
decl_stmt|;
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
operator|.
name|tokenStream
argument_list|(
literal|"bogus"
argument_list|,
name|text
argument_list|)
operator|.
name|close
argument_list|()
expr_stmt|;
name|BaseTokenStreamTestCase
operator|.
name|assertAnalyzesTo
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
name|text
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"one"
block|,
literal|"one two"
block|,
literal|"two"
block|,
literal|"two three"
block|,
literal|"three"
block|,
literal|"three four"
block|,
literal|"four"
block|,
literal|"four five"
block|,
literal|"five"
block|,
literal|"five six"
block|,
literal|"six"
block|}
argument_list|)
expr_stmt|;
comment|// StandardTokenizer, maxShingleSize = 3, and outputUnigrams = false
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getAnalyzerFactoryConfig
argument_list|(
literal|"shingle-analyzer"
argument_list|,
literal|"StandardTokenizer,ShingleFilter(maxShingleSize:3,outputUnigrams:false)"
argument_list|)
argument_list|)
expr_stmt|;
name|BaseTokenStreamTestCase
operator|.
name|assertAnalyzesTo
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
name|text
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"one two"
block|,
literal|"one two three"
block|,
literal|"two three"
block|,
literal|"two three four"
block|,
literal|"three four"
block|,
literal|"three four five"
block|,
literal|"four five"
block|,
literal|"four five six"
block|,
literal|"five six"
block|}
argument_list|)
expr_stmt|;
comment|// WhitespaceTokenizer, default maxShingleSize and outputUnigrams
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getAnalyzerFactoryConfig
argument_list|(
literal|"shingle-analyzer"
argument_list|,
literal|"WhitespaceTokenizer,ShingleFilter"
argument_list|)
argument_list|)
expr_stmt|;
name|BaseTokenStreamTestCase
operator|.
name|assertAnalyzesTo
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
name|text
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"one,two,three,"
block|,
literal|"one,two,three, four"
block|,
literal|"four"
block|,
literal|"four five"
block|,
literal|"five"
block|,
literal|"five six"
block|,
literal|"six"
block|}
argument_list|)
expr_stmt|;
comment|// WhitespaceTokenizer, maxShingleSize=3 and outputUnigrams=false
name|benchmark
operator|=
name|execBenchmark
argument_list|(
name|getAnalyzerFactoryConfig
argument_list|(
literal|"shingle-factory"
argument_list|,
literal|"WhitespaceTokenizer,ShingleFilter(outputUnigrams:false,maxShingleSize:3)"
argument_list|)
argument_list|)
expr_stmt|;
name|BaseTokenStreamTestCase
operator|.
name|assertAnalyzesTo
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
name|text
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"one,two,three, four"
block|,
literal|"one,two,three, four five"
block|,
literal|"four five"
block|,
literal|"four five six"
block|,
literal|"five six"
block|}
argument_list|)
expr_stmt|;
block|}
DECL|method|getAnalyzerFactoryConfig
specifier|private
name|String
index|[]
name|getAnalyzerFactoryConfig
parameter_list|(
name|String
name|name
parameter_list|,
name|String
name|params
parameter_list|)
block|{
specifier|final
name|String
name|singleQuoteEscapedName
init|=
name|name
operator|.
name|replaceAll
argument_list|(
literal|"'"
argument_list|,
literal|"\\\\'"
argument_list|)
decl_stmt|;
name|String
name|algLines
index|[]
init|=
block|{
literal|"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"
block|,
literal|"docs.file="
operator|+
name|getReuters20LinesFile
argument_list|()
block|,
literal|"work.dir="
operator|+
name|getWorkDir
argument_list|()
operator|.
name|toAbsolutePath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|replaceAll
argument_list|(
literal|"\\\\"
argument_list|,
literal|"/"
argument_list|)
block|,
comment|// Fix Windows path
literal|"content.source.forever=false"
block|,
literal|"directory=RAMDirectory"
block|,
literal|"AnalyzerFactory(name:'"
operator|+
name|singleQuoteEscapedName
operator|+
literal|"', "
operator|+
name|params
operator|+
literal|")"
block|,
literal|"NewAnalyzer('"
operator|+
name|singleQuoteEscapedName
operator|+
literal|"')"
block|,
literal|"CreateIndex"
block|,
literal|"{ \"AddDocs\"  AddDoc> : * "
block|}
decl_stmt|;
return|return
name|algLines
return|;
block|}
DECL|method|testAnalyzerFactory
specifier|public
name|void
name|testAnalyzerFactory
parameter_list|()
throws|throws
name|Exception
block|{
name|String
name|text
init|=
literal|"Fortieth, QuarantiÃ¨me, CuadragÃ©simo"
decl_stmt|;
name|Benchmark
name|benchmark
init|=
name|execBenchmark
argument_list|(
name|getAnalyzerFactoryConfig
argument_list|(
literal|"ascii folded, pattern replaced, standard tokenized, downcased, bigrammed.'analyzer'"
argument_list|,
literal|"positionIncrementGap:100,offsetGap:1111,"
operator|+
literal|"MappingCharFilter(mapping:'test-mapping-ISOLatin1Accent-partial.txt'),"
operator|+
literal|"PatternReplaceCharFilterFactory(pattern:'e(\\\\\\\\S*)m',replacement:\"$1xxx$1\"),"
operator|+
literal|"StandardTokenizer,LowerCaseFilter,NGramTokenFilter(minGramSize:2,maxGramSize:2)"
argument_list|)
argument_list|)
decl_stmt|;
name|BaseTokenStreamTestCase
operator|.
name|assertAnalyzesTo
argument_list|(
name|benchmark
operator|.
name|getRunData
argument_list|()
operator|.
name|getAnalyzer
argument_list|()
argument_list|,
name|text
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"fo"
block|,
literal|"or"
block|,
literal|"rt"
block|,
literal|"ti"
block|,
literal|"ie"
block|,
literal|"et"
block|,
literal|"th"
block|,
literal|"qu"
block|,
literal|"ua"
block|,
literal|"ar"
block|,
literal|"ra"
block|,
literal|"an"
block|,
literal|"nt"
block|,
literal|"ti"
block|,
literal|"ix"
block|,
literal|"xx"
block|,
literal|"xx"
block|,
literal|"xe"
block|,
literal|"cu"
block|,
literal|"ua"
block|,
literal|"ad"
block|,
literal|"dr"
block|,
literal|"ra"
block|,
literal|"ag"
block|,
literal|"gs"
block|,
literal|"si"
block|,
literal|"ix"
block|,
literal|"xx"
block|,
literal|"xx"
block|,
literal|"xs"
block|,
literal|"si"
block|,
literal|"io"
block|}
argument_list|)
expr_stmt|;
block|}
DECL|method|getReuters20LinesFile
specifier|private
name|String
name|getReuters20LinesFile
parameter_list|()
block|{
return|return
name|getWorkDirResourcePath
argument_list|(
literal|"reuters.first20.lines.txt"
argument_list|)
return|;
block|}
block|}
end_class
end_unit
