begin_unit
begin_package
DECL|package|org.apache.lucene.facet.taxonomy.directory
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|directory
package|;
end_package
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedOutputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Files
import|;
end_import
begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Path
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|CharTermAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|PositionIncrementAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|Document
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|Field
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|FieldType
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|StringField
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|TextField
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|FacetsConfig
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|FacetLabel
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|TaxonomyReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|TaxonomyWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|Cl2oTaxonomyWriterCache
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|LruTaxonomyWriterCache
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|TaxonomyWriterCache
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|CorruptIndexException
import|;
end_import
begin_comment
comment|// javadocs
end_comment
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|DirectoryReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|PostingsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
operator|.
name|OpenMode
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LeafReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LeafReaderContext
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LogByteSizeMergePolicy
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|ReaderManager
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|SegmentInfos
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Terms
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|TermsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|TieredMergePolicy
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|AlreadyClosedException
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|Directory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|LockObtainFailedException
import|;
end_import
begin_comment
comment|// javadocs
end_comment
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_comment
comment|/**  * {@link TaxonomyWriter} which uses a {@link Directory} to store the taxonomy  * information on disk, and keeps an additional in-memory cache of some or all  * categories.  *<p>  * In addition to the permanently-stored information in the {@link Directory},  * efficiency dictates that we also keep an in-memory cache of<B>recently  * seen</B> or<B>all</B> categories, so that we do not need to go back to disk  * for every category addition to see which ordinal this category already has,  * if any. A {@link TaxonomyWriterCache} object determines the specific caching  * algorithm used.  *<p>  * This class offers some hooks for extending classes to control the  * {@link IndexWriter} instance that is used. See {@link #openIndexWriter}.  *   * @lucene.experimental  */
end_comment
begin_class
DECL|class|DirectoryTaxonomyWriter
specifier|public
class|class
name|DirectoryTaxonomyWriter
implements|implements
name|TaxonomyWriter
block|{
comment|/**    * Property name of user commit data that contains the index epoch. The epoch    * changes whenever the taxonomy is recreated (i.e. opened with    * {@link OpenMode#CREATE}.    *<p>    * Applications should not use this property in their commit data because it    * will be overridden by this taxonomy writer.    */
DECL|field|INDEX_EPOCH
specifier|public
specifier|static
specifier|final
name|String
name|INDEX_EPOCH
init|=
literal|"index.epoch"
decl_stmt|;
DECL|field|dir
specifier|private
specifier|final
name|Directory
name|dir
decl_stmt|;
DECL|field|indexWriter
specifier|private
specifier|final
name|IndexWriter
name|indexWriter
decl_stmt|;
DECL|field|cache
specifier|private
specifier|final
name|TaxonomyWriterCache
name|cache
decl_stmt|;
DECL|field|cacheMisses
specifier|private
specifier|final
name|AtomicInteger
name|cacheMisses
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|// Records the taxonomy index epoch, updated on replaceTaxonomy as well.
DECL|field|indexEpoch
specifier|private
name|long
name|indexEpoch
decl_stmt|;
DECL|field|parentStream
specifier|private
name|SinglePositionTokenStream
name|parentStream
init|=
operator|new
name|SinglePositionTokenStream
argument_list|(
name|Consts
operator|.
name|PAYLOAD_PARENT
argument_list|)
decl_stmt|;
DECL|field|parentStreamField
specifier|private
name|Field
name|parentStreamField
decl_stmt|;
DECL|field|fullPathField
specifier|private
name|Field
name|fullPathField
decl_stmt|;
DECL|field|cacheMissesUntilFill
specifier|private
name|int
name|cacheMissesUntilFill
init|=
literal|11
decl_stmt|;
DECL|field|shouldFillCache
specifier|private
name|boolean
name|shouldFillCache
init|=
literal|true
decl_stmt|;
comment|// even though lazily initialized, not volatile so that access to it is
comment|// faster. we keep a volatile boolean init instead.
DECL|field|readerManager
specifier|private
name|ReaderManager
name|readerManager
decl_stmt|;
DECL|field|initializedReaderManager
specifier|private
specifier|volatile
name|boolean
name|initializedReaderManager
init|=
literal|false
decl_stmt|;
DECL|field|shouldRefreshReaderManager
specifier|private
specifier|volatile
name|boolean
name|shouldRefreshReaderManager
decl_stmt|;
comment|/**    * We call the cache "complete" if we know that every category in our    * taxonomy is in the cache. When the cache is<B>not</B> complete, and    * we can't find a category in the cache, we still need to look for it    * in the on-disk index; Therefore when the cache is not complete, we    * need to open a "reader" to the taxonomy index.    * The cache becomes incomplete if it was never filled with the existing    * categories, or if a put() to the cache ever returned true (meaning    * that some of the cached data was cleared).    */
DECL|field|cacheIsComplete
specifier|private
specifier|volatile
name|boolean
name|cacheIsComplete
decl_stmt|;
DECL|field|isClosed
specifier|private
specifier|volatile
name|boolean
name|isClosed
init|=
literal|false
decl_stmt|;
DECL|field|taxoArrays
specifier|private
specifier|volatile
name|TaxonomyIndexArrays
name|taxoArrays
decl_stmt|;
DECL|field|nextID
specifier|private
specifier|volatile
name|int
name|nextID
decl_stmt|;
comment|/** Reads the commit data from a Directory. */
DECL|method|readCommitData
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|readCommitData
parameter_list|(
name|Directory
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|SegmentInfos
name|infos
init|=
name|SegmentInfos
operator|.
name|readLatestCommit
argument_list|(
name|dir
argument_list|)
decl_stmt|;
return|return
name|infos
operator|.
name|getUserData
argument_list|()
return|;
block|}
comment|/**    * Construct a Taxonomy writer.    *     * @param directory    *    The {@link Directory} in which to store the taxonomy. Note that    *    the taxonomy is written directly to that directory (not to a    *    subdirectory of it).    * @param openMode    *    Specifies how to open a taxonomy for writing:<code>APPEND</code>    *    means open an existing index for append (failing if the index does    *    not yet exist).<code>CREATE</code> means create a new index (first    *    deleting the old one if it already existed).    *<code>APPEND_OR_CREATE</code> appends to an existing index if there    *    is one, otherwise it creates a new index.    * @param cache    *    A {@link TaxonomyWriterCache} implementation which determines    *    the in-memory caching policy. See for example    *    {@link LruTaxonomyWriterCache} and {@link Cl2oTaxonomyWriterCache}.    *    If null or missing, {@link #defaultTaxonomyWriterCache()} is used.    * @throws CorruptIndexException    *     if the taxonomy is corrupted.    * @throws LockObtainFailedException    *     if the taxonomy is locked by another writer.    * @throws IOException    *     if another error occurred.    */
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|OpenMode
name|openMode
parameter_list|,
name|TaxonomyWriterCache
name|cache
parameter_list|)
throws|throws
name|IOException
block|{
name|dir
operator|=
name|directory
expr_stmt|;
name|IndexWriterConfig
name|config
init|=
name|createIndexWriterConfig
argument_list|(
name|openMode
argument_list|)
decl_stmt|;
name|indexWriter
operator|=
name|openIndexWriter
argument_list|(
name|dir
argument_list|,
name|config
argument_list|)
expr_stmt|;
comment|// verify (to some extent) that merge policy in effect would preserve category docids
assert|assert
operator|!
operator|(
name|indexWriter
operator|.
name|getConfig
argument_list|()
operator|.
name|getMergePolicy
argument_list|()
operator|instanceof
name|TieredMergePolicy
operator|)
operator|:
literal|"for preserving category docids, merging none-adjacent segments is not allowed"
assert|;
comment|// after we opened the writer, and the index is locked, it's safe to check
comment|// the commit data and read the index epoch
name|openMode
operator|=
name|config
operator|.
name|getOpenMode
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|DirectoryReader
operator|.
name|indexExists
argument_list|(
name|directory
argument_list|)
condition|)
block|{
name|indexEpoch
operator|=
literal|1
expr_stmt|;
block|}
else|else
block|{
name|String
name|epochStr
init|=
literal|null
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|commitData
init|=
name|readCommitData
argument_list|(
name|directory
argument_list|)
decl_stmt|;
if|if
condition|(
name|commitData
operator|!=
literal|null
condition|)
block|{
name|epochStr
operator|=
name|commitData
operator|.
name|get
argument_list|(
name|INDEX_EPOCH
argument_list|)
expr_stmt|;
block|}
comment|// no commit data, or no epoch in it means an old taxonomy, so set its epoch to 1, for lack
comment|// of a better value.
name|indexEpoch
operator|=
name|epochStr
operator|==
literal|null
condition|?
literal|1
else|:
name|Long
operator|.
name|parseLong
argument_list|(
name|epochStr
argument_list|,
literal|16
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|openMode
operator|==
name|OpenMode
operator|.
name|CREATE
condition|)
block|{
operator|++
name|indexEpoch
expr_stmt|;
block|}
name|FieldType
name|ft
init|=
operator|new
name|FieldType
argument_list|(
name|TextField
operator|.
name|TYPE_NOT_STORED
argument_list|)
decl_stmt|;
name|ft
operator|.
name|setOmitNorms
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|parentStreamField
operator|=
operator|new
name|Field
argument_list|(
name|Consts
operator|.
name|FIELD_PAYLOADS
argument_list|,
name|parentStream
argument_list|,
name|ft
argument_list|)
expr_stmt|;
name|fullPathField
operator|=
operator|new
name|StringField
argument_list|(
name|Consts
operator|.
name|FULL
argument_list|,
literal|""
argument_list|,
name|Field
operator|.
name|Store
operator|.
name|YES
argument_list|)
expr_stmt|;
name|nextID
operator|=
name|indexWriter
operator|.
name|maxDoc
argument_list|()
expr_stmt|;
if|if
condition|(
name|cache
operator|==
literal|null
condition|)
block|{
name|cache
operator|=
name|defaultTaxonomyWriterCache
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|cache
operator|=
name|cache
expr_stmt|;
if|if
condition|(
name|nextID
operator|==
literal|0
condition|)
block|{
name|cacheIsComplete
operator|=
literal|true
expr_stmt|;
comment|// Make sure that the taxonomy always contain the root category
comment|// with category id 0.
name|addCategory
argument_list|(
operator|new
name|FacetLabel
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// There are some categories on the disk, which we have not yet
comment|// read into the cache, and therefore the cache is incomplete.
comment|// We choose not to read all the categories into the cache now,
comment|// to avoid terrible performance when a taxonomy index is opened
comment|// to add just a single category. We will do it later, after we
comment|// notice a few cache misses.
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
block|}
block|}
comment|/**    * Open internal index writer, which contains the taxonomy data.    *<p>    * Extensions may provide their own {@link IndexWriter} implementation or instance.     *<br><b>NOTE:</b> the instance this method returns will be closed upon calling    * to {@link #close()}.    *<br><b>NOTE:</b> the merge policy in effect must not merge none adjacent segments. See    * comment in {@link #createIndexWriterConfig(IndexWriterConfig.OpenMode)} for the logic behind this.    *      * @see #createIndexWriterConfig(IndexWriterConfig.OpenMode)    *     * @param directory    *          the {@link Directory} on top of which an {@link IndexWriter}    *          should be opened.    * @param config    *          configuration for the internal index writer.    */
DECL|method|openIndexWriter
specifier|protected
name|IndexWriter
name|openIndexWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|IndexWriterConfig
name|config
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|IndexWriter
argument_list|(
name|directory
argument_list|,
name|config
argument_list|)
return|;
block|}
comment|/**    * Create the {@link IndexWriterConfig} that would be used for opening the internal index writer.    *<br>Extensions can configure the {@link IndexWriter} as they see fit,    * including setting a {@link org.apache.lucene.index.MergeScheduler merge-scheduler}, or    * {@link org.apache.lucene.index.IndexDeletionPolicy deletion-policy}, different RAM size    * etc.<br>    *<br><b>NOTE:</b> internal docids of the configured index must not be altered.    * For that, categories are never deleted from the taxonomy index.    * In addition, merge policy in effect must not merge none adjacent segments.    *     * @see #openIndexWriter(Directory, IndexWriterConfig)    *     * @param openMode see {@link OpenMode}    */
DECL|method|createIndexWriterConfig
specifier|protected
name|IndexWriterConfig
name|createIndexWriterConfig
parameter_list|(
name|OpenMode
name|openMode
parameter_list|)
block|{
comment|// TODO: should we use a more optimized Codec?
comment|// The taxonomy has a unique structure, where each term is associated with one document
comment|// Make sure we use a MergePolicy which always merges adjacent segments and thus
comment|// keeps the doc IDs ordered as well (this is crucial for the taxonomy index).
return|return
operator|new
name|IndexWriterConfig
argument_list|(
literal|null
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|openMode
argument_list|)
operator|.
name|setMergePolicy
argument_list|(
operator|new
name|LogByteSizeMergePolicy
argument_list|()
argument_list|)
return|;
block|}
comment|/** Opens a {@link ReaderManager} from the internal {@link IndexWriter}. */
DECL|method|initReaderManager
specifier|private
name|void
name|initReaderManager
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|initializedReaderManager
condition|)
block|{
synchronized|synchronized
init|(
name|this
init|)
block|{
comment|// verify that the taxo-writer hasn't been closed on us.
name|ensureOpen
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|initializedReaderManager
condition|)
block|{
name|readerManager
operator|=
operator|new
name|ReaderManager
argument_list|(
name|indexWriter
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|shouldRefreshReaderManager
operator|=
literal|false
expr_stmt|;
name|initializedReaderManager
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/**    * Creates a new instance with a default cache as defined by    * {@link #defaultTaxonomyWriterCache()}.    */
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|OpenMode
name|openMode
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|directory
argument_list|,
name|openMode
argument_list|,
name|defaultTaxonomyWriterCache
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Defines the default {@link TaxonomyWriterCache} to use in constructors    * which do not specify one.    *<P>      * The current default is {@link Cl2oTaxonomyWriterCache} constructed    * with the parameters (1024, 0.15f, 3), i.e., the entire taxonomy is    * cached in memory while building it.    */
DECL|method|defaultTaxonomyWriterCache
specifier|public
specifier|static
name|TaxonomyWriterCache
name|defaultTaxonomyWriterCache
parameter_list|()
block|{
return|return
operator|new
name|Cl2oTaxonomyWriterCache
argument_list|(
literal|1024
argument_list|,
literal|0.15f
argument_list|,
literal|3
argument_list|)
return|;
block|}
comment|/** Create this with {@code OpenMode.CREATE_OR_APPEND}. */
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|d
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|d
argument_list|,
name|OpenMode
operator|.
name|CREATE_OR_APPEND
argument_list|)
expr_stmt|;
block|}
comment|/**    * Frees used resources as well as closes the underlying {@link IndexWriter},    * which commits whatever changes made to it to the underlying    * {@link Directory}.    */
annotation|@
name|Override
DECL|method|close
specifier|public
specifier|synchronized
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|isClosed
condition|)
block|{
name|commit
argument_list|()
expr_stmt|;
name|indexWriter
operator|.
name|close
argument_list|()
expr_stmt|;
name|doClose
argument_list|()
expr_stmt|;
block|}
block|}
DECL|method|doClose
specifier|private
name|void
name|doClose
parameter_list|()
throws|throws
name|IOException
block|{
name|isClosed
operator|=
literal|true
expr_stmt|;
name|closeResources
argument_list|()
expr_stmt|;
block|}
comment|/**    * A hook for extending classes to close additional resources that were used.    * The default implementation closes the {@link IndexReader} as well as the    * {@link TaxonomyWriterCache} instances that were used.<br>    *<b>NOTE:</b> if you override this method, you should include a    *<code>super.closeResources()</code> call in your implementation.    */
DECL|method|closeResources
specifier|protected
specifier|synchronized
name|void
name|closeResources
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|initializedReaderManager
condition|)
block|{
name|readerManager
operator|.
name|close
argument_list|()
expr_stmt|;
name|readerManager
operator|=
literal|null
expr_stmt|;
name|initializedReaderManager
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
name|cache
operator|!=
literal|null
condition|)
block|{
name|cache
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Look up the given category in the cache and/or the on-disk storage,    * returning the category's ordinal, or a negative number in case the    * category does not yet exist in the taxonomy.    */
DECL|method|findCategory
specifier|protected
specifier|synchronized
name|int
name|findCategory
parameter_list|(
name|FacetLabel
name|categoryPath
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If we can find the category in the cache, or we know the cache is
comment|// complete, we can return the response directly from it
name|int
name|res
init|=
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|>=
literal|0
operator|||
name|cacheIsComplete
condition|)
block|{
return|return
name|res
return|;
block|}
name|cacheMisses
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
comment|// After a few cache misses, it makes sense to read all the categories
comment|// from disk and into the cache. The reason not to do this on the first
comment|// cache miss (or even when opening the writer) is that it will
comment|// significantly slow down the case when a taxonomy is opened just to
comment|// add one category. The idea only spending a long time on reading
comment|// after enough time was spent on cache misses is known as an "online
comment|// algorithm".
name|perhapsFillCache
argument_list|()
expr_stmt|;
name|res
operator|=
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|res
operator|>=
literal|0
operator|||
name|cacheIsComplete
condition|)
block|{
comment|// if after filling the cache from the info on disk, the category is in it
comment|// or the cache is complete, return whatever cache.get returned.
return|return
name|res
return|;
block|}
comment|// if we get here, it means the category is not in the cache, and it is not
comment|// complete, and therefore we must look for the category on disk.
comment|// We need to get an answer from the on-disk index.
name|initReaderManager
argument_list|()
expr_stmt|;
name|int
name|doc
init|=
operator|-
literal|1
decl_stmt|;
name|DirectoryReader
name|reader
init|=
name|readerManager
operator|.
name|acquire
argument_list|()
decl_stmt|;
try|try
block|{
specifier|final
name|BytesRef
name|catTerm
init|=
operator|new
name|BytesRef
argument_list|(
name|FacetsConfig
operator|.
name|pathToString
argument_list|(
name|categoryPath
operator|.
name|components
argument_list|,
name|categoryPath
operator|.
name|length
argument_list|)
argument_list|)
decl_stmt|;
name|PostingsEnum
name|docs
init|=
literal|null
decl_stmt|;
comment|// reuse
for|for
control|(
name|LeafReaderContext
name|ctx
range|:
name|reader
operator|.
name|leaves
argument_list|()
control|)
block|{
name|Terms
name|terms
init|=
name|ctx
operator|.
name|reader
argument_list|()
operator|.
name|terms
argument_list|(
name|Consts
operator|.
name|FULL
argument_list|)
decl_stmt|;
if|if
condition|(
name|terms
operator|!=
literal|null
condition|)
block|{
comment|// TODO: share per-segment TermsEnum here!
name|TermsEnum
name|termsEnum
init|=
name|terms
operator|.
name|iterator
argument_list|()
decl_stmt|;
if|if
condition|(
name|termsEnum
operator|.
name|seekExact
argument_list|(
name|catTerm
argument_list|)
condition|)
block|{
comment|// liveDocs=null because the taxonomy has no deletes
name|docs
operator|=
name|termsEnum
operator|.
name|postings
argument_list|(
literal|null
argument_list|,
name|docs
argument_list|,
literal|0
comment|/* freqs not required */
argument_list|)
expr_stmt|;
comment|// if the term was found, we know it has exactly one document.
name|doc
operator|=
name|docs
operator|.
name|nextDoc
argument_list|()
operator|+
name|ctx
operator|.
name|docBase
expr_stmt|;
break|break;
block|}
block|}
block|}
block|}
finally|finally
block|{
name|readerManager
operator|.
name|release
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|doc
operator|>
literal|0
condition|)
block|{
name|addToCache
argument_list|(
name|categoryPath
argument_list|,
name|doc
argument_list|)
expr_stmt|;
block|}
return|return
name|doc
return|;
block|}
annotation|@
name|Override
DECL|method|addCategory
specifier|public
name|int
name|addCategory
parameter_list|(
name|FacetLabel
name|categoryPath
parameter_list|)
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
comment|// check the cache outside the synchronized block. this results in better
comment|// concurrency when categories are there.
name|int
name|res
init|=
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|<
literal|0
condition|)
block|{
comment|// the category is not in the cache - following code cannot be executed in parallel.
synchronized|synchronized
init|(
name|this
init|)
block|{
name|res
operator|=
name|findCategory
argument_list|(
name|categoryPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|res
operator|<
literal|0
condition|)
block|{
comment|// This is a new category, and we need to insert it into the index
comment|// (and the cache). Actually, we might also need to add some of
comment|// the category's ancestors before we can add the category itself
comment|// (while keeping the invariant that a parent is always added to
comment|// the taxonomy before its child). internalAddCategory() does all
comment|// this recursively
name|res
operator|=
name|internalAddCategory
argument_list|(
name|categoryPath
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|res
return|;
block|}
comment|/**    * Add a new category into the index (and the cache), and return its new    * ordinal.    *<p>    * Actually, we might also need to add some of the category's ancestors    * before we can add the category itself (while keeping the invariant that a    * parent is always added to the taxonomy before its child). We do this by    * recursion.    */
DECL|method|internalAddCategory
specifier|private
name|int
name|internalAddCategory
parameter_list|(
name|FacetLabel
name|cp
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Find our parent's ordinal (recursively adding the parent category
comment|// to the taxonomy if it's not already there). Then add the parent
comment|// ordinal as payloads (rather than a stored field; payloads can be
comment|// more efficiently read into memory in bulk by LuceneTaxonomyReader)
name|int
name|parent
decl_stmt|;
if|if
condition|(
name|cp
operator|.
name|length
operator|>
literal|1
condition|)
block|{
name|FacetLabel
name|parentPath
init|=
name|cp
operator|.
name|subpath
argument_list|(
name|cp
operator|.
name|length
operator|-
literal|1
argument_list|)
decl_stmt|;
name|parent
operator|=
name|findCategory
argument_list|(
name|parentPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|parent
operator|<
literal|0
condition|)
block|{
name|parent
operator|=
name|internalAddCategory
argument_list|(
name|parentPath
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|cp
operator|.
name|length
operator|==
literal|1
condition|)
block|{
name|parent
operator|=
name|TaxonomyReader
operator|.
name|ROOT_ORDINAL
expr_stmt|;
block|}
else|else
block|{
name|parent
operator|=
name|TaxonomyReader
operator|.
name|INVALID_ORDINAL
expr_stmt|;
block|}
name|int
name|id
init|=
name|addCategoryDocument
argument_list|(
name|cp
argument_list|,
name|parent
argument_list|)
decl_stmt|;
return|return
name|id
return|;
block|}
comment|/**    * Verifies that this instance wasn't closed, or throws    * {@link AlreadyClosedException} if it is.    */
DECL|method|ensureOpen
specifier|protected
specifier|final
name|void
name|ensureOpen
parameter_list|()
block|{
if|if
condition|(
name|isClosed
condition|)
block|{
throw|throw
operator|new
name|AlreadyClosedException
argument_list|(
literal|"The taxonomy writer has already been closed"
argument_list|)
throw|;
block|}
block|}
comment|/**    * Note that the methods calling addCategoryDocument() are synchornized, so    * this method is effectively synchronized as well.    */
DECL|method|addCategoryDocument
specifier|private
name|int
name|addCategoryDocument
parameter_list|(
name|FacetLabel
name|categoryPath
parameter_list|,
name|int
name|parent
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Before Lucene 2.9, position increments>=0 were supported, so we
comment|// added 1 to parent to allow the parent -1 (the parent of the root).
comment|// Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
comment|// no longer enough, since 0 is not encoded consistently either (see
comment|// comment in SinglePositionTokenStream). But because we must be
comment|// backward-compatible with existing indexes, we can't just fix what
comment|// we write here (e.g., to write parent+2), and need to do a workaround
comment|// in the reader (which knows that anyway only category 0 has a parent
comment|// -1).
name|parentStream
operator|.
name|set
argument_list|(
name|Math
operator|.
name|max
argument_list|(
name|parent
operator|+
literal|1
argument_list|,
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|Document
name|d
init|=
operator|new
name|Document
argument_list|()
decl_stmt|;
name|d
operator|.
name|add
argument_list|(
name|parentStreamField
argument_list|)
expr_stmt|;
name|fullPathField
operator|.
name|setStringValue
argument_list|(
name|FacetsConfig
operator|.
name|pathToString
argument_list|(
name|categoryPath
operator|.
name|components
argument_list|,
name|categoryPath
operator|.
name|length
argument_list|)
argument_list|)
expr_stmt|;
name|d
operator|.
name|add
argument_list|(
name|fullPathField
argument_list|)
expr_stmt|;
comment|// Note that we do no pass an Analyzer here because the fields that are
comment|// added to the Document are untokenized or contains their own TokenStream.
comment|// Therefore the IndexWriter's Analyzer has no effect.
name|indexWriter
operator|.
name|addDocument
argument_list|(
name|d
argument_list|)
expr_stmt|;
name|int
name|id
init|=
name|nextID
operator|++
decl_stmt|;
comment|// added a category document, mark that ReaderManager is not up-to-date
name|shouldRefreshReaderManager
operator|=
literal|true
expr_stmt|;
comment|// also add to the parent array
name|taxoArrays
operator|=
name|getTaxoArrays
argument_list|()
operator|.
name|add
argument_list|(
name|id
argument_list|,
name|parent
argument_list|)
expr_stmt|;
comment|// NOTE: this line must be executed last, or else the cache gets updated
comment|// before the parents array (LUCENE-4596)
name|addToCache
argument_list|(
name|categoryPath
argument_list|,
name|id
argument_list|)
expr_stmt|;
return|return
name|id
return|;
block|}
DECL|class|SinglePositionTokenStream
specifier|private
specifier|static
class|class
name|SinglePositionTokenStream
extends|extends
name|TokenStream
block|{
DECL|field|termAtt
specifier|private
name|CharTermAttribute
name|termAtt
decl_stmt|;
DECL|field|posIncrAtt
specifier|private
name|PositionIncrementAttribute
name|posIncrAtt
decl_stmt|;
DECL|field|returned
specifier|private
name|boolean
name|returned
decl_stmt|;
DECL|field|val
specifier|private
name|int
name|val
decl_stmt|;
DECL|field|word
specifier|private
specifier|final
name|String
name|word
decl_stmt|;
DECL|method|SinglePositionTokenStream
specifier|public
name|SinglePositionTokenStream
parameter_list|(
name|String
name|word
parameter_list|)
block|{
name|termAtt
operator|=
name|addAttribute
argument_list|(
name|CharTermAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|posIncrAtt
operator|=
name|addAttribute
argument_list|(
name|PositionIncrementAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|this
operator|.
name|word
operator|=
name|word
expr_stmt|;
name|returned
operator|=
literal|true
expr_stmt|;
block|}
comment|/**      * Set the value we want to keep, as the position increment.      * Note that when TermPositions.nextPosition() is later used to      * retrieve this value, val-1 will be returned, not val.      *<P>      * IMPORTANT NOTE: Before Lucene 2.9, val&gt;=0 were safe (for val==0,      * the retrieved position would be -1). But starting with Lucene 2.9,      * this unfortunately changed, and only val&gt;0 are safe. val=0 can      * still be used, but don't count on the value you retrieve later      * (it could be 0 or -1, depending on circumstances or versions).      * This change is described in Lucene's JIRA: LUCENE-1542.       */
DECL|method|set
specifier|public
name|void
name|set
parameter_list|(
name|int
name|val
parameter_list|)
block|{
name|this
operator|.
name|val
operator|=
name|val
expr_stmt|;
name|returned
operator|=
literal|false
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|incrementToken
specifier|public
name|boolean
name|incrementToken
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|returned
condition|)
block|{
return|return
literal|false
return|;
block|}
name|clearAttributes
argument_list|()
expr_stmt|;
name|posIncrAtt
operator|.
name|setPositionIncrement
argument_list|(
name|val
argument_list|)
expr_stmt|;
name|termAtt
operator|.
name|setEmpty
argument_list|()
expr_stmt|;
name|termAtt
operator|.
name|append
argument_list|(
name|word
argument_list|)
expr_stmt|;
name|returned
operator|=
literal|true
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
DECL|method|addToCache
specifier|private
name|void
name|addToCache
parameter_list|(
name|FacetLabel
name|categoryPath
parameter_list|,
name|int
name|id
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|cache
operator|.
name|put
argument_list|(
name|categoryPath
argument_list|,
name|id
argument_list|)
condition|)
block|{
comment|// If cache.put() returned true, it means the cache was limited in
comment|// size, became full, and parts of it had to be evicted. It is
comment|// possible that a relatively-new category that isn't yet visible
comment|// to our 'reader' was evicted, and therefore we must now refresh
comment|// the reader.
name|refreshReaderManager
argument_list|()
expr_stmt|;
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
block|}
block|}
DECL|method|refreshReaderManager
specifier|private
specifier|synchronized
name|void
name|refreshReaderManager
parameter_list|()
throws|throws
name|IOException
block|{
comment|// this method is synchronized since it cannot happen concurrently with
comment|// addCategoryDocument -- when this method returns, we must know that the
comment|// reader manager's state is current. also, it sets shouldRefresh to false,
comment|// and this cannot overlap with addCatDoc too.
comment|// NOTE: since this method is sync'ed, it can call maybeRefresh, instead of
comment|// maybeRefreshBlocking. If ever this is changed, make sure to change the
comment|// call too.
if|if
condition|(
name|shouldRefreshReaderManager
operator|&&
name|initializedReaderManager
condition|)
block|{
name|readerManager
operator|.
name|maybeRefresh
argument_list|()
expr_stmt|;
name|shouldRefreshReaderManager
operator|=
literal|false
expr_stmt|;
block|}
block|}
annotation|@
name|Override
DECL|method|commit
specifier|public
specifier|synchronized
name|void
name|commit
parameter_list|()
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
comment|// LUCENE-4972: if we always call setCommitData, we create empty commits
name|String
name|epochStr
init|=
name|indexWriter
operator|.
name|getCommitData
argument_list|()
operator|.
name|get
argument_list|(
name|INDEX_EPOCH
argument_list|)
decl_stmt|;
if|if
condition|(
name|epochStr
operator|==
literal|null
operator|||
name|Long
operator|.
name|parseLong
argument_list|(
name|epochStr
argument_list|,
literal|16
argument_list|)
operator|!=
name|indexEpoch
condition|)
block|{
name|indexWriter
operator|.
name|setCommitData
argument_list|(
name|combinedCommitData
argument_list|(
name|indexWriter
operator|.
name|getCommitData
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|indexWriter
operator|.
name|commit
argument_list|()
expr_stmt|;
block|}
comment|/** Combine original user data with the taxonomy epoch. */
DECL|method|combinedCommitData
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|combinedCommitData
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|commitData
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|m
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|commitData
operator|!=
literal|null
condition|)
block|{
name|m
operator|.
name|putAll
argument_list|(
name|commitData
argument_list|)
expr_stmt|;
block|}
name|m
operator|.
name|put
argument_list|(
name|INDEX_EPOCH
argument_list|,
name|Long
operator|.
name|toString
argument_list|(
name|indexEpoch
argument_list|,
literal|16
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|m
return|;
block|}
annotation|@
name|Override
DECL|method|setCommitData
specifier|public
name|void
name|setCommitData
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|commitUserData
parameter_list|)
block|{
name|indexWriter
operator|.
name|setCommitData
argument_list|(
name|combinedCommitData
argument_list|(
name|commitUserData
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|getCommitData
specifier|public
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|getCommitData
parameter_list|()
block|{
return|return
name|combinedCommitData
argument_list|(
name|indexWriter
operator|.
name|getCommitData
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * prepare most of the work needed for a two-phase commit.    * See {@link IndexWriter#prepareCommit}.    */
annotation|@
name|Override
DECL|method|prepareCommit
specifier|public
specifier|synchronized
name|void
name|prepareCommit
parameter_list|()
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
comment|// LUCENE-4972: if we always call setCommitData, we create empty commits
name|String
name|epochStr
init|=
name|indexWriter
operator|.
name|getCommitData
argument_list|()
operator|.
name|get
argument_list|(
name|INDEX_EPOCH
argument_list|)
decl_stmt|;
if|if
condition|(
name|epochStr
operator|==
literal|null
operator|||
name|Long
operator|.
name|parseLong
argument_list|(
name|epochStr
argument_list|,
literal|16
argument_list|)
operator|!=
name|indexEpoch
condition|)
block|{
name|indexWriter
operator|.
name|setCommitData
argument_list|(
name|combinedCommitData
argument_list|(
name|indexWriter
operator|.
name|getCommitData
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|indexWriter
operator|.
name|prepareCommit
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|getSize
specifier|public
name|int
name|getSize
parameter_list|()
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
return|return
name|nextID
return|;
block|}
comment|/**    * Set the number of cache misses before an attempt is made to read the entire    * taxonomy into the in-memory cache.    *<p>    * This taxonomy writer holds an in-memory cache of recently seen categories    * to speed up operation. On each cache-miss, the on-disk index needs to be    * consulted. When an existing taxonomy is opened, a lot of slow disk reads    * like that are needed until the cache is filled, so it is more efficient to    * read the entire taxonomy into memory at once. We do this complete read    * after a certain number (defined by this method) of cache misses.    *<p>    * If the number is set to {@code 0}, the entire taxonomy is read into the    * cache on first use, without fetching individual categories first.    *<p>    * NOTE: it is assumed that this method is called immediately after the    * taxonomy writer has been created.    */
DECL|method|setCacheMissesUntilFill
specifier|public
name|void
name|setCacheMissesUntilFill
parameter_list|(
name|int
name|i
parameter_list|)
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
name|cacheMissesUntilFill
operator|=
name|i
expr_stmt|;
block|}
comment|// we need to guarantee that if several threads call this concurrently, only
comment|// one executes it, and after it returns, the cache is updated and is either
comment|// complete or not.
DECL|method|perhapsFillCache
specifier|private
specifier|synchronized
name|void
name|perhapsFillCache
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|cacheMisses
operator|.
name|get
argument_list|()
operator|<
name|cacheMissesUntilFill
condition|)
block|{
return|return;
block|}
if|if
condition|(
operator|!
name|shouldFillCache
condition|)
block|{
comment|// we already filled the cache once, there's no need to re-fill it
return|return;
block|}
name|shouldFillCache
operator|=
literal|false
expr_stmt|;
name|initReaderManager
argument_list|()
expr_stmt|;
name|boolean
name|aborted
init|=
literal|false
decl_stmt|;
name|DirectoryReader
name|reader
init|=
name|readerManager
operator|.
name|acquire
argument_list|()
decl_stmt|;
try|try
block|{
name|PostingsEnum
name|postingsEnum
init|=
literal|null
decl_stmt|;
for|for
control|(
name|LeafReaderContext
name|ctx
range|:
name|reader
operator|.
name|leaves
argument_list|()
control|)
block|{
name|Terms
name|terms
init|=
name|ctx
operator|.
name|reader
argument_list|()
operator|.
name|terms
argument_list|(
name|Consts
operator|.
name|FULL
argument_list|)
decl_stmt|;
if|if
condition|(
name|terms
operator|!=
literal|null
condition|)
block|{
comment|// cannot really happen, but be on the safe side
comment|// TODO: share per-segment TermsEnum here!
name|TermsEnum
name|termsEnum
init|=
name|terms
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|termsEnum
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|cache
operator|.
name|isFull
argument_list|()
condition|)
block|{
name|BytesRef
name|t
init|=
name|termsEnum
operator|.
name|term
argument_list|()
decl_stmt|;
comment|// Since we guarantee uniqueness of categories, each term has exactly
comment|// one document. Also, since we do not allow removing categories (and
comment|// hence documents), there are no deletions in the index. Therefore, it
comment|// is sufficient to call next(), and then doc(), exactly once with no
comment|// 'validation' checks.
name|FacetLabel
name|cp
init|=
operator|new
name|FacetLabel
argument_list|(
name|FacetsConfig
operator|.
name|stringToPath
argument_list|(
name|t
operator|.
name|utf8ToString
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|postingsEnum
operator|=
name|termsEnum
operator|.
name|postings
argument_list|(
literal|null
argument_list|,
name|postingsEnum
argument_list|,
name|PostingsEnum
operator|.
name|NONE
argument_list|)
expr_stmt|;
name|boolean
name|res
init|=
name|cache
operator|.
name|put
argument_list|(
name|cp
argument_list|,
name|postingsEnum
operator|.
name|nextDoc
argument_list|()
operator|+
name|ctx
operator|.
name|docBase
argument_list|)
decl_stmt|;
assert|assert
operator|!
name|res
operator|:
literal|"entries should not have been evicted from the cache"
assert|;
block|}
else|else
block|{
comment|// the cache is full and the next put() will evict entries from it, therefore abort the iteration.
name|aborted
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
block|}
if|if
condition|(
name|aborted
condition|)
block|{
break|break;
block|}
block|}
block|}
finally|finally
block|{
name|readerManager
operator|.
name|release
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
name|cacheIsComplete
operator|=
operator|!
name|aborted
expr_stmt|;
if|if
condition|(
name|cacheIsComplete
condition|)
block|{
synchronized|synchronized
init|(
name|this
init|)
block|{
comment|// everything is in the cache, so no need to keep readerManager open.
comment|// this block is executed in a sync block so that it works well with
comment|// initReaderManager called in parallel.
name|readerManager
operator|.
name|close
argument_list|()
expr_stmt|;
name|readerManager
operator|=
literal|null
expr_stmt|;
name|initializedReaderManager
operator|=
literal|false
expr_stmt|;
block|}
block|}
block|}
DECL|method|getTaxoArrays
specifier|private
name|TaxonomyIndexArrays
name|getTaxoArrays
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|taxoArrays
operator|==
literal|null
condition|)
block|{
synchronized|synchronized
init|(
name|this
init|)
block|{
if|if
condition|(
name|taxoArrays
operator|==
literal|null
condition|)
block|{
name|initReaderManager
argument_list|()
expr_stmt|;
name|DirectoryReader
name|reader
init|=
name|readerManager
operator|.
name|acquire
argument_list|()
decl_stmt|;
try|try
block|{
comment|// according to Java Concurrency, this might perform better on some
comment|// JVMs, since the object initialization doesn't happen on the
comment|// volatile member.
name|TaxonomyIndexArrays
name|tmpArrays
init|=
operator|new
name|TaxonomyIndexArrays
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|taxoArrays
operator|=
name|tmpArrays
expr_stmt|;
block|}
finally|finally
block|{
name|readerManager
operator|.
name|release
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
name|taxoArrays
return|;
block|}
annotation|@
name|Override
DECL|method|getParent
specifier|public
name|int
name|getParent
parameter_list|(
name|int
name|ordinal
parameter_list|)
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
comment|// Note: the following if() just enforces that a user can never ask
comment|// for the parent of a nonexistant category - even if the parent array
comment|// was allocated bigger than it really needs to be.
if|if
condition|(
name|ordinal
operator|>=
name|nextID
condition|)
block|{
throw|throw
operator|new
name|ArrayIndexOutOfBoundsException
argument_list|(
literal|"requested ordinal is bigger than the largest ordinal in the taxonomy"
argument_list|)
throw|;
block|}
name|int
index|[]
name|parents
init|=
name|getTaxoArrays
argument_list|()
operator|.
name|parents
argument_list|()
decl_stmt|;
assert|assert
name|ordinal
operator|<
name|parents
operator|.
name|length
operator|:
literal|"requested ordinal ("
operator|+
name|ordinal
operator|+
literal|"); parents.length ("
operator|+
name|parents
operator|.
name|length
operator|+
literal|") !"
assert|;
return|return
name|parents
index|[
name|ordinal
index|]
return|;
block|}
comment|/**    * Takes the categories from the given taxonomy directory, and adds the    * missing ones to this taxonomy. Additionally, it fills the given    * {@link OrdinalMap} with a mapping from the original ordinal to the new    * ordinal.    */
DECL|method|addTaxonomy
specifier|public
name|void
name|addTaxonomy
parameter_list|(
name|Directory
name|taxoDir
parameter_list|,
name|OrdinalMap
name|map
parameter_list|)
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
name|DirectoryReader
name|r
init|=
name|DirectoryReader
operator|.
name|open
argument_list|(
name|taxoDir
argument_list|)
decl_stmt|;
try|try
block|{
specifier|final
name|int
name|size
init|=
name|r
operator|.
name|numDocs
argument_list|()
decl_stmt|;
specifier|final
name|OrdinalMap
name|ordinalMap
init|=
name|map
decl_stmt|;
name|ordinalMap
operator|.
name|setSize
argument_list|(
name|size
argument_list|)
expr_stmt|;
name|int
name|base
init|=
literal|0
decl_stmt|;
name|PostingsEnum
name|docs
init|=
literal|null
decl_stmt|;
for|for
control|(
specifier|final
name|LeafReaderContext
name|ctx
range|:
name|r
operator|.
name|leaves
argument_list|()
control|)
block|{
specifier|final
name|LeafReader
name|ar
init|=
name|ctx
operator|.
name|reader
argument_list|()
decl_stmt|;
specifier|final
name|Terms
name|terms
init|=
name|ar
operator|.
name|terms
argument_list|(
name|Consts
operator|.
name|FULL
argument_list|)
decl_stmt|;
comment|// TODO: share per-segment TermsEnum here!
name|TermsEnum
name|te
init|=
name|terms
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|te
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|FacetLabel
name|cp
init|=
operator|new
name|FacetLabel
argument_list|(
name|FacetsConfig
operator|.
name|stringToPath
argument_list|(
name|te
operator|.
name|term
argument_list|()
operator|.
name|utf8ToString
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
specifier|final
name|int
name|ordinal
init|=
name|addCategory
argument_list|(
name|cp
argument_list|)
decl_stmt|;
name|docs
operator|=
name|te
operator|.
name|postings
argument_list|(
literal|null
argument_list|,
name|docs
argument_list|,
name|PostingsEnum
operator|.
name|NONE
argument_list|)
expr_stmt|;
name|ordinalMap
operator|.
name|addMapping
argument_list|(
name|docs
operator|.
name|nextDoc
argument_list|()
operator|+
name|base
argument_list|,
name|ordinal
argument_list|)
expr_stmt|;
block|}
name|base
operator|+=
name|ar
operator|.
name|maxDoc
argument_list|()
expr_stmt|;
comment|// no deletions, so we're ok
block|}
name|ordinalMap
operator|.
name|addDone
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|r
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Mapping from old ordinal to new ordinals, used when merging indexes     * wit separate taxonomies.    *<p>     * addToTaxonomies() merges one or more taxonomies into the given taxonomy    * (this). An OrdinalMap is filled for each of the added taxonomies,    * containing the new ordinal (in the merged taxonomy) of each of the    * categories in the old taxonomy.    *<P>      * There exist two implementations of OrdinalMap: MemoryOrdinalMap and    * DiskOrdinalMap. As their names suggest, the former keeps the map in    * memory and the latter in a temporary disk file. Because these maps will    * later be needed one by one (to remap the counting lists), not all at the    * same time, it is recommended to put the first taxonomy's map in memory,    * and all the rest on disk (later to be automatically read into memory one    * by one, when needed).    */
DECL|interface|OrdinalMap
specifier|public
specifier|static
interface|interface
name|OrdinalMap
block|{
comment|/**      * Set the size of the map. This MUST be called before addMapping().      * It is assumed (but not verified) that addMapping() will then be      * called exactly 'size' times, with different origOrdinals between 0      * and size-1.        */
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|size
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/** Record a mapping. */
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * Call addDone() to say that all addMapping() have been done.      * In some implementations this might free some resources.       */
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * Return the map from the taxonomy's original (consecutive) ordinals      * to the new taxonomy's ordinals. If the map has to be read from disk      * and ordered appropriately, it is done when getMap() is called.      * getMap() should only be called once, and only when the map is actually      * needed. Calling it will also free all resources that the map might      * be holding (such as temporary disk space), other than the returned int[].      */
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * {@link OrdinalMap} maintained in memory    */
DECL|class|MemoryOrdinalMap
specifier|public
specifier|static
specifier|final
class|class
name|MemoryOrdinalMap
implements|implements
name|OrdinalMap
block|{
DECL|field|map
name|int
index|[]
name|map
decl_stmt|;
comment|/** Sole constructor. */
DECL|method|MemoryOrdinalMap
specifier|public
name|MemoryOrdinalMap
parameter_list|()
block|{     }
annotation|@
name|Override
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|taxonomySize
parameter_list|)
block|{
name|map
operator|=
operator|new
name|int
index|[
name|taxonomySize
index|]
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
block|{
name|map
index|[
name|origOrdinal
index|]
operator|=
name|newOrdinal
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
block|{
comment|/* nothing to do */
block|}
annotation|@
name|Override
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
block|{
return|return
name|map
return|;
block|}
block|}
comment|/**    * {@link OrdinalMap} maintained on file system    */
DECL|class|DiskOrdinalMap
specifier|public
specifier|static
specifier|final
class|class
name|DiskOrdinalMap
implements|implements
name|OrdinalMap
block|{
DECL|field|tmpfile
name|Path
name|tmpfile
decl_stmt|;
DECL|field|out
name|DataOutputStream
name|out
decl_stmt|;
comment|/** Sole constructor. */
DECL|method|DiskOrdinalMap
specifier|public
name|DiskOrdinalMap
parameter_list|(
name|Path
name|tmpfile
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|tmpfile
operator|=
name|tmpfile
expr_stmt|;
name|out
operator|=
operator|new
name|DataOutputStream
argument_list|(
operator|new
name|BufferedOutputStream
argument_list|(
name|Files
operator|.
name|newOutputStream
argument_list|(
name|tmpfile
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|origOrdinal
argument_list|)
expr_stmt|;
name|out
operator|.
name|writeInt
argument_list|(
name|newOrdinal
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|taxonomySize
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|taxonomySize
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|out
operator|!=
literal|null
condition|)
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
name|out
operator|=
literal|null
expr_stmt|;
block|}
block|}
DECL|field|map
name|int
index|[]
name|map
init|=
literal|null
decl_stmt|;
annotation|@
name|Override
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|map
operator|!=
literal|null
condition|)
block|{
return|return
name|map
return|;
block|}
name|addDone
argument_list|()
expr_stmt|;
comment|// in case this wasn't previously called
name|DataInputStream
name|in
init|=
operator|new
name|DataInputStream
argument_list|(
operator|new
name|BufferedInputStream
argument_list|(
name|Files
operator|.
name|newInputStream
argument_list|(
name|tmpfile
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
name|map
operator|=
operator|new
name|int
index|[
name|in
operator|.
name|readInt
argument_list|()
index|]
expr_stmt|;
comment|// NOTE: The current code assumes here that the map is complete,
comment|// i.e., every ordinal gets one and exactly one value. Otherwise,
comment|// we may run into an EOF here, or vice versa, not read everything.
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|map
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|int
name|origordinal
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
name|int
name|newordinal
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
name|map
index|[
name|origordinal
index|]
operator|=
name|newordinal
expr_stmt|;
block|}
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Delete the temporary file, which is no longer needed.
name|Files
operator|.
name|delete
argument_list|(
name|tmpfile
argument_list|)
expr_stmt|;
return|return
name|map
return|;
block|}
block|}
comment|/**    * Rollback changes to the taxonomy writer and closes the instance. Following    * this method the instance becomes unusable (calling any of its API methods    * will yield an {@link AlreadyClosedException}).    */
annotation|@
name|Override
DECL|method|rollback
specifier|public
specifier|synchronized
name|void
name|rollback
parameter_list|()
throws|throws
name|IOException
block|{
name|ensureOpen
argument_list|()
expr_stmt|;
name|indexWriter
operator|.
name|rollback
argument_list|()
expr_stmt|;
name|doClose
argument_list|()
expr_stmt|;
block|}
comment|/**    * Replaces the current taxonomy with the given one. This method should    * generally be called in conjunction with    * {@link IndexWriter#addIndexes(Directory...)} to replace both the taxonomy    * as well as the search index content.    */
DECL|method|replaceTaxonomy
specifier|public
specifier|synchronized
name|void
name|replaceTaxonomy
parameter_list|(
name|Directory
name|taxoDir
parameter_list|)
throws|throws
name|IOException
block|{
comment|// replace the taxonomy by doing IW optimized operations
name|indexWriter
operator|.
name|deleteAll
argument_list|()
expr_stmt|;
name|indexWriter
operator|.
name|addIndexes
argument_list|(
name|taxoDir
argument_list|)
expr_stmt|;
name|shouldRefreshReaderManager
operator|=
literal|true
expr_stmt|;
name|initReaderManager
argument_list|()
expr_stmt|;
comment|// ensure that it's initialized
name|refreshReaderManager
argument_list|()
expr_stmt|;
name|nextID
operator|=
name|indexWriter
operator|.
name|maxDoc
argument_list|()
expr_stmt|;
name|taxoArrays
operator|=
literal|null
expr_stmt|;
comment|// must nullify so that it's re-computed next time it's needed
comment|// need to clear the cache, so that addCategory won't accidentally return
comment|// old categories that are in the cache.
name|cache
operator|.
name|clear
argument_list|()
expr_stmt|;
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
name|shouldFillCache
operator|=
literal|true
expr_stmt|;
name|cacheMisses
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
comment|// update indexEpoch as a taxonomy replace is just like it has be recreated
operator|++
name|indexEpoch
expr_stmt|;
block|}
comment|/** Returns the {@link Directory} of this taxonomy writer. */
DECL|method|getDirectory
specifier|public
name|Directory
name|getDirectory
parameter_list|()
block|{
return|return
name|dir
return|;
block|}
comment|/**    * Used by {@link DirectoryTaxonomyReader} to support NRT.    *<p>    *<b>NOTE:</b> you should not use the obtained {@link IndexWriter} in any    * way, other than opening an IndexReader on it, or otherwise, the taxonomy    * index may become corrupt!    */
DECL|method|getInternalIndexWriter
specifier|final
name|IndexWriter
name|getInternalIndexWriter
parameter_list|()
block|{
return|return
name|indexWriter
return|;
block|}
comment|/** Expert: returns current index epoch, if this is a    * near-real-time reader.  Used by {@link    * DirectoryTaxonomyReader} to support NRT.     *    * @lucene.internal */
DECL|method|getTaxonomyEpoch
specifier|public
specifier|final
name|long
name|getTaxonomyEpoch
parameter_list|()
block|{
return|return
name|indexEpoch
return|;
block|}
block|}
end_class
end_unit
