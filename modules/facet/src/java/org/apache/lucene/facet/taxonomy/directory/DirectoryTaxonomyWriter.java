begin_unit
begin_package
DECL|package|org.apache.lucene.facet.taxonomy.directory
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|directory
package|;
end_package
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedOutputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileInputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileOutputStream
import|;
end_import
begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import
begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|core
operator|.
name|KeywordAnalyzer
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|PositionIncrementAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|CharTermAttribute
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|Document
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|Field
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|FieldType
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|StringField
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|document
operator|.
name|TextField
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|CorruptIndexException
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|DocsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|IndexWriterConfig
operator|.
name|OpenMode
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|LogByteSizeMergePolicy
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|MultiFields
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|Terms
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|index
operator|.
name|TermsEnum
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|search
operator|.
name|DocIdSetIterator
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|Directory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|LockObtainFailedException
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|NativeFSLockFactory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|store
operator|.
name|SimpleFSLockFactory
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|Bits
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|Version
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|CategoryPath
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|TaxonomyReader
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|TaxonomyWriter
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|TaxonomyWriterCache
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|cl2o
operator|.
name|Cl2oTaxonomyWriterCache
import|;
end_import
begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|facet
operator|.
name|taxonomy
operator|.
name|writercache
operator|.
name|lru
operator|.
name|LruTaxonomyWriterCache
import|;
end_import
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment
begin_comment
comment|/**  * {@link TaxonomyWriter} which uses a {@link Directory} to store the taxonomy  * information on disk, and keeps an additional in-memory cache of some or all  * categories.  *<p>  * In addition to the permanently-stored information in the {@link Directory},  * efficiency dictates that we also keep an in-memory cache of<B>recently  * seen</B> or<B>all</B> categories, so that we do not need to go back to disk  * for every category addition to see which ordinal this category already has,  * if any. A {@link TaxonomyWriterCache} object determines the specific caching  * algorithm used.  *<p>  * This class offers some hooks for extending classes to control the  * {@link IndexWriter} instance that is used. See {@link #openIndexWriter}.  *   * @lucene.experimental  */
end_comment
begin_class
DECL|class|DirectoryTaxonomyWriter
specifier|public
class|class
name|DirectoryTaxonomyWriter
implements|implements
name|TaxonomyWriter
block|{
DECL|field|indexWriter
specifier|private
name|IndexWriter
name|indexWriter
decl_stmt|;
DECL|field|nextID
specifier|private
name|int
name|nextID
decl_stmt|;
DECL|field|delimiter
specifier|private
name|char
name|delimiter
init|=
name|Consts
operator|.
name|DEFAULT_DELIMITER
decl_stmt|;
DECL|field|parentStream
specifier|private
name|SinglePositionTokenStream
name|parentStream
init|=
operator|new
name|SinglePositionTokenStream
argument_list|(
name|Consts
operator|.
name|PAYLOAD_PARENT
argument_list|)
decl_stmt|;
DECL|field|parentStreamField
specifier|private
name|Field
name|parentStreamField
decl_stmt|;
DECL|field|fullPathField
specifier|private
name|Field
name|fullPathField
decl_stmt|;
DECL|field|cache
specifier|private
name|TaxonomyWriterCache
name|cache
decl_stmt|;
comment|/**    * We call the cache "complete" if we know that every category in our    * taxonomy is in the cache. When the cache is<B>not</B> complete, and    * we can't find a category in the cache, we still need to look for it    * in the on-disk index; Therefore when the cache is not complete, we    * need to open a "reader" to the taxonomy index.    * The cache becomes incomplete if it was never filled with the existing    * categories, or if a put() to the cache ever returned true (meaning    * that some of the cached data was cleared).    */
DECL|field|cacheIsComplete
specifier|private
name|boolean
name|cacheIsComplete
decl_stmt|;
DECL|field|reader
specifier|private
name|IndexReader
name|reader
decl_stmt|;
DECL|field|cacheMisses
specifier|private
name|int
name|cacheMisses
decl_stmt|;
comment|/**    * setDelimiter changes the character that the taxonomy uses in its internal    * storage as a delimiter between category components. Do not use this    * method unless you really know what you are doing. It has nothing to do    * with whatever character the application may be using to represent    * categories for its own use.    *<P>    * If you do use this method, make sure you call it before any other methods    * that actually queries the taxonomy. Moreover, make sure you always pass    * the same delimiter for all LuceneTaxonomyWriter and LuceneTaxonomyReader    * objects you create for the same directory.    */
DECL|method|setDelimiter
specifier|public
name|void
name|setDelimiter
parameter_list|(
name|char
name|delimiter
parameter_list|)
block|{
name|this
operator|.
name|delimiter
operator|=
name|delimiter
expr_stmt|;
block|}
comment|/**    * Forcibly unlocks the taxonomy in the named directory.    *<P>    * Caution: this should only be used by failure recovery code, when it is    * known that no other process nor thread is in fact currently accessing    * this taxonomy.    *<P>    * This method is unnecessary if your {@link Directory} uses a    * {@link NativeFSLockFactory} instead of the default    * {@link SimpleFSLockFactory}. When the "native" lock is used, a lock    * does not stay behind forever when the process using it dies.     */
DECL|method|unlock
specifier|public
specifier|static
name|void
name|unlock
parameter_list|(
name|Directory
name|directory
parameter_list|)
throws|throws
name|IOException
block|{
name|IndexWriter
operator|.
name|unlock
argument_list|(
name|directory
argument_list|)
expr_stmt|;
block|}
comment|/**    * Construct a Taxonomy writer.    *     * @param directory    *    The {@link Directory} in which to store the taxonomy. Note that    *    the taxonomy is written directly to that directory (not to a    *    subdirectory of it).    * @param openMode    *    Specifies how to open a taxonomy for writing:<code>APPEND</code>    *    means open an existing index for append (failing if the index does    *    not yet exist).<code>CREATE</code> means create a new index (first    *    deleting the old one if it already existed).    *<code>APPEND_OR_CREATE</code> appends to an existing index if there    *    is one, otherwise it creates a new index.    * @param cache    *    A {@link TaxonomyWriterCache} implementation which determines    *    the in-memory caching policy. See for example    *    {@link LruTaxonomyWriterCache} and {@link Cl2oTaxonomyWriterCache}.    *    If null or missing, {@link #defaultTaxonomyWriterCache()} is used.    * @throws CorruptIndexException    *     if the taxonomy is corrupted.    * @throws LockObtainFailedException    *     if the taxonomy is locked by another writer. If it is known    *     that no other concurrent writer is active, the lock might    *     have been left around by an old dead process, and should be    *     removed using {@link #unlock(Directory)}.    * @throws IOException    *     if another error occurred.    */
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|OpenMode
name|openMode
parameter_list|,
name|TaxonomyWriterCache
name|cache
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|LockObtainFailedException
throws|,
name|IOException
block|{
name|indexWriter
operator|=
name|openIndexWriter
argument_list|(
name|directory
argument_list|,
name|openMode
argument_list|)
expr_stmt|;
name|reader
operator|=
literal|null
expr_stmt|;
name|FieldType
name|ft
init|=
operator|new
name|FieldType
argument_list|(
name|TextField
operator|.
name|TYPE_UNSTORED
argument_list|)
decl_stmt|;
name|ft
operator|.
name|setOmitNorms
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|parentStreamField
operator|=
operator|new
name|Field
argument_list|(
name|Consts
operator|.
name|FIELD_PAYLOADS
argument_list|,
name|parentStream
argument_list|,
name|ft
argument_list|)
expr_stmt|;
name|fullPathField
operator|=
operator|new
name|Field
argument_list|(
name|Consts
operator|.
name|FULL
argument_list|,
literal|""
argument_list|,
name|StringField
operator|.
name|TYPE_STORED
argument_list|)
expr_stmt|;
name|this
operator|.
name|nextID
operator|=
name|indexWriter
operator|.
name|maxDoc
argument_list|()
expr_stmt|;
if|if
condition|(
name|cache
operator|==
literal|null
condition|)
block|{
name|cache
operator|=
name|defaultTaxonomyWriterCache
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|cache
operator|=
name|cache
expr_stmt|;
if|if
condition|(
name|nextID
operator|==
literal|0
condition|)
block|{
name|cacheIsComplete
operator|=
literal|true
expr_stmt|;
comment|// Make sure that the taxonomy always contain the root category
comment|// with category id 0.
name|addCategory
argument_list|(
operator|new
name|CategoryPath
argument_list|()
argument_list|)
expr_stmt|;
name|refreshReader
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// There are some categories on the disk, which we have not yet
comment|// read into the cache, and therefore the cache is incomplete.
comment|// We chose not to read all the categories into the cache now,
comment|// to avoid terrible performance when a taxonomy index is opened
comment|// to add just a single category. We will do it later, after we
comment|// notice a few cache misses.
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
block|}
name|cacheMisses
operator|=
literal|0
expr_stmt|;
block|}
comment|/**    * A hook for extensions of this class to provide their own    * {@link IndexWriter} implementation or instance. Extending classes can    * instantiate and configure the {@link IndexWriter} as they see fit,    * including setting a {@link org.apache.lucene.index.MergeScheduler}, or    * {@link org.apache.lucene.index.IndexDeletionPolicy}, different RAM size    * etc.<br>    *<b>NOTE:</b> the instance this method returns will be closed upon calling    * to {@link #close()}.    *     * @param directory    *          the {@link Directory} on top of which an {@link IndexWriter}    *          should be opened.    * @param openMode    *          see {@link OpenMode}    */
DECL|method|openIndexWriter
specifier|protected
name|IndexWriter
name|openIndexWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|OpenMode
name|openMode
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Make sure we use a MergePolicy which merges segments in-order and thus
comment|// keeps the doc IDs ordered as well (this is crucial for the taxonomy
comment|// index).
name|IndexWriterConfig
name|config
init|=
operator|new
name|IndexWriterConfig
argument_list|(
name|Version
operator|.
name|LUCENE_40
argument_list|,
operator|new
name|KeywordAnalyzer
argument_list|()
argument_list|)
operator|.
name|setOpenMode
argument_list|(
name|openMode
argument_list|)
operator|.
name|setMergePolicy
argument_list|(
operator|new
name|LogByteSizeMergePolicy
argument_list|()
argument_list|)
decl_stmt|;
return|return
operator|new
name|IndexWriter
argument_list|(
name|directory
argument_list|,
name|config
argument_list|)
return|;
block|}
comment|// Currently overridden by a unit test that verifies that every index we open
comment|// is close()ed.
comment|/**    * Open an {@link IndexReader} from the {@link #indexWriter} member, by    * calling {@link IndexWriter#getReader()}. Extending classes can override    * this method to return their own {@link IndexReader}.    */
DECL|method|openReader
specifier|protected
name|IndexReader
name|openReader
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|IndexReader
operator|.
name|open
argument_list|(
name|indexWriter
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Creates a new instance with a default cached as defined by    * {@link #defaultTaxonomyWriterCache()}.    */
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|directory
parameter_list|,
name|OpenMode
name|openMode
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|LockObtainFailedException
throws|,
name|IOException
block|{
name|this
argument_list|(
name|directory
argument_list|,
name|openMode
argument_list|,
name|defaultTaxonomyWriterCache
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Defines the default {@link TaxonomyWriterCache} to use in constructors    * which do not specify one.    *<P>      * The current default is {@link Cl2oTaxonomyWriterCache} constructed    * with the parameters (1024, 0.15f, 3), i.e., the entire taxonomy is    * cached in memory while building it.    */
DECL|method|defaultTaxonomyWriterCache
specifier|public
specifier|static
name|TaxonomyWriterCache
name|defaultTaxonomyWriterCache
parameter_list|()
block|{
return|return
operator|new
name|Cl2oTaxonomyWriterCache
argument_list|(
literal|1024
argument_list|,
literal|0.15f
argument_list|,
literal|3
argument_list|)
return|;
block|}
comment|// convenience constructors:
DECL|method|DirectoryTaxonomyWriter
specifier|public
name|DirectoryTaxonomyWriter
parameter_list|(
name|Directory
name|d
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|LockObtainFailedException
throws|,
name|IOException
block|{
name|this
argument_list|(
name|d
argument_list|,
name|OpenMode
operator|.
name|CREATE_OR_APPEND
argument_list|)
expr_stmt|;
block|}
comment|/**    * Frees used resources as well as closes the underlying {@link IndexWriter},    * which commits whatever changes made to it to the underlying    * {@link Directory}.    */
annotation|@
name|Override
DECL|method|close
specifier|public
specifier|synchronized
name|void
name|close
parameter_list|()
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
if|if
condition|(
name|indexWriter
operator|!=
literal|null
condition|)
block|{
name|indexWriter
operator|.
name|close
argument_list|()
expr_stmt|;
name|indexWriter
operator|=
literal|null
expr_stmt|;
block|}
name|closeResources
argument_list|()
expr_stmt|;
block|}
comment|/**    * Returns the number of memory bytes used by the cache.    * @return Number of cache bytes in memory, for CL2O only; zero otherwise.    */
DECL|method|getCacheMemoryUsage
specifier|public
name|int
name|getCacheMemoryUsage
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|cache
operator|==
literal|null
operator|||
operator|!
operator|(
name|this
operator|.
name|cache
operator|instanceof
name|Cl2oTaxonomyWriterCache
operator|)
condition|)
block|{
return|return
literal|0
return|;
block|}
return|return
operator|(
operator|(
name|Cl2oTaxonomyWriterCache
operator|)
name|this
operator|.
name|cache
operator|)
operator|.
name|getMemoryUsage
argument_list|()
return|;
block|}
comment|/**    * A hook for extending classes to close additional resources that were used.    * The default implementation closes the {@link IndexReader} as well as the    * {@link TaxonomyWriterCache} instances that were used.<br>    *<b>NOTE:</b> if you override this method, you should include a    *<code>super.closeResources()</code> call in your implementation.    */
DECL|method|closeResources
specifier|protected
specifier|synchronized
name|void
name|closeResources
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
name|reader
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|cache
operator|!=
literal|null
condition|)
block|{
name|cache
operator|.
name|close
argument_list|()
expr_stmt|;
name|cache
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/**    * Look up the given category in the cache and/or the on-disk storage,    * returning the category's ordinal, or a negative number in case the    * category does not yet exist in the taxonomy.    */
DECL|method|findCategory
specifier|protected
name|int
name|findCategory
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If we can find the category in our cache, we can return the
comment|// response directly from it:
name|int
name|res
init|=
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|>=
literal|0
condition|)
block|{
return|return
name|res
return|;
block|}
comment|// If we know that the cache is complete, i.e., contains every category
comment|// which exists, we can return -1 immediately. However, if the cache is
comment|// not complete, we need to check the disk.
if|if
condition|(
name|cacheIsComplete
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|cacheMisses
operator|++
expr_stmt|;
comment|// After a few cache misses, it makes sense to read all the categories
comment|// from disk and into the cache. The reason not to do this on the first
comment|// cache miss (or even when opening the writer) is that it will
comment|// significantly slow down the case when a taxonomy is opened just to
comment|// add one category. The idea only spending a long time on reading
comment|// after enough time was spent on cache misses is known as a "online
comment|// algorithm".
if|if
condition|(
name|perhapsFillCache
argument_list|()
condition|)
block|{
return|return
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|)
return|;
block|}
comment|// We need to get an answer from the on-disk index. If a reader
comment|// is not yet open, do it now:
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|reader
operator|=
name|openReader
argument_list|()
expr_stmt|;
block|}
comment|// TODO (Facet): avoid Multi*?
name|Bits
name|liveDocs
init|=
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|DocsEnum
name|docs
init|=
name|MultiFields
operator|.
name|getTermDocsEnum
argument_list|(
name|reader
argument_list|,
name|liveDocs
argument_list|,
name|Consts
operator|.
name|FULL
argument_list|,
operator|new
name|BytesRef
argument_list|(
name|categoryPath
operator|.
name|toString
argument_list|(
name|delimiter
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|docs
operator|==
literal|null
operator|||
name|docs
operator|.
name|nextDoc
argument_list|()
operator|==
name|DocIdSetIterator
operator|.
name|NO_MORE_DOCS
condition|)
block|{
return|return
operator|-
literal|1
return|;
comment|// category does not exist in taxonomy
block|}
comment|// Note: we do NOT add to the cache the fact that the category
comment|// does not exist. The reason is that our only use for this
comment|// method is just before we actually add this category. If
comment|// in the future this usage changes, we should consider caching
comment|// the fact that the category is not in the taxonomy.
name|addToCache
argument_list|(
name|categoryPath
argument_list|,
name|docs
operator|.
name|docID
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|docs
operator|.
name|docID
argument_list|()
return|;
block|}
comment|/**    * Look up the given prefix of the given category in the cache and/or the    * on-disk storage, returning that prefix's ordinal, or a negative number in    * case the category does not yet exist in the taxonomy.    */
DECL|method|findCategory
specifier|private
name|int
name|findCategory
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|,
name|int
name|prefixLen
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|res
init|=
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|,
name|prefixLen
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|>=
literal|0
condition|)
block|{
return|return
name|res
return|;
block|}
if|if
condition|(
name|cacheIsComplete
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|cacheMisses
operator|++
expr_stmt|;
if|if
condition|(
name|perhapsFillCache
argument_list|()
condition|)
block|{
return|return
name|cache
operator|.
name|get
argument_list|(
name|categoryPath
argument_list|,
name|prefixLen
argument_list|)
return|;
block|}
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|reader
operator|=
name|openReader
argument_list|()
expr_stmt|;
block|}
name|Bits
name|liveDocs
init|=
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|DocsEnum
name|docs
init|=
name|MultiFields
operator|.
name|getTermDocsEnum
argument_list|(
name|reader
argument_list|,
name|liveDocs
argument_list|,
name|Consts
operator|.
name|FULL
argument_list|,
operator|new
name|BytesRef
argument_list|(
name|categoryPath
operator|.
name|toString
argument_list|(
name|delimiter
argument_list|,
name|prefixLen
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|docs
operator|==
literal|null
operator|||
name|docs
operator|.
name|nextDoc
argument_list|()
operator|==
name|DocIdSetIterator
operator|.
name|NO_MORE_DOCS
condition|)
block|{
return|return
operator|-
literal|1
return|;
comment|// category does not exist in taxonomy
block|}
name|addToCache
argument_list|(
name|categoryPath
argument_list|,
name|prefixLen
argument_list|,
name|docs
operator|.
name|docID
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|docs
operator|.
name|docID
argument_list|()
return|;
block|}
comment|// TODO (Facet): addCategory() is synchronized. This means that if indexing is
comment|// multi-threaded, a new category that needs to be written to disk (and
comment|// potentially even trigger a lengthy merge) locks out other addCategory()
comment|// calls - even those which could immediately return a cached value.
comment|// We definitely need to fix this situation!
annotation|@
name|Override
DECL|method|addCategory
specifier|public
specifier|synchronized
name|int
name|addCategory
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If the category is already in the cache and/or the taxonomy, we
comment|// should return its existing ordinal:
name|int
name|res
init|=
name|findCategory
argument_list|(
name|categoryPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|<
literal|0
condition|)
block|{
comment|// This is a new category, and we need to insert it into the index
comment|// (and the cache). Actually, we might also need to add some of
comment|// the category's ancestors before we can add the category itself
comment|// (while keeping the invariant that a parent is always added to
comment|// the taxonomy before its child). internalAddCategory() does all
comment|// this recursively:
name|res
operator|=
name|internalAddCategory
argument_list|(
name|categoryPath
argument_list|,
name|categoryPath
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|res
return|;
block|}
comment|/**    * Add a new category into the index (and the cache), and return its new    * ordinal.    *<P>    * Actually, we might also need to add some of the category's ancestors    * before we can add the category itself (while keeping the invariant that a    * parent is always added to the taxonomy before its child). We do this by    * recursion.    */
DECL|method|internalAddCategory
specifier|private
name|int
name|internalAddCategory
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|,
name|int
name|length
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
comment|// Find our parent's ordinal (recursively adding the parent category
comment|// to the taxonomy if it's not already there). Then add the parent
comment|// ordinal as payloads (rather than a stored field; payloads can be
comment|// more efficiently read into memory in bulk by LuceneTaxonomyReader)
name|int
name|parent
decl_stmt|;
if|if
condition|(
name|length
operator|>
literal|1
condition|)
block|{
name|parent
operator|=
name|findCategory
argument_list|(
name|categoryPath
argument_list|,
name|length
operator|-
literal|1
argument_list|)
expr_stmt|;
if|if
condition|(
name|parent
operator|<
literal|0
condition|)
block|{
name|parent
operator|=
name|internalAddCategory
argument_list|(
name|categoryPath
argument_list|,
name|length
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|length
operator|==
literal|1
condition|)
block|{
name|parent
operator|=
name|TaxonomyReader
operator|.
name|ROOT_ORDINAL
expr_stmt|;
block|}
else|else
block|{
name|parent
operator|=
name|TaxonomyReader
operator|.
name|INVALID_ORDINAL
expr_stmt|;
block|}
name|int
name|id
init|=
name|addCategoryDocument
argument_list|(
name|categoryPath
argument_list|,
name|length
argument_list|,
name|parent
argument_list|)
decl_stmt|;
return|return
name|id
return|;
block|}
comment|// Note that the methods calling addCategoryDocument() are synchornized,
comment|// so this method is effectively synchronized as well, but we'll add
comment|// synchronized to be on the safe side, and we can reuse class-local objects
comment|// instead of allocating them every time
DECL|method|addCategoryDocument
specifier|protected
specifier|synchronized
name|int
name|addCategoryDocument
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|,
name|int
name|length
parameter_list|,
name|int
name|parent
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
comment|// Before Lucene 2.9, position increments>=0 were supported, so we
comment|// added 1 to parent to allow the parent -1 (the parent of the root).
comment|// Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
comment|// no longer enough, since 0 is not encoded consistently either (see
comment|// comment in SinglePositionTokenStream). But because we must be
comment|// backward-compatible with existing indexes, we can't just fix what
comment|// we write here (e.g., to write parent+2), and need to do a workaround
comment|// in the reader (which knows that anyway only category 0 has a parent
comment|// -1).
name|parentStream
operator|.
name|set
argument_list|(
name|parent
operator|+
literal|1
argument_list|)
expr_stmt|;
name|Document
name|d
init|=
operator|new
name|Document
argument_list|()
decl_stmt|;
name|d
operator|.
name|add
argument_list|(
name|parentStreamField
argument_list|)
expr_stmt|;
name|fullPathField
operator|.
name|setValue
argument_list|(
name|categoryPath
operator|.
name|toString
argument_list|(
name|delimiter
argument_list|,
name|length
argument_list|)
argument_list|)
expr_stmt|;
name|d
operator|.
name|add
argument_list|(
name|fullPathField
argument_list|)
expr_stmt|;
comment|// Note that we do no pass an Analyzer here because the fields that are
comment|// added to the Document are untokenized or contains their own TokenStream.
comment|// Therefore the IndexWriter's Analyzer has no effect.
name|indexWriter
operator|.
name|addDocument
argument_list|(
name|d
argument_list|)
expr_stmt|;
name|int
name|id
init|=
name|nextID
operator|++
decl_stmt|;
name|addToCache
argument_list|(
name|categoryPath
argument_list|,
name|length
argument_list|,
name|id
argument_list|)
expr_stmt|;
comment|// also add to the parent array
name|getParentArray
argument_list|()
operator|.
name|add
argument_list|(
name|id
argument_list|,
name|parent
argument_list|)
expr_stmt|;
return|return
name|id
return|;
block|}
DECL|class|SinglePositionTokenStream
specifier|private
specifier|static
class|class
name|SinglePositionTokenStream
extends|extends
name|TokenStream
block|{
DECL|field|termAtt
specifier|private
name|CharTermAttribute
name|termAtt
decl_stmt|;
DECL|field|posIncrAtt
specifier|private
name|PositionIncrementAttribute
name|posIncrAtt
decl_stmt|;
DECL|field|returned
specifier|private
name|boolean
name|returned
decl_stmt|;
DECL|method|SinglePositionTokenStream
specifier|public
name|SinglePositionTokenStream
parameter_list|(
name|String
name|word
parameter_list|)
block|{
name|termAtt
operator|=
name|addAttribute
argument_list|(
name|CharTermAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|posIncrAtt
operator|=
name|addAttribute
argument_list|(
name|PositionIncrementAttribute
operator|.
name|class
argument_list|)
expr_stmt|;
name|termAtt
operator|.
name|setEmpty
argument_list|()
operator|.
name|append
argument_list|(
name|word
argument_list|)
expr_stmt|;
name|returned
operator|=
literal|true
expr_stmt|;
block|}
comment|/**      * Set the value we want to keep, as the position increment.      * Note that when TermPositions.nextPosition() is later used to      * retrieve this value, val-1 will be returned, not val.      *<P>      * IMPORTANT NOTE: Before Lucene 2.9, val>=0 were safe (for val==0,      * the retrieved position would be -1). But starting with Lucene 2.9,      * this unfortunately changed, and only val>0 are safe. val=0 can      * still be used, but don't count on the value you retrieve later      * (it could be 0 or -1, depending on circumstances or versions).      * This change is described in Lucene's JIRA: LUCENE-1542.       */
DECL|method|set
specifier|public
name|void
name|set
parameter_list|(
name|int
name|val
parameter_list|)
block|{
name|posIncrAtt
operator|.
name|setPositionIncrement
argument_list|(
name|val
argument_list|)
expr_stmt|;
name|returned
operator|=
literal|false
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|incrementToken
specifier|public
name|boolean
name|incrementToken
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|returned
condition|)
block|{
return|return
literal|false
return|;
block|}
name|returned
operator|=
literal|true
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
DECL|method|addToCache
specifier|private
name|void
name|addToCache
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|,
name|int
name|id
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
if|if
condition|(
name|cache
operator|.
name|put
argument_list|(
name|categoryPath
argument_list|,
name|id
argument_list|)
condition|)
block|{
comment|// If cache.put() returned true, it means the cache was limited in
comment|// size, became full, so parts of it had to be cleared.
comment|// Unfortunately we don't know which part was cleared - it is
comment|// possible that a relatively-new category that hasn't yet been
comment|// committed to disk (and therefore isn't yet visible in our
comment|// "reader") was deleted from the cache, and therefore we must
comment|// now refresh the reader.
comment|// Because this is a slow operation, cache implementations are
comment|// expected not to delete entries one-by-one but rather in bulk
comment|// (LruTaxonomyWriterCache removes the 2/3rd oldest entries).
name|refreshReader
argument_list|()
expr_stmt|;
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
block|}
block|}
DECL|method|addToCache
specifier|private
name|void
name|addToCache
parameter_list|(
name|CategoryPath
name|categoryPath
parameter_list|,
name|int
name|prefixLen
parameter_list|,
name|int
name|id
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
if|if
condition|(
name|cache
operator|.
name|put
argument_list|(
name|categoryPath
argument_list|,
name|prefixLen
argument_list|,
name|id
argument_list|)
condition|)
block|{
name|refreshReader
argument_list|()
expr_stmt|;
name|cacheIsComplete
operator|=
literal|false
expr_stmt|;
block|}
block|}
DECL|method|refreshReader
specifier|private
specifier|synchronized
name|void
name|refreshReader
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|IndexReader
name|r2
init|=
name|IndexReader
operator|.
name|openIfChanged
argument_list|(
name|reader
argument_list|)
decl_stmt|;
if|if
condition|(
name|r2
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
name|reader
operator|=
name|r2
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Calling commit() ensures that all the categories written so far are    * visible to a reader that is opened (or reopened) after that call.    * When the index is closed(), commit() is also implicitly done.    * See {@link TaxonomyWriter#commit()}    */
annotation|@
name|Override
DECL|method|commit
specifier|public
specifier|synchronized
name|void
name|commit
parameter_list|()
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
name|indexWriter
operator|.
name|commit
argument_list|()
expr_stmt|;
name|refreshReader
argument_list|()
expr_stmt|;
block|}
comment|/**    * Like commit(), but also store properties with the index. These properties    * are retrievable by {@link DirectoryTaxonomyReader#getCommitUserData}.    * See {@link TaxonomyWriter#commit(Map)}.     */
annotation|@
name|Override
DECL|method|commit
specifier|public
specifier|synchronized
name|void
name|commit
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|commitUserData
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
name|indexWriter
operator|.
name|commit
argument_list|(
name|commitUserData
argument_list|)
expr_stmt|;
name|refreshReader
argument_list|()
expr_stmt|;
block|}
comment|/**    * prepare most of the work needed for a two-phase commit.    * See {@link IndexWriter#prepareCommit}.    */
annotation|@
name|Override
DECL|method|prepareCommit
specifier|public
specifier|synchronized
name|void
name|prepareCommit
parameter_list|()
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
name|indexWriter
operator|.
name|prepareCommit
argument_list|()
expr_stmt|;
block|}
comment|/**    * Like above, and also prepares to store user data with the index.    * See {@link IndexWriter#prepareCommit(Map)}    */
annotation|@
name|Override
DECL|method|prepareCommit
specifier|public
specifier|synchronized
name|void
name|prepareCommit
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|commitUserData
parameter_list|)
throws|throws
name|CorruptIndexException
throws|,
name|IOException
block|{
name|indexWriter
operator|.
name|prepareCommit
argument_list|(
name|commitUserData
argument_list|)
expr_stmt|;
block|}
comment|/**    * getSize() returns the number of categories in the taxonomy.    *<P>    * Because categories are numbered consecutively starting with 0, it means    * the taxonomy contains ordinals 0 through getSize()-1.    *<P>    * Note that the number returned by getSize() is often slightly higher than    * the number of categories inserted into the taxonomy; This is because when    * a category is added to the taxonomy, its ancestors are also added    * automatically (including the root, which always get ordinal 0).    */
annotation|@
name|Override
DECL|method|getSize
specifier|synchronized
specifier|public
name|int
name|getSize
parameter_list|()
block|{
return|return
name|indexWriter
operator|.
name|maxDoc
argument_list|()
return|;
block|}
DECL|field|alreadyCalledFillCache
specifier|private
name|boolean
name|alreadyCalledFillCache
init|=
literal|false
decl_stmt|;
comment|/**    * Set the number of cache misses before an attempt is made to read the    * entire taxonomy into the in-memory cache.    *<P>     * LuceneTaxonomyWriter holds an in-memory cache of recently seen    * categories to speed up operation. On each cache-miss, the on-disk index    * needs to be consulted. When an existing taxonomy is opened, a lot of    * slow disk reads like that are needed until the cache is filled, so it    * is more efficient to read the entire taxonomy into memory at once.    * We do this complete read after a certain number (defined by this method)    * of cache misses.    *<P>    * If the number is set to<CODE>0</CODE>, the entire taxonomy is read    * into the cache on first use, without fetching individual categories    * first.    *<P>    * Note that if the memory cache of choice is limited in size, and cannot    * hold the entire content of the on-disk taxonomy, then it is never    * read in its entirety into the cache, regardless of the setting of this    * method.     */
DECL|method|setCacheMissesUntilFill
specifier|public
name|void
name|setCacheMissesUntilFill
parameter_list|(
name|int
name|i
parameter_list|)
block|{
name|cacheMissesUntilFill
operator|=
name|i
expr_stmt|;
block|}
DECL|field|cacheMissesUntilFill
specifier|private
name|int
name|cacheMissesUntilFill
init|=
literal|11
decl_stmt|;
DECL|method|perhapsFillCache
specifier|private
name|boolean
name|perhapsFillCache
parameter_list|()
throws|throws
name|IOException
block|{
comment|// Note: we assume that we're only called when cacheIsComplete==false.
comment|// TODO (Facet): parametrize this criterion:
if|if
condition|(
name|cacheMisses
operator|<
name|cacheMissesUntilFill
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// If the cache was already filled (or we decided not to fill it because
comment|// there was no room), there is no sense in trying it again.
if|if
condition|(
name|alreadyCalledFillCache
condition|)
block|{
return|return
literal|false
return|;
block|}
name|alreadyCalledFillCache
operator|=
literal|true
expr_stmt|;
comment|// TODO (Facet): we should probably completely clear the cache before starting
comment|// to read it?
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|reader
operator|=
name|openReader
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|cache
operator|.
name|hasRoom
argument_list|(
name|reader
operator|.
name|numDocs
argument_list|()
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|CategoryPath
name|cp
init|=
operator|new
name|CategoryPath
argument_list|()
decl_stmt|;
name|Terms
name|terms
init|=
name|MultiFields
operator|.
name|getTerms
argument_list|(
name|reader
argument_list|,
name|Consts
operator|.
name|FULL
argument_list|)
decl_stmt|;
comment|// The check is done here to avoid checking it on every iteration of the
comment|// below loop. A null term wlil be returned if there are no terms in the
comment|// lexicon, or after the Consts.FULL term. However while the loop is
comment|// executed we're safe, because we only iterate as long as there are next()
comment|// terms.
if|if
condition|(
name|terms
operator|!=
literal|null
condition|)
block|{
name|TermsEnum
name|termsEnum
init|=
name|terms
operator|.
name|iterator
argument_list|()
decl_stmt|;
name|Bits
name|liveDocs
init|=
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|DocsEnum
name|docsEnum
init|=
literal|null
decl_stmt|;
while|while
condition|(
name|termsEnum
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|BytesRef
name|t
init|=
name|termsEnum
operator|.
name|term
argument_list|()
decl_stmt|;
comment|// Since we guarantee uniqueness of categories, each term has exactly
comment|// one document. Also, since we do not allow removing categories (and
comment|// hence documents), there are no deletions in the index. Therefore, it
comment|// is sufficient to call next(), and then doc(), exactly once with no
comment|// 'validation' checks.
name|docsEnum
operator|=
name|termsEnum
operator|.
name|docs
argument_list|(
name|liveDocs
argument_list|,
name|docsEnum
argument_list|)
expr_stmt|;
name|docsEnum
operator|.
name|nextDoc
argument_list|()
expr_stmt|;
name|cp
operator|.
name|clear
argument_list|()
expr_stmt|;
comment|// TODO (Facet): avoid String creation/use bytes?
name|cp
operator|.
name|add
argument_list|(
name|t
operator|.
name|utf8ToString
argument_list|()
argument_list|,
name|delimiter
argument_list|)
expr_stmt|;
name|cache
operator|.
name|put
argument_list|(
name|cp
argument_list|,
name|docsEnum
operator|.
name|docID
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|cacheIsComplete
operator|=
literal|true
expr_stmt|;
comment|// No sense to keep the reader open - we will not need to read from it
comment|// if everything is in the cache.
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
name|reader
operator|=
literal|null
expr_stmt|;
return|return
literal|true
return|;
block|}
DECL|field|parentArray
specifier|private
name|ParentArray
name|parentArray
decl_stmt|;
DECL|method|getParentArray
specifier|private
specifier|synchronized
name|ParentArray
name|getParentArray
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|parentArray
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|reader
operator|=
name|openReader
argument_list|()
expr_stmt|;
block|}
name|parentArray
operator|=
operator|new
name|ParentArray
argument_list|()
expr_stmt|;
name|parentArray
operator|.
name|refresh
argument_list|(
name|reader
argument_list|)
expr_stmt|;
block|}
return|return
name|parentArray
return|;
block|}
annotation|@
name|Override
DECL|method|getParent
specifier|public
name|int
name|getParent
parameter_list|(
name|int
name|ordinal
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Note: the following if() just enforces that a user can never ask
comment|// for the parent of a nonexistant category - even if the parent array
comment|// was allocated bigger than it really needs to be.
if|if
condition|(
name|ordinal
operator|>=
name|getSize
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|ArrayIndexOutOfBoundsException
argument_list|()
throw|;
block|}
return|return
name|getParentArray
argument_list|()
operator|.
name|getArray
argument_list|()
index|[
name|ordinal
index|]
return|;
block|}
comment|/**    * Take all the categories of one or more given taxonomies, and add them to    * the main taxonomy (this), if they are not already there.    *<P>    * Additionally, fill a<I>mapping</I> for each of the added taxonomies,    * mapping its ordinals to the ordinals in the enlarged main taxonomy.    * These mapping are saved into an array of OrdinalMap objects given by the    * user, one for each of the given taxonomies (not including "this", the main    * taxonomy). Often the first of these will be a MemoryOrdinalMap and the    * others will be a DiskOrdinalMap - see discussion in {OrdinalMap}.     *<P>     * Note that the taxonomies to be added are given as Directory objects,    * not opened TaxonomyReader/TaxonomyWriter objects, so if any of them are    * currently managed by an open TaxonomyWriter, make sure to commit() (or    * close()) it first. The main taxonomy (this) is an open TaxonomyWriter,    * and does not need to be commit()ed before this call.     */
DECL|method|addTaxonomies
specifier|public
name|void
name|addTaxonomies
parameter_list|(
name|Directory
index|[]
name|taxonomies
parameter_list|,
name|OrdinalMap
index|[]
name|ordinalMaps
parameter_list|)
throws|throws
name|IOException
block|{
comment|// To prevent us stepping on the rest of this class's decisions on when
comment|// to open a reader, and when not, we'll be opening a new reader instead
comment|// of using the existing "reader" object:
name|IndexReader
name|mainreader
init|=
name|openReader
argument_list|()
decl_stmt|;
comment|// TODO (Facet): can this then go segment-by-segment and avoid MultiDocsEnum etc?
name|Terms
name|terms
init|=
name|MultiFields
operator|.
name|getTerms
argument_list|(
name|mainreader
argument_list|,
name|Consts
operator|.
name|FULL
argument_list|)
decl_stmt|;
assert|assert
name|terms
operator|!=
literal|null
assert|;
comment|// TODO (Facet): explicit check / throw exception?
name|TermsEnum
name|mainte
init|=
name|terms
operator|.
name|iterator
argument_list|()
decl_stmt|;
name|DocsEnum
name|mainde
init|=
literal|null
decl_stmt|;
name|IndexReader
index|[]
name|otherreaders
init|=
operator|new
name|IndexReader
index|[
name|taxonomies
operator|.
name|length
index|]
decl_stmt|;
name|TermsEnum
index|[]
name|othertes
init|=
operator|new
name|TermsEnum
index|[
name|taxonomies
operator|.
name|length
index|]
decl_stmt|;
name|DocsEnum
index|[]
name|otherdocsEnum
init|=
operator|new
name|DocsEnum
index|[
name|taxonomies
operator|.
name|length
index|]
decl_stmt|;
comment|// just for reuse
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|otherreaders
index|[
name|i
index|]
operator|=
name|IndexReader
operator|.
name|open
argument_list|(
name|taxonomies
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|terms
operator|=
name|MultiFields
operator|.
name|getTerms
argument_list|(
name|otherreaders
index|[
name|i
index|]
argument_list|,
name|Consts
operator|.
name|FULL
argument_list|)
expr_stmt|;
assert|assert
name|terms
operator|!=
literal|null
assert|;
comment|// TODO (Facet): explicit check / throw exception?
name|othertes
index|[
name|i
index|]
operator|=
name|terms
operator|.
name|iterator
argument_list|()
expr_stmt|;
comment|// Also tell the ordinal maps their expected sizes:
name|ordinalMaps
index|[
name|i
index|]
operator|.
name|setSize
argument_list|(
name|otherreaders
index|[
name|i
index|]
operator|.
name|numDocs
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|CategoryPath
name|cp
init|=
operator|new
name|CategoryPath
argument_list|()
decl_stmt|;
comment|// We keep a "current" cursor over the alphabetically-ordered list of
comment|// categories in each taxonomy. We start the cursor on the first
comment|// (alphabetically) category of each taxonomy:
name|String
name|currentMain
decl_stmt|;
name|String
index|[]
name|currentOthers
init|=
operator|new
name|String
index|[
name|taxonomies
operator|.
name|length
index|]
decl_stmt|;
name|currentMain
operator|=
name|nextTE
argument_list|(
name|mainte
argument_list|)
expr_stmt|;
name|int
name|otherTaxonomiesLeft
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|currentOthers
index|[
name|i
index|]
operator|=
name|nextTE
argument_list|(
name|othertes
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|currentOthers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
name|otherTaxonomiesLeft
operator|++
expr_stmt|;
block|}
block|}
comment|// And then, at each step look at the first (alphabetically) of the
comment|// current taxonomies.
comment|// NOTE: The most efficient way we could have done this is using a
comment|// PriorityQueue. But for simplicity, and assuming that usually we'll
comment|// have a very small number of other taxonomies (often just 1), we use
comment|// a more naive algorithm (o(ntaxonomies) instead of o(ln ntaxonomies)
comment|// per step)
while|while
condition|(
name|otherTaxonomiesLeft
operator|>
literal|0
condition|)
block|{
comment|// TODO: use a pq here
name|String
name|first
init|=
literal|null
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|currentOthers
index|[
name|i
index|]
operator|==
literal|null
condition|)
continue|continue;
if|if
condition|(
name|first
operator|==
literal|null
operator|||
name|first
operator|.
name|compareTo
argument_list|(
name|currentOthers
index|[
name|i
index|]
argument_list|)
operator|>
literal|0
condition|)
block|{
name|first
operator|=
name|currentOthers
index|[
name|i
index|]
expr_stmt|;
block|}
block|}
name|int
name|comp
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|currentMain
operator|==
literal|null
operator|||
operator|(
name|comp
operator|=
name|currentMain
operator|.
name|compareTo
argument_list|(
name|first
argument_list|)
operator|)
operator|>
literal|0
condition|)
block|{
comment|// If 'first' is before currentMain, or currentMain is null,
comment|// then 'first' is a new category and we need to add it to the
comment|// main taxonomy. Then for all taxonomies with this 'first'
comment|// category, we need to add the new category number to their
comment|// map, and move to the next category in all of them.
name|cp
operator|.
name|clear
argument_list|()
expr_stmt|;
name|cp
operator|.
name|add
argument_list|(
name|first
argument_list|,
name|delimiter
argument_list|)
expr_stmt|;
comment|// We can call internalAddCategory() instead of addCategory()
comment|// because we know the category hasn't been seen yet.
name|int
name|newordinal
init|=
name|internalAddCategory
argument_list|(
name|cp
argument_list|,
name|cp
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
comment|// TODO (Facet): we already had this term in our hands before, in nextTE...
comment|// // TODO (Facet): no need to make this term?
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|first
operator|.
name|equals
argument_list|(
name|currentOthers
index|[
name|i
index|]
argument_list|)
condition|)
block|{
comment|// remember the remapping of this ordinal. Note how
comment|// this requires reading a posting list from the index -
comment|// but since we do this in lexical order of terms, just
comment|// like Lucene's merge works, we hope there are few seeks.
comment|// TODO (Facet): is there a quicker way? E.g., not specifying the
comment|// next term by name every time?
name|otherdocsEnum
index|[
name|i
index|]
operator|=
name|othertes
index|[
name|i
index|]
operator|.
name|docs
argument_list|(
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|otherreaders
index|[
name|i
index|]
argument_list|)
argument_list|,
name|otherdocsEnum
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|otherdocsEnum
index|[
name|i
index|]
operator|.
name|nextDoc
argument_list|()
expr_stmt|;
comment|// TODO (Facet): check?
name|int
name|origordinal
init|=
name|otherdocsEnum
index|[
name|i
index|]
operator|.
name|docID
argument_list|()
decl_stmt|;
name|ordinalMaps
index|[
name|i
index|]
operator|.
name|addMapping
argument_list|(
name|origordinal
argument_list|,
name|newordinal
argument_list|)
expr_stmt|;
comment|// and move to the next category in the i'th taxonomy
name|currentOthers
index|[
name|i
index|]
operator|=
name|nextTE
argument_list|(
name|othertes
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|currentOthers
index|[
name|i
index|]
operator|==
literal|null
condition|)
block|{
name|otherTaxonomiesLeft
operator|--
expr_stmt|;
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|comp
operator|==
literal|0
condition|)
block|{
comment|// 'first' and currentMain are the same, so both the main and some
comment|// other taxonomies need to be moved, but a category doesn't need
comment|// to be added because it already existed in the main taxonomy.
comment|// TODO (Facet): Again, is there a quicker way?
name|mainde
operator|=
name|mainte
operator|.
name|docs
argument_list|(
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|mainreader
argument_list|)
argument_list|,
name|mainde
argument_list|)
expr_stmt|;
name|mainde
operator|.
name|nextDoc
argument_list|()
expr_stmt|;
comment|// TODO (Facet): check?
name|int
name|newordinal
init|=
name|mainde
operator|.
name|docID
argument_list|()
decl_stmt|;
name|currentMain
operator|=
name|nextTE
argument_list|(
name|mainte
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|first
operator|.
name|equals
argument_list|(
name|currentOthers
index|[
name|i
index|]
argument_list|)
condition|)
block|{
comment|// TODO (Facet): again, is there a quicker way?
name|otherdocsEnum
index|[
name|i
index|]
operator|=
name|othertes
index|[
name|i
index|]
operator|.
name|docs
argument_list|(
name|MultiFields
operator|.
name|getLiveDocs
argument_list|(
name|otherreaders
index|[
name|i
index|]
argument_list|)
argument_list|,
name|otherdocsEnum
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|otherdocsEnum
index|[
name|i
index|]
operator|.
name|nextDoc
argument_list|()
expr_stmt|;
comment|// TODO (Facet): check?
name|int
name|origordinal
init|=
name|otherdocsEnum
index|[
name|i
index|]
operator|.
name|docID
argument_list|()
decl_stmt|;
name|ordinalMaps
index|[
name|i
index|]
operator|.
name|addMapping
argument_list|(
name|origordinal
argument_list|,
name|newordinal
argument_list|)
expr_stmt|;
comment|// and move to the next category
name|currentOthers
index|[
name|i
index|]
operator|=
name|nextTE
argument_list|(
name|othertes
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|currentOthers
index|[
name|i
index|]
operator|==
literal|null
condition|)
block|{
name|otherTaxonomiesLeft
operator|--
expr_stmt|;
block|}
block|}
block|}
block|}
else|else
comment|/* comp> 0 */
block|{
comment|// The currentMain doesn't appear in any of the other taxonomies -
comment|// we don't need to do anything, just continue to the next one
name|currentMain
operator|=
name|nextTE
argument_list|(
name|mainte
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Close all the readers we've opened, and also tell the ordinal maps
comment|// we're done adding to them
name|mainreader
operator|.
name|close
argument_list|()
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taxonomies
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|otherreaders
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// We never actually added a mapping for the root ordinal - let's do
comment|// it now, just so that the map is complete (every ordinal between 0
comment|// and size-1 is remapped)
name|ordinalMaps
index|[
name|i
index|]
operator|.
name|addMapping
argument_list|(
literal|0
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|ordinalMaps
index|[
name|i
index|]
operator|.
name|addDone
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Mapping from old ordinal to new ordinals, used when merging indexes     * wit separate taxonomies.    *<p>     * addToTaxonomies() merges one or more taxonomies into the given taxonomy    * (this). An OrdinalMap is filled for each of the added taxonomies,    * containing the new ordinal (in the merged taxonomy) of each of the    * categories in the old taxonomy.    *<P>      * There exist two implementations of OrdinalMap: MemoryOrdinalMap and    * DiskOrdinalMap. As their names suggest, the former keeps the map in    * memory and the latter in a temporary disk file. Because these maps will    * later be needed one by one (to remap the counting lists), not all at the    * same time, it is recommended to put the first taxonomy's map in memory,    * and all the rest on disk (later to be automatically read into memory one    * by one, when needed).    */
DECL|interface|OrdinalMap
specifier|public
specifier|static
interface|interface
name|OrdinalMap
block|{
comment|/**      * Set the size of the map. This MUST be called before addMapping().      * It is assumed (but not verified) that addMapping() will then be      * called exactly 'size' times, with different origOrdinals between 0      * and size-1.        */
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|size
parameter_list|)
throws|throws
name|IOException
function_decl|;
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * Call addDone() to say that all addMapping() have been done.      * In some implementations this might free some resources.       */
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * Return the map from the taxonomy's original (consecutive) ordinals      * to the new taxonomy's ordinals. If the map has to be read from disk      * and ordered appropriately, it is done when getMap() is called.      * getMap() should only be called once, and only when the map is actually      * needed. Calling it will also free all resources that the map might      * be holding (such as temporary disk space), other than the returned int[].      */
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * {@link OrdinalMap} maintained in memory    */
DECL|class|MemoryOrdinalMap
specifier|public
specifier|static
specifier|final
class|class
name|MemoryOrdinalMap
implements|implements
name|OrdinalMap
block|{
DECL|field|map
name|int
index|[]
name|map
decl_stmt|;
annotation|@
name|Override
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|taxonomySize
parameter_list|)
block|{
name|map
operator|=
operator|new
name|int
index|[
name|taxonomySize
index|]
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
block|{
name|map
index|[
name|origOrdinal
index|]
operator|=
name|newOrdinal
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
block|{
comment|/* nothing to do */
block|}
annotation|@
name|Override
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
block|{
return|return
name|map
return|;
block|}
block|}
comment|/**    * {@link OrdinalMap} maintained on file system    */
DECL|class|DiskOrdinalMap
specifier|public
specifier|static
specifier|final
class|class
name|DiskOrdinalMap
implements|implements
name|OrdinalMap
block|{
DECL|field|tmpfile
name|File
name|tmpfile
decl_stmt|;
DECL|field|out
name|DataOutputStream
name|out
decl_stmt|;
DECL|method|DiskOrdinalMap
specifier|public
name|DiskOrdinalMap
parameter_list|(
name|File
name|tmpfile
parameter_list|)
throws|throws
name|FileNotFoundException
block|{
name|this
operator|.
name|tmpfile
operator|=
name|tmpfile
expr_stmt|;
name|out
operator|=
operator|new
name|DataOutputStream
argument_list|(
operator|new
name|BufferedOutputStream
argument_list|(
operator|new
name|FileOutputStream
argument_list|(
name|tmpfile
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addMapping
specifier|public
name|void
name|addMapping
parameter_list|(
name|int
name|origOrdinal
parameter_list|,
name|int
name|newOrdinal
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|origOrdinal
argument_list|)
expr_stmt|;
name|out
operator|.
name|writeInt
argument_list|(
name|newOrdinal
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|setSize
specifier|public
name|void
name|setSize
parameter_list|(
name|int
name|taxonomySize
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|taxonomySize
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|addDone
specifier|public
name|void
name|addDone
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|out
operator|!=
literal|null
condition|)
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
name|out
operator|=
literal|null
expr_stmt|;
block|}
block|}
DECL|field|map
name|int
index|[]
name|map
init|=
literal|null
decl_stmt|;
annotation|@
name|Override
DECL|method|getMap
specifier|public
name|int
index|[]
name|getMap
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|map
operator|!=
literal|null
condition|)
block|{
return|return
name|map
return|;
block|}
name|addDone
argument_list|()
expr_stmt|;
comment|// in case this wasn't previously called
name|DataInputStream
name|in
init|=
operator|new
name|DataInputStream
argument_list|(
operator|new
name|BufferedInputStream
argument_list|(
operator|new
name|FileInputStream
argument_list|(
name|tmpfile
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
name|map
operator|=
operator|new
name|int
index|[
name|in
operator|.
name|readInt
argument_list|()
index|]
expr_stmt|;
comment|// NOTE: The current code assumes here that the map is complete,
comment|// i.e., every ordinal gets one and exactly one value. Otherwise,
comment|// we may run into an EOF here, or vice versa, not read everything.
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|map
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|int
name|origordinal
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
name|int
name|newordinal
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
name|map
index|[
name|origordinal
index|]
operator|=
name|newordinal
expr_stmt|;
block|}
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Delete the temporary file, which is no longer needed.
if|if
condition|(
operator|!
name|tmpfile
operator|.
name|delete
argument_list|()
condition|)
block|{
name|tmpfile
operator|.
name|deleteOnExit
argument_list|()
expr_stmt|;
block|}
return|return
name|map
return|;
block|}
block|}
DECL|method|nextTE
specifier|private
specifier|static
specifier|final
name|String
name|nextTE
parameter_list|(
name|TermsEnum
name|te
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|te
operator|.
name|next
argument_list|()
operator|!=
literal|null
condition|)
block|{
return|return
name|te
operator|.
name|term
argument_list|()
operator|.
name|utf8ToString
argument_list|()
return|;
comment|// TODO (Facet): avoid String creation/use Bytes?
block|}
return|return
literal|null
return|;
block|}
annotation|@
name|Override
DECL|method|rollback
specifier|public
name|void
name|rollback
parameter_list|()
throws|throws
name|IOException
block|{
name|indexWriter
operator|.
name|rollback
argument_list|()
expr_stmt|;
name|refreshReader
argument_list|()
expr_stmt|;
block|}
block|}
end_class
end_unit
